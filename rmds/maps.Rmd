---
title: "Mapping Notebook"
output: html_notebook
---

Temporality stable groupings
```{r}
# ──────────────────────────────────────────────────────────────
# 5-year average maps by temporally-stable IHME groups — NO EXTRA PROCESSING
# ──────────────────────────────────────────────────────────────

# 0) Packages & options
required <- c("sf","tigris","readr","dplyr","stringr","purrr","ggplot2","here","scales")
missing <- setdiff(required, rownames(installed.packages()))
if (length(missing)) install.packages(missing)
invisible(lapply(required, library, character.only = TRUE))
options(tigris_use_cache = TRUE, tigris_class = "sf")

`%||%` <- function(a, b) if (!is.null(a)) a else b

# 1) Variables to map (no transformations will be applied)
vars_to_map <- c(
  "prop_garbage","prop_light","pct_acc_miss","pct_overd_miss","DQ_overall","proportion_undercounted"
)

# 2) Load your county-year data (as-is); add time windows only
dq <- readr::read_csv(here::here("data","county_year_quality_metrics.csv.gz"), show_col_types = FALSE) %>%
  dplyr::mutate(
    year        = as.integer(year),
    county_ihme = stringr::str_pad(as.character(county_ihme), 5, pad = "0"),
    time_window = dplyr::case_when(
      year >= 1999 & year <= 2004 ~ "1999_2004",
      year >= 2005 & year <= 2010 ~ "2005_2010",
      year >= 2011 & year <= 2017 ~ "2011_2017",
      year >= 2018 & year <= 2022 ~ "2018_2022",
      TRUE ~ NA_character_
    ),
    proportion_undercounted = ifelse(sim_k_15p_nofent > 0,
                                     sim_added_k_15p_nofent / sim_k_15p_nofent,
                                     NA_real_)
  )

# Keep only variables that actually exist; no conversions
vars_present <- intersect(vars_to_map, names(dq))
if (!length(vars_present)) stop("None of vars_to_map exist in dq. Check column names.")
vars_missing <- setdiff(vars_to_map, vars_present)
if (length(vars_missing)) message("Skipping missing vars: ", paste(vars_missing, collapse = ", "))

# 3) 5-year averages by temporally-stable group (means of existing cols only)
dq_avg <- dq %>%
  dplyr::group_by(county_ihme, time_window) %>%
  dplyr::summarise(
    dplyr::across(dplyr::all_of(vars_present), ~ mean(.x, na.rm = TRUE)),
    n_years = dplyr::n(),
    .groups = "drop"
  )

# 4) Crosswalk for temporality-stable shapes (dissolve multi-GEOID groups)
load(here::here("data_raw","ihme_fips.rda"))  # provides ihme_fips
ihme_map <- ihme_fips %>%
  dplyr::transmute(
    GEOID       = stringr::str_pad(orig_fips, 5, pad = "0"),
    county_ihme = stringr::str_pad(ihme_fips, 5, pad = "0")
  ) %>%
  dplyr::distinct()

# 5) Build shapes per period & join averages
crs_proj <- 2163
shapefile_years <- c("1999_2004"=2000, "2005_2010"=2010, "2011_2017"=2015, "2018_2022"=2020)

st_shift <- function(x, offset) { sf::st_geometry(x) <- sf::st_geometry(x) + offset; x }
st_scale <- function(x, factor) { ctr <- sf::st_centroid(sf::st_union(x)); sf::st_geometry(x) <- (sf::st_geometry(x) - ctr) * factor + ctr; x }

# ——— Normalizer: ensure a 5‑digit GEOID exists, with robust fallbacks ———
normalize_geoid <- function(sf, year) {
  nms <- names(sf)

  # A) If any column starts with GEOID (GEOID, GEOID10, GEOID20…), use it.
  geoid_col <- grep("^GEOID", nms, value = TRUE)[1]
  if (!is.na(geoid_col)) {
    message("[", year, "] Using GEOID column: ", geoid_col)
    sf <- dplyr::rename(sf, GEOID = !!rlang::sym(geoid_col))
    sf$GEOID <- stringr::str_pad(as.character(sf$GEOID), 5, pad = "0")
    return(sf)
  }

  # B) Else build from STATEFP + COUNTYFP (present in 2000 & 2010)
  state_col  <- grep("^STATE.*FP$", nms, value = TRUE)[1]
  county_col <- grep("^COUNTY.*FP$", nms, value = TRUE)[1]
  if (!is.na(state_col) && !is.na(county_col)) {
    message("[", year, "] Constructing GEOID from ", state_col, " + ", county_col)
    sf$GEOID <- paste0(
      stringr::str_pad(as.character(sf[[state_col]]),  2, pad = "0"),
      stringr::str_pad(as.character(sf[[county_col]]), 3, pad = "0")
    )
    return(sf)
  }

  # C) Last resort: legacy combined id
  combo_col <- grep("^CNTYIDFP$", nms, value = TRUE)[1]
  if (!is.na(combo_col)) {
    message("[", year, "] Using combined ID column: ", combo_col, " as GEOID")
    sf <- dplyr::rename(sf, GEOID = !!rlang::sym(combo_col))
    sf$GEOID <- stringr::str_pad(as.character(sf$GEOID), 5, pad = "0")
    return(sf)
  }

  stop("No GEOID-compatible columns found in shapefile for year ", year,
       ". Columns were: ", paste(nms, collapse = ", "))
}

# ——— Build groups: normalize → transform → dissolve by county_ihme ———
build_groups_sf <- function(year) {
  counties_raw <- tigris::counties(year = year, cb = TRUE, class = "sf") %>%
    sf::st_zm(drop = TRUE, what = "ZM")

  counties_norm <- normalize_geoid(counties_raw, year) %>%  # ← normalize FIRST
    sf::st_transform(crs_proj)

  groups_sf <- counties_norm %>%
    dplyr::left_join(ihme_map, by = "GEOID") %>%
    dplyr::mutate(county_ihme = dplyr::coalesce(county_ihme, GEOID)) %>%
    dplyr::group_by(county_ihme) %>%
    dplyr::summarise(.groups = "drop")  # st_union here

  # AK / HI layout (unchanged)
  alaska   <- groups_sf %>% dplyr::filter(substr(county_ihme,1,2)=="02") %>%
    { ctr <- sf::st_centroid(sf::st_union(.)); sf::st_geometry(.) <- (sf::st_geometry(.) - ctr)*0.40 + ctr; . } %>%
    { sf::st_geometry(.) <- sf::st_geometry(.) + c(1300000, -4900000); . } %>%
    sf::st_set_crs(crs_proj)
  hawaii   <- groups_sf %>% dplyr::filter(substr(county_ihme,1,2)=="15") %>%
    { ctr <- sf::st_centroid(sf::st_union(.)); sf::st_geometry(.) <- (sf::st_geometry(.) - ctr)*1.50 + ctr; . } %>%
    { sf::st_geometry(.) <- sf::st_geometry(.) + c(5200000, -1400000); . } %>%
    sf::st_set_crs(crs_proj)
  mainland <- groups_sf %>% dplyr::filter(!substr(county_ihme,1,2) %in% c("02","15"))

  dplyr::bind_rows(mainland, alaska, hawaii)
}


joined_by_period <- purrr::imap(
  shapefile_years,
  function(year, window) {
    shape_all <- build_groups_sf(year)
    dplyr::left_join(shape_all, dplyr::filter(dq_avg, time_window == window), by = "county_ihme")
  }
)

# 6) Your original make_map (unchanged)
make_map <- function(sf_data, var, title = NULL, palette = "RdBu") {
  states <- tigris::states(cb = TRUE, class = "sf") |> st_transform(2163)

  fill_scale <- if (var == "prop_garbage") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0.15, 0.4),
    oob = scales::squish,
    direction = -1,
    na.value = "grey90"
  )
} else if (var == "prop_light") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0.20, 0.40),
    oob = scales::squish,
    direction = -1,
    na.value = "grey90"
  )
} else if (var == "pct_overd_miss") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0, 0.75),
    oob = scales::squish,
    direction = -1,
    na.value = "grey90"
  )
} else if (var == "direction_score") {
  scale_fill_distiller(
    palette = palette,
    limits = c(-1,1.5),
    oob = scales::squish,
    direction = -1,
    na.value = "grey90"
  )
} else if (var == "DQ_overall") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0.90,1),
    oob = scales::squish,
    direction = 1,
    na.value = "grey90"
  )
} else if (var == "pct_acc_miss") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0, 0.75),
    oob = scales::squish,
    direction = -1,
    na.value = "grey90"
  )
} else if (var == "pct_mandeath_comp_k") {
  scale_fill_distiller(
    palette = palette,
    limits = c(5, 20),
    oob = scales::squish,
    direction = 1,
    na.value = "grey90"
  )
} else if (var == "pct_placdth_comp_k") {
  scale_fill_distiller(
    palette = palette,
    limits = c(98, 100),
    oob = scales::squish,
    direction = 1,
    na.value = "grey90"
  )
} else if (var == "pct_educ_comp_k") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0, 100),
    oob = scales::squish,
    direction = 1,
    na.value = "grey90"
  )
} else if (var == "pct_marstat_comp_k") {
  scale_fill_distiller(
    palette = palette,
    limits = c(97, 100),
    oob = scales::squish,
    direction = 1,
    na.value = "grey90"
  )
} else if (var %in% c("pct_age_comp_k", "pct_sex_comp_k", "pct_race_comp_k")) {
  scale_fill_distiller(
    palette = palette,
    limits = c(99.5, 100),
    oob = scales::squish,
    direction = 1,
    na.value = "grey90"
  )
} else if (var == "pct_gc_I64") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0.01, 0.05),
    oob = scales::squish,
    direction = -1,
    na.value = "grey90"
  )
} else if (var == "pct_gc_C_misc") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0.005, 0.03),
    oob = scales::squish,
    direction = -1,
    na.value = "grey90"
  )
} else if (var == "pct_gc_I10") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0, 0.02),
    oob = scales::squish,
    direction = -1,
    na.value = "grey90"
  )
} else if (var == "pct_gc_R_misc") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0, 0.03),
    oob = scales::squish,
    direction = -1,
    na.value = "grey90"
  )
} else if (var == "pct_gc_N19") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0, 0.025),
    oob = scales::squish,
    direction = -1,
    na.value = "grey90"
  )
} else if (var == "pct_gc_J80") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0, 0.005),
    oob = scales::squish,
    direction = -1,
    na.value = "grey90"
  )
} else if (startsWith(var, "pct_") || var == "overall_completeness_pct") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0, 100),
    oob = scales::squish,
    direction = 1,
    na.value = "grey90"
  )
} else {
  scale_fill_distiller(
    palette = palette,
    oob = scales::squish,
    na.value = "grey90",
    direction = -1
  )
}

  ggplot() +
    geom_sf(data = sf_data, aes(fill = .data[[var]]), colour = NA) +
    geom_sf(data = states, fill = NA, colour = "white", linewidth = 0.3) +
    fill_scale +
    coord_sf(xlim = c(-2500000, 2500000),
             ylim = c(-2200000, 730000),
             expand = FALSE) +
    labs(title = title %||% var, fill = NULL) +
    theme_void() +
    theme(
      plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
      legend.text = element_text(size = 9)
    )
}

# 7) Save maps
output_dir <- here::here("figures","5yr_avg_stable")
dir.create(output_dir, showWarnings = FALSE, recursive = TRUE)

purrr::walk(vars_present, function(var) {
  purrr::walk(names(shapefile_years), function(win) {
    sf_dat  <- joined_by_period[[win]]
    if (!var %in% names(sf_dat)) return(invisible(NULL))
    map_plot <- make_map(sf_dat, var, paste(var, win))
    ggsave(filename = file.path(output_dir, paste0(var, "_", win, ".png")),
           plot = map_plot, width = 8, height = 6, dpi = 320)
  })
})

message("✓ Done. Maps saved to: ", output_dir)
```
Relationship between rural and garbage
```{r}
# 0 · Load packages -------------------------------------------------------
library(dplyr)
library(stringr)
library(ggplot2)
library(scales)
library(tigris)
library(tidycensus)
library(readr)
library(here)

# 1 · Load your data ------------------------------------------------------
dq <- read_csv(here("data", "county_year_quality_metrics.csv.gz"),
               show_col_types = FALSE)

# 3 · Convert county codes to GEOID ---------------------------------------
dq_all <- dq %>%
  mutate(
    GEOID = county_ihme
  )

cat("Counties without valid GEOID:", sum(is.na(dq_all$GEOID)), "\n")

# 4 · Get 2022 population estimates ---------------------------------------
census_api_key("671b64055fc192103cbc199d2dff91b46d9cc781", overwrite = FALSE)

pop22 <- get_acs(
  geography = "county",
  variables = "B01003_001",
  year = 2022,
  survey = "acs5"
) %>%
  select(GEOID, pop_2022 = estimate)

# 5 · Define county size buckets ------------------------------------------
cuts   <- c(0, 50000, 100000, 250000, 1000000, Inf)
labels <- c("<50k", "50–100k", "100–250k", "250k–1M", "≥1M")

county_sizes <- pop22 %>%
  mutate(size_bucket = cut(pop_2022, breaks = cuts, labels = labels, right = FALSE)) %>%
  select(GEOID, pop_2022, size_bucket)

# 6 · Join population and size to main data -------------------------------
dq_all <- dq_all %>%
  left_join(pop22, by = "GEOID") %>%
  left_join(county_sizes %>% select(GEOID, size_bucket), by = "GEOID")

# 7 · Indicators to plot --------------------------------------------------
indicators <- c(
  "prop_garbage",
  "prop_light",
  "pct_overd_miss",
  "pct_acc_miss")

# 8 · Time trends by size bucket ------------------------------------------
for (var in indicators) {
  ts_df <- dq_all %>%
    filter(!is.na(size_bucket)) %>%
    group_by(year, size_bucket) %>%
    summarise(avg_prop = mean(.data[[var]], na.rm = TRUE), .groups = "drop")
  
  g <- ggplot(ts_df, aes(year, avg_prop, colour = size_bucket)) +
    geom_line(linewidth = 1) +
    scale_y_continuous(labels = percent_format(accuracy = 0.1)) +
    labs(
      title = paste("Trend of", gsub("_", " ", var), "by county size"),
      x = NULL,
      y = "Mean percentage",
      colour = "2022 size bucket"
    ) +
    theme_bw()
  
  print(g)
  
  ggsave(
    filename = here("figures", paste0("trend_", var, "_by_size_bucket.png")),
    plot = g,
    width = 7, height = 4, dpi = 300
  )
}

# 9 · Scatterplots: pop vs indicator (2022) -------------------------------
scatter_data <- dq_all %>%
  filter(year == 2022, !is.na(pop_2022))

for (var in indicators) {
  scatter_df <- scatter_data %>%
    filter(!is.na(.data[[var]]))
  
  g <- ggplot(scatter_df, aes(x = pop_2022, y = .data[[var]])) +
    geom_point(alpha = 0.6) +
    scale_x_log10(labels = comma) +
    scale_y_continuous(labels = percent_format(accuracy = 0.1)) +
    labs(
      title = paste("County population vs", gsub("_", " ", var), "(2022)"),
      x = "County population (log10 scale)",
      y = paste("Proportion:", gsub("_", " ", var))
    ) +
    theme_bw()
  
  print(g)
  
  ggsave(
    filename = here("figures", paste0("scatter_", var, "_vs_population_2022.png")),
    plot = g,
    width = 7, height = 4, dpi = 300
  )
}
```
Clustering or anomaly detection: Uses unsupervised ML (clustering, autoencoders) to identify unusual records or patterns. If a county’s records are more often flagged as outliers, that suggests quality issues.
```{r}
# 0.  PACKAGES ------------------------------------------------------------------
library(tidyverse)
library(solitude)
library(dbscan)
library(factoextra)
library(patchwork)
library(here)

# 1.  LOAD & CLEAN DATA --------------------------------------------------------
df <- read_csv(
  here("data", "county_year_quality_metrics.csv.gz"),
  show_col_types = FALSE
) %>%
  mutate(
    year = as.integer(year)
  )

# 2.  COMPUTE prop_all_comp & DEFINE VARIABLES --------------------------------
df <- df %>%
  mutate(
    prop_all_comp = (
      marstat_comp_k + placdth_comp_k + educ_comp_k +
      age_comp_k     + sex_comp_k      + mandeath_comp_k
    ) / (6 * n_cert)
  )

quality_vars <- c(
  "DQ_prop_garbage", "prop_light", "pct_overd_miss", "pct_acc_miss"
)

# 3.  AVERAGE OVER YEARS BY COUNTY_ihme ----------------------------------------
county_avg <- df %>%
  filter(year >= 1999, year <= 2022) %>%
  group_by(county_ihme) %>%
  summarise(
    across(all_of(quality_vars), ~ mean(.x, na.rm = TRUE)),
    n_cert = mean(n_cert, na.rm = TRUE),
    .groups = "drop"
  )

# ------------------------------------------------------------------
# 4 · SCALE, FIT MODELS & ADD SCORES   (corrected)
# ------------------------------------------------------------------

## 4.1  Impute medians for NAs -------------------------------------
mat <- county_avg %>%
  mutate(across(all_of(quality_vars), 
                ~ ifelse(is.na(.x), median(.x, na.rm = TRUE), .x)))

## 4.2  Drop any rows missing county_ihme --------------------------
if (any(is.na(mat$county_ihme))) {
  dropped <- mat %>% filter(is.na(county_ihme)) %>% nrow()
  message("Dropping ", dropped, " rows with missing county_ihme.")
  mat <- mat %>% filter(!is.na(county_ihme))
}

## 4.3  Build scaled predictor matrix ------------------------------
X_df <- mat %>%
  select(all_of(quality_vars)) %>%
  scale() %>%
  as.data.frame()
row.names(X_df) <- mat$county_ihme

# 4.4  Isolation Forest (solitude) ------------------------------
iso_mod <- solitude::isolationForest$new(num_trees = 100)
iso_mod$fit(X_df)
iso_scores <- iso_mod$predict(X_df)$anomaly_score

# 4.5  Local Outlier Factor (dbscan) -----------------------------
lof_scores <- dbscan::lof(as.matrix(X_df),
                          minPts = round(sqrt(nrow(X_df))))

# 4.6  k-means clustering (k = 3) --------------------------------
set.seed(123)
km <- kmeans(X_df, centers = 3, nstart = 25)

# 4.7  Bind results back to mat ----------------------------------
mat <- mat %>%
  mutate(
    iso_score  = iso_scores,
    lof_score  = lof_scores,
    km_cluster = km$cluster,
    iso_rank   = rank(-iso_score, ties.method = "first"),
    lof_rank   = rank(-lof_score,  ties.method = "first"),
    anomaly_flag = iso_rank <= ceiling(0.05 * n()) |
                   lof_rank <= ceiling(0.05 * n())
  )

# store final results
res <- mat

# 5.  PCA FOR PLOTTING --------------------------------------------------------
pca <- prcomp(X_df, center = TRUE, scale. = FALSE)
pca2 <- as_tibble(pca$x[,1:2]) %>%
  set_names(c("PC1", "PC2")) %>%
  mutate(
    county_ihme  = row.names(pca$x),
    iso_score    = res$iso_score,
    km_cluster   = factor(res$km_cluster),
    anomaly_flag = res$anomaly_flag
  )

# 6.  SAVE OUTPUTS ------------------------------------------------------------
write_csv(res,
          here("output", "county_outlier_scores_1999_2022_avg.csv"))

p1 <- ggplot(pca2, aes(PC1, PC2, colour = km_cluster)) +
  geom_point(alpha = 0.7, size = 3) +
  labs(title = "K-means clusters (k = 3)") +
  theme_minimal()

p2 <- ggplot(pca2, aes(PC1, PC2, size = iso_score)) +
  geom_point(colour = "red", alpha = 0.5) +
  labs(title = "Isolation Forest anomaly score") +
  theme_minimal()

(p1 + p2) %>%
  ggsave(
    filename = here("figures", "outlier_plot_1999_2022_avg.png"),
    width    = 10, height = 5, dpi = 300
  )

# 7.  PRINT TOP OUTLIERS & COMPUTE DIRECTION_SCORE ---------------------------
message("Top 15 counties by Isolation Forest score:\n")
print(res %>% arrange(desc(iso_score)) %>% slice_head(n = 15))

message("Top 15 counties by LOF score:\n")
print(res %>% arrange(desc(lof_score)) %>% slice_head(n = 15))

res <- res %>%
  mutate(
    across(all_of(quality_vars),
           ~ as.numeric(scale(.x)),
           .names = "z_{.col}"),
    direction_score = (
      z_prop_light     +
      z_pct_overd_miss +
      z_pct_acc_miss   +
      z_DQ_prop_garbage
    ) / 4
  )

message("Script complete.")
```
map direction scores
```{r}

# map direction scores
# ──────────────────────────────────────────────────────────────
# 0 · Packages (load after your existing libs)
# ──────────────────────────────────────────────────────────────
library(sf)
library(tigris)      # for county & state shapes
library(ggplot2)
library(dplyr)
library(stringr)

options(tigris_use_cache = TRUE, tigris_class = "sf")
crs_proj <- 2163     # CONUS Albers (EPSG:2163)

# ──────────────────────────────────────────────────────────────
# 1 · Pull direction_score & county_ihme from your results
# ──────────────────────────────────────────────────────────────
dir_scores <- res %>%       
  select(county_ihme, direction_score)

# ──────────────────────────────────────────────────────────────
# 2 · Build county geometry collapsed to county_ihme
# ──────────────────────────────────────────────────────────────
# 2015 TIGER/CB shapefile is a good compromise between detail & performance
county_sf <- counties(cb = TRUE, year = 2015, class = "sf") %>%
  st_transform(crs_proj) %>%
  mutate(fips = GEOID,
         fips = str_pad(fips, 5, pad = "0")) %>%
  # Attach IHME mapping and collapse to temporally-stable polygons
  left_join(ihme_xwalk, by = c("fips" = "fips")) %>%
  mutate(county_ihme = coalesce(county_ihme, fips)) %>%
  select(county_ihme, geometry) %>%
  group_by(county_ihme) %>%
  summarise(geometry = st_union(geometry), .groups = "drop")  # dissolve splits


# ──────────────────────────────────────────────────────────────
# 3 · Join scores ➜ geometry
# ──────────────────────────────────────────────────────────────
map_sf <- left_join(county_sf, dir_scores, by = "county_ihme")

# ──────────────────────────────────────────────────────────────
# 4 · Build colour scale limits (symmetric diverging)
# ──────────────────────────────────────────────────────────────
max_abs <- max(abs(map_sf$direction_score), na.rm = TRUE)

# ─────────────────────────────────────────────────────────
# 0 · Prepare geometry for every county_ihme
# ─────────────────────────────────────────────────────────
library(sf)
library(tigris)
options(tigris_use_cache = TRUE, tigris_class = "sf")

crs_proj <- 2163

# county_sf: one polygon per IHME county_ihme
county_sf <- counties(year = 2015, cb = TRUE, class = "sf") |>
  st_transform(crs_proj) |>
  mutate(fips = GEOID) |>
  left_join(ihme_xwalk, by = c("fips" = "fips")) |>
  mutate(county_ihme = dplyr::coalesce(county_ihme, fips)) |>
  select(county_ihme, geometry) |>
  group_by(county_ihme) |>
  summarise(geometry = st_union(geometry), .groups = "drop")

# ─────────────────────────────────────────────────────────
# 1 · Join your results (res) to geometry
#    res must contain 'county_ihme' and the variable you want to map
# ─────────────────────────────────────────────────────────
map_sf <- county_sf |>
  left_join(res, by = "county_ihme")      # res is your data-frame

# ─────────────────────────────────────────────────────────
# 2 · Call make_map()
#    Example: visualise the direction_score column
# ─────────────────────────────────────────────────────────
map_plot <- make_map(
  sf_data = map_sf,
  var     = "direction_score",            # any column present in map_sf
  title   = "Average Z-score with direction (1999–2022)"
)

# Show in RStudio viewer
print(map_plot)

# Save to file (optional)
ggsave(here("figures", "direction_score_map.png"), map_plot,
       width = 9, height = 6, dpi = 320)
```
Direction scores by 5 year period with global mean and standard deviation
```{r}
# ──────────────────────────────────────────────────────────────
# 0 · Globals  (μ and σ from the overall 1999–2022 county means)
# ──────────────────────────────────────────────────────────────
quality_vars <- c("DQ_prop_garbage", "prop_light", "pct_overd_miss", "pct_acc_miss")

# If `mat` from the earlier chunk is in memory, use that; otherwise rebuild quickly
if (!exists("mat")) {
  mat <- df %>%                            # `df` was read in the anomaly-detection chunk
    filter(year >= 1999, year <= 2022) %>%
    mutate(prop_all_comp = (marstat_comp_k + placdth_comp_k +
                             age_comp_k + sex_comp_k ) /
                           (4 * n_cert)) %>%
    group_by(county_ihme) %>%
    summarise(across(all_of(quality_vars), ~ mean(.x, na.rm = TRUE)),
              .groups = "drop")
}

global_means <- sapply(mat[quality_vars], mean, na.rm = TRUE)
global_sds   <- sapply(mat[quality_vars], sd,   na.rm = TRUE)

# ──────────────────────────────────────────────────────────────
# 1 · Period definitions
# ──────────────────────────────────────────────────────────────
periods <- list(
  `1999_2004` = 1999:2004,
  `2005_2010` = 2005:2010,
  `2011_2017` = 2011:2017,
  `2018_2022` = 2018:2022,
  `2020_2022` = 2020:2022
)

# ──────────────────────────────────────────────────────────────
# 2 · Geometry (one polygon per IHME county_ihme)
# ──────────────────────────────────────────────────────────────
if (!exists("county_sf")) {
  county_sf <- counties(cb = TRUE, year = 2015, class = "sf") %>%
    st_transform(2163) %>%
    mutate(fips = GEOID) %>%
    left_join(ihme_xwalk, by = "fips") %>%
    mutate(county_ihme = coalesce(county_ihme, fips)) %>%
    select(county_ihme, geometry) %>%
    group_by(county_ihme) %>%
    summarise(geometry = st_union(geometry), .groups = "drop")
}

# ──────────────────────────────────────────────────────────────
# 3 · Loop over periods: compute & map direction_score
# ──────────────────────────────────────────────────────────────
out_dir <- here::here("figures", "direction_score_periods")
dir.create(out_dir, showWarnings = FALSE, recursive = TRUE)

for (pname in names(periods)) {
  yrs <- periods[[pname]]

  # county means within the period
  peri_df <- df %>%
    filter(year %in% yrs) %>%
    mutate(prop_all_comp = (marstat_comp_k + placdth_comp_k + educ_comp_k +
                             age_comp_k + sex_comp_k + mandeath_comp_k) /
                           (6 * n_cert)) %>%
    group_by(county_ihme) %>%
    summarise(across(all_of(quality_vars), ~ mean(.x, na.rm = TRUE)),
              .groups = "drop")

  # fixed-reference Z-scores
  peri_df <- peri_df %>%
    mutate(across(all_of(quality_vars),
                  \(x, nm=cur_column()) (x - global_means[nm]) / global_sds[nm],
                  .names = "z_{.col}")) %>%
    mutate(
      direction_score = (z_prop_light + z_pct_overd_miss +
                         z_pct_acc_miss + z_DQ_prop_garbage) / 4
    )
  
    # county means within the period  ──────────────────────────────
  peri_df <- df %>%
    filter(year %in% yrs) %>%
    mutate(prop_all_comp = (marstat_comp_k + placdth_comp_k + educ_comp_k +
                            age_comp_k + sex_comp_k + mandeath_comp_k) /
                           (6 * n_cert)) %>%
    group_by(county_ihme) %>%
    summarise(across(all_of(quality_vars), ~ mean(.x, na.rm = TRUE)),
              .groups = "drop")

  ## NEW 1: guarantee every county_ihme appears (even if all NA)
  peri_df <- county_sf %>%                        # drop geometry, keep IDs
    st_drop_geometry() %>%
    select(county_ihme) %>%
    left_join(peri_df, by = "county_ihme")

  ## NEW 2: impute missing period means with the *same* values
  ##        we used for the overall map (global_means)
  peri_df <- peri_df %>%
    mutate(across(all_of(quality_vars),
                  \(x, nm = cur_column()) replace_na(x, global_means[nm])))

  # fixed-reference Z-scores  ───────────────────────────────────
  peri_df <- peri_df %>%
    mutate(across(all_of(quality_vars),
                  \(x, nm = cur_column()) (x - global_means[nm]) / global_sds[nm],
                  .names = "z_{.col}")) %>%
    ## NEW 3: compute score with all four components now present
    mutate(direction_score = (z_prop_light + z_pct_overd_miss +
                              z_pct_acc_miss  + z_DQ_prop_garbage) / 4)


  # join geometry ➜ map
  map_sf <- county_sf %>% left_join(peri_df, by = "county_ihme")

  p <- make_map(map_sf,
                var   = "direction_score",
                title = glue::glue("Direction score {pname} (scaled to 1999–2022 μ/σ)"))

  ggsave(file.path(out_dir, glue::glue("direction_score_{pname}.png")),
         plot = p, width = 9, height = 6, dpi = 320)
}
```

```{r}
# Assign each county its 2022 size bucket (once per county)
size_r2 <- direction_year %>%
  filter(year >= 1999, year <= 2022) %>%
  group_by(county_ihme) %>%
  summarise(
    mean_direction = mean(direction_score, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  left_join(pop22, by = "county_ihme") %>%
  mutate(size_bucket = cut(pop_2022, breaks = size_breaks, labels = size_labels, right = FALSE))

# Fit one-way ANOVA
fit_size <- lm(mean_direction ~ size_bucket, data = size_r2)
r2_size <- summary(fit_size)$r.squared
r2p_size <- round(r2_size * 100, 1)

cat("County size R²:", sprintf("%.3f", r2_size),
    "→", r2p_size, "% of variance in average direction_scores explained by 2022 county size bucket.\n")

income_r2 <- direction_year %>%
  filter(year >= 1999, year <= 2022) %>%
  group_by(county_ihme) %>%
  summarise(
    mean_direction = mean(direction_score, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  left_join(income22, by = "county_ihme") %>%
  mutate(income_bucket = cut(medhhinc_2022, breaks = income_breaks, labels = income_labels, right = FALSE))

fit_income <- lm(mean_direction ~ income_bucket, data = income_r2)
r2_income <- summary(fit_income)$r.squared
r2p_income <- round(r2_income * 100, 1)

cat("Income R²:", sprintf("%.3f", r2_income),
    "→", r2p_income, "% of variance in average direction_scores explained by 2022 income bucket.\n")

```


```{r}
# ──────────────────────────────────────────────────────────────
# 11 · Education: time-series + variance explained
# ──────────────────────────────────────────────────────────────
library(tidycensus)
library(dplyr)
library(ggplot2)
library(glue)
library(scales)

# 1. Get global means and sds for each variable across all years
global_stats <- df %>%
  summarise(across(
    c(DQ_prop_garbage, prop_light, pct_overd_miss, pct_acc_miss),
    list(mean = ~mean(.x, na.rm = TRUE),
         sd   = ~sd(.x, na.rm = TRUE)),
    .names = "{.col}_{.fn}"
  ))

# 2. Define a function to compute global z-score
zscore_global <- function(x, m, s) (x - m) / s

# 3. Calculate direction_score for every county-year using global stats
direction_year <- df %>%
  filter(year >= 1999, year <= 2022) %>%
  mutate(
    z_DQ_prop_garbage  = zscore_global(DQ_prop_garbage, global_stats$DQ_prop_garbage_mean, global_stats$DQ_prop_garbage_sd),
    z_prop_light       = zscore_global(prop_light,      global_stats$prop_light_mean,      global_stats$prop_light_sd),
    z_pct_overd_miss   = zscore_global(pct_overd_miss,  global_stats$pct_overd_miss_mean,  global_stats$pct_overd_miss_sd),
    z_pct_acc_miss     = zscore_global(pct_acc_miss,    global_stats$pct_acc_miss_mean,    global_stats$pct_acc_miss_sd),
    direction_score    = (z_prop_light + z_pct_overd_miss + z_pct_acc_miss + z_DQ_prop_garbage) / 4
  ) %>%
  select(county_ihme, year, direction_score) %>%
  filter(!is.na(county_ihme), !is.na(direction_score))


# 11-a · 2022 % bachelor’s+ from ACS table B15003
edu22_raw <- get_acs(
  geography = "county",
  table     = "B15003",
  year      = 2022,
  survey    = "acs5",
  cache_table = TRUE
)

# Keep total (001) and bachelor+ (022-025) → compute share
edu22 <- edu22_raw %>%
  select(GEOID, variable, estimate) %>%
  pivot_wider(names_from = variable, values_from = estimate) %>%
  transmute(
    county_ihme        = GEOID,
    total_25_plus      = B15003_001,
    bach_plus          = B15003_022 + B15003_023 + B15003_024 + B15003_025,
    pct_bachplus_2022  = bach_plus / total_25_plus
  )

# 11-b · Build education buckets (share bachelor’s+)
edu_breaks  <- c(-Inf, .20, .30, .40, .50, Inf)
edu_labels  <- c("<20%", "20–30%", "30–40%", "40–50%", "≥50%")

direction_year <- direction_year %>%
  left_join(edu22, by = "county_ihme") %>%
  mutate(edu_bucket = cut(pct_bachplus_2022,
                          breaks = edu_breaks,
                          labels = edu_labels,
                          right  = FALSE))

# Join education data to annual direction scores
direction_year <- direction_year %>%
  left_join(edu22, by = "county_ihme")


# Proceed with analysis and plotting
ts_edu <- direction_year %>%
  filter(!is.na(edu_bucket)) %>%
  group_by(year, edu_bucket) %>%
  summarise(avg_direction = mean(direction_score, na.rm = TRUE),
            .groups = "drop")

g_edu <- ggplot(ts_edu,
                aes(year, avg_direction, colour = edu_bucket)) +
  geom_line(linewidth = 1) +
  labs(title = "Direction-score trend by 2022 education bucket",
       x = NULL, y = "Mean direction score",
       colour = "% bachelor’s or higher") +
  theme_bw()

ggsave(here("figures", "timeseries_direction_by_edu_bucket.png"),
       g_edu, width = 7, height = 4, dpi = 300)
print(g_edu)

res <- res %>%
  mutate(
    z_DQ_prop_garbage  = as.numeric(scale(DQ_prop_garbage)),
    z_prop_light       = as.numeric(scale(prop_light)),
    z_pct_overd_miss   = as.numeric(scale(pct_overd_miss)),
    z_pct_acc_miss     = as.numeric(scale(pct_acc_miss)),
    direction_score    = (z_prop_light + z_pct_overd_miss + z_pct_acc_miss + z_DQ_prop_garbage) / 4
  )

# 11-d · R²: variance explained by education
var_edu <- res %>%                       # 1999-2022 averages
  left_join(edu22, by = "county_ihme") %>%
  filter(!is.na(direction_score), !is.na(pct_bachplus_2022))

fit_edu <- lm(direction_score ~ pct_bachplus_2022, data = var_edu)
r2e  <- summary(fit_edu)$r.squared
r2ep <- round(r2e * 100, 1)

cat("Education R²:", sprintf("%.3f", r2e),
    "→", r2ep, "% of variance in average direction_scores explained by pct bachelor’s+.\n")

message("✓ Time-series plot saved: timeseries_direction_by_edu_bucket.png")
```

```{r}
# ---- County population size bucket (using 2022) ----
library(tidycensus)

# Get 2022 population for each county
pop22_raw <- get_acs(
  geography = "county",
  variables = "B01003_001",
  year = 2022,
  survey = "acs5",
  cache_table = TRUE
)

pop22 <- pop22_raw %>%
  transmute(
    county_ihme = GEOID,
    pop_2022 = estimate
  )

# Create population size buckets
size_breaks <- c(-Inf, 50000, 100000, 250000, 1e6, Inf)
size_labels <- c("<50k", "50–100k", "100–250k", "250k–1M", "≥1M")

direction_year_size <- direction_year %>%
  left_join(pop22, by = "county_ihme") %>%
  mutate(
    size_bucket = cut(pop_2022, breaks = size_breaks, labels = size_labels, right = FALSE)
  )

ts_size <- direction_year_size %>%
  filter(!is.na(size_bucket)) %>%
  group_by(year, size_bucket) %>%
  summarise(avg_direction = mean(direction_score, na.rm = TRUE), .groups = "drop")

g_size <- ggplot(ts_size, aes(year, avg_direction, colour = size_bucket)) +
  geom_line(linewidth = 1) +
  labs(
    title = "Direction-score trend by 2022 county size bucket",
    x = NULL, y = "Mean direction score",
    colour = "2022 size bucket"
  ) +
  theme_bw()

ggsave(here("figures", "timeseries_direction_by_size_bucket.png"),
       g_size, width = 7, height = 4, dpi = 300)
print(g_size)

# ---- County household income bucket (using 2022) ----

# Get 2022 median household income (ACS table B19013)
income22_raw <- get_acs(
  geography = "county",
  variables = "B19013_001",
  year = 2022,
  survey = "acs5",
  cache_table = TRUE
)

income22 <- income22_raw %>%
  transmute(
    county_ihme = GEOID,
    medhhinc_2022 = estimate
  )

# Create household income buckets (edit cutpoints as desired)
income_breaks <- c(-Inf, 45000, 55000, 65000, 75000, Inf)
income_labels <- c("<$45k", "$45–55k", "$55–65k", "$65–75k", "$75k+")

direction_year_income <- direction_year %>%
  left_join(income22, by = "county_ihme") %>%
  mutate(
    income_bucket = cut(medhhinc_2022, breaks = income_breaks, labels = income_labels, right = FALSE)
  )

ts_income <- direction_year_income %>%
  filter(!is.na(income_bucket)) %>%
  group_by(year, income_bucket) %>%
  summarise(avg_direction = mean(direction_score, na.rm = TRUE), .groups = "drop")

g_income <- ggplot(ts_income, aes(year, avg_direction, colour = income_bucket)) +
  geom_line(linewidth = 1) +
  labs(
    title = "Direction-score trend by 2022 income bucket",
    x = NULL, y = "Mean direction score",
    colour = "Household income"
  ) +
  theme_bw()

ggsave(here("figures", "timeseries_direction_by_income_bucket.png"),
       g_income, width = 7, height = 4, dpi = 300)
print(g_income)

```
map diversity
```{r}
# ──────────────────────────────────────────────────────────────
# Map cluster-level diversity (S, Rao's Q) — with NA fill-in
# ──────────────────────────────────────────────────────────────
suppressPackageStartupMessages({
  library(dplyr); library(readr); library(stringr); library(purrr)
  library(sf); library(tigris); library(ggplot2); library(glue); library(here)
})

options(tigris_use_cache = TRUE, tigris_class = "sf")
crs_proj <- 2163

# 0) Inputs -----------------------------------------------------
div_tbl <- read_csv(here("output", "cluster_cod_diversity.csv"), show_col_types = FALSE)
stopifnot(all(c("cluster","period","deaths","S","I") %in% names(div_tbl)))

ccm <- read_csv(here("output", "county_cluster_membership_all_periods.csv"),
                show_col_types = FALSE) %>%
  mutate(fips = stringr::str_pad(as.character(fips), 5, pad = "0")) %>%
  select(fips, period, cluster)

# match your analysis/mapping vintages
shapefile_years <- c("1999_2004" = 2000,
                     "2005_2010" = 2010,
                     "2011_2017" = 2015,
                     "2018_2022" = 2020)

# 1) Helpers ----------------------------------------------------
.pick_fips <- function(df) {
  hits <- grep("^GEOID", names(df), value = TRUE)
  if (length(hits) >= 1) return(df[[hits[1]]])
  if (all(c("STATEFP","COUNTYFP") %in% names(df))) return(paste0(df$STATEFP, df$COUNTYFP))
  stop("No GEOID or STATEFP/COUNTYFP in counties() output: ", paste(names(df), collapse = ", "))
}

build_graph_for_year <- function(year) {
  sf_raw <- tigris::counties(year = year, cb = TRUE, class = "sf") |>
    sf::st_zm(drop = TRUE, what = "ZM") |>
    sf::st_transform(crs_proj)

  # compute FIPS outside mutate (no '.' pronoun issues)
  fips_vec <- stringr::str_pad(.pick_fips(sf_raw), 5, pad = "0")

  sf <- sf_raw |>
    dplyr::mutate(fips = fips_vec) |>
    dplyr::select(fips, geometry)

  adj <- sf::st_touches(sf)
  edges <- tibble::tibble(
    from = rep(sf$fips, lengths(adj)),
    to   = sf$fips[unlist(adj)]
  ) |>
    dplyr::filter(from < to)

  g <- igraph::graph_from_data_frame(edges, directed = FALSE, vertices = sf$fips)
  list(sf = sf, g = g)
}

# Fill missing cluster labels by assigning each unlabeled county
# to the modal cluster among its neighboring counties (or the largest cluster overall if isolated).
fix_na_by_nearest <- function(clu_named_vec, g) {
  stopifnot(!is.null(names(clu_named_vec)))
  unlabeled <- names(clu_named_vec)[is.na(clu_named_vec)]
  if (!length(unlabeled)) return(clu_named_vec)

  for (u in unlabeled) {
    # neighbors present in the vector
    neigh <- igraph::neighbors(g, u) |> names()
    neigh <- neigh[neigh %in% names(clu_named_vec)]
    nclu  <- clu_named_vec[neigh]
    nclu  <- nclu[!is.na(nclu)]

    if (length(nclu)) {
      tab <- sort(table(nclu), decreasing = TRUE)
      clu_named_vec[u] <- names(tab)[1]
    } else {
      # total fallback: largest cluster overall among labeled
      tab <- sort(table(clu_named_vec[!is.na(clu_named_vec)]), decreasing = TRUE)
      if (length(tab)) clu_named_vec[u] <- names(tab)[1]
    }
  }
  clu_named_vec
}

make_cluster_map <- function(sf_data, var, title = NULL) {
  states <- tigris::states(cb = TRUE, class = "sf") %>% sf::st_transform(crs_proj)
  vr   <- range(sf_data[[var]], na.rm = TRUE)
  lims <- c(floor(min(vr)*1000)/1000, ceiling(max(vr)*1000)/1000)
  plot_title <- if (is.null(title)) var else title

  ggplot() +
    geom_sf(data = sf_data, aes(fill = .data[[var]]), colour = NA) +
    geom_sf(data = states, fill = NA, colour = "white", linewidth = 0.3) +
    scale_fill_distiller(palette = "RdBu", direction = -1,
                         limits = lims, oob = scales::squish, na.value = "grey90") +
    coord_sf(xlim = c(-2500000, 2500000),
             ylim = c(-2200000, 730000),
             expand = FALSE) +
    labs(title = plot_title, fill = NULL) +
    theme_void() +
    theme(
      plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
      legend.text = element_text(size = 9)
    )
}

# 2) Build & plot (fills NAs before union) ---------------------
out_dir <- here("figures", "cluster_diversity_maps")
dir.create(out_dir, showWarnings = FALSE, recursive = TRUE)

imap(shapefile_years, function(yr, win) {
  message("Mapping ", win, " …")

  # counties + graph for THIS period's geography
  geo <- build_graph_for_year(yr)
  counties_sf <- geo$sf
  g_period    <- geo$g

  # membership for this period; ensure vector named by fips
  mem <- ccm %>% filter(period == win) %>% select(fips, cluster)

  # start from all counties in that year — join, then fill missing cluster labels
  all_cty <- counties_sf %>% left_join(mem, by = "fips")
  clu_vec <- setNames(all_cty$cluster, all_cty$fips)
  clu_vec <- fix_na_by_nearest(clu_vec, g_period)

  # attach filled clusters back
  all_cty$cluster <- unname(clu_vec[all_cty$fips])

  # build cluster polygons
  clusters_sf <- all_cty %>%
    group_by(cluster) %>%
    summarise(geometry = sf::st_union(geometry), .groups = "drop")

  # attach diversity for this period
  div_p <- div_tbl %>% filter(period == win) %>% select(cluster, S, I, deaths)

  sf_dat <- clusters_sf %>% left_join(div_p, by = "cluster")

  # plot & save
  pS <- make_cluster_map(sf_dat, "S", glue("Cause-of-death diversity S — {win}"))
  pI <- make_cluster_map(sf_dat, "I", glue("CoD inequality (Rao’s Q) — {win}"))

  ggsave(filename = file.path(out_dir, glue("cluster_S_{win}.png")), plot = pS, width = 8, height = 6, dpi = 320)
  ggsave(filename = file.path(out_dir, glue("cluster_I_{win}.png")), plot = pI, width = 8, height = 6, dpi = 320)

  print(pS); print(pI)
})
```
Correlate Phillips detail with age-bucket shares by period
```{r}
suppressPackageStartupMessages({
  library(arrow); library(dplyr); library(stringr); library(readr)
  library(tidyr); library(here)
})

parquet_dir <- if (fs::dir_exists(here("data_private","mcod"))) here("data_private","mcod") else here("data_private","mcod_sample")
county_var  <- "county_ihme"

detail_df <- read_csv(here("output","cluster_phillips_detail.csv"), show_col_types = FALSE) %>%
  transmute(period, cluster, detail = coalesce(detail_phillips_refsize, detail_phillips_raw)) %>%
  distinct()

ccm <- read_csv(here("output","county_cluster_membership.csv"), show_col_types = FALSE) %>%
  mutate(fips = str_pad(as.character(fips), 5, pad = "0")) %>%
  select(fips, period, cluster)

period_of <- function(y){
  dplyr::case_when(
    y >= 1999 & y <= 2006 ~ "1999_2006",
    y >= 2007 & y <= 2014 ~ "2007_2014",
    y >= 2015 & y <= 2022 ~ "2015_2022",
    TRUE ~ NA_character_
  )
}

ds <- open_dataset(parquet_dir)
stopifnot(all(c("year", county_var, "age") %in% names(ds$schema)))

age_levels <- c("0-14","15-24","25-44","45-64","65-74","75-84","85+")
cuts <- c(-Inf, 14, 24, 44, 64, 74, 84, Inf)

# --- ONE row per (period, cluster, age_bucket) ---
ages_clu <- ds %>%
  filter(!is.na(year), !is.na(!!sym(county_var)), !is.na(age)) %>%
  select(year, !!sym(county_var), age) %>%
  collect() %>%
  mutate(fips = str_pad(as.character(.data[[county_var]]), 5, pad = "0"),
         period = period_of(year)) %>%
  filter(!is.na(period), is.finite(age), age >= 0, age <= 110) %>%
  inner_join(ccm, by = c("fips","period")) %>%
  mutate(age_bucket = cut(age, breaks = cuts, labels = age_levels, right = TRUE)) %>%
  count(period, cluster, age_bucket, name = "n") %>%
  group_by(period, cluster) %>%
  mutate(share = n / sum(n)) %>%
  ungroup() %>%
  distinct(period, cluster, age_bucket, share)   # <- dedupe

ages_det <- ages_clu %>% inner_join(detail_df, by = c("period","cluster"))

# sanity: cluster counts should match detail_df
ages_det %>% distinct(period, cluster) %>% count(period, name = "n_clusters_actual")
# compare to:
detail_df %>% count(period, name = "n_clusters_expected")

# --- Cross-cluster correlations (per period × bucket) ---
age_corr <- ages_det %>%
  group_by(period, age_bucket) %>%
  summarise(
    n_clusters = n_distinct(cluster[is.finite(share) & is.finite(detail)]),
    cor_pearson  = {x<-share; y<-detail; if (sd(x,na.rm=TRUE)==0 || sd(y,na.rm=TRUE)==0) NA_real_
                    else suppressWarnings(cor(x,y,use="complete.obs"))},
    cor_spearman = {x<-share; y<-detail; if (sd(x,na.rm=TRUE)==0 || sd(y,na.rm=TRUE)==0) NA_real_
                    else suppressWarnings(cor(x,y,use="complete.obs",method="spearman"))},
    .groups = "drop"
  ) %>% arrange(age_bucket, period)

write_csv(age_corr, here("output","corr_detail_by_agebucket_timeseries_FIXED.csv"))
age_corr
```
maps phillips detail metric
```{r}


# ──────────────────────────────────────────────────────────────
# Map Phillips detail — CLUSTERS ONLY (by period)
# ──────────────────────────────────────────────────────────────
suppressPackageStartupMessages({
  library(dplyr); library(readr); library(stringr); library(purrr)
  library(sf); library(tigris); library(ggplot2); library(glue); library(here)
})

options(tigris_use_cache = TRUE, tigris_class = "sf")
crs_proj <- 2163

# ◼ Choose which metric to plot: "detail_phillips_refsize" or "detail_phillips_raw"
VAR <- "detail_phillips_refsize"

# ‣ Inputs -----------------------------------------------------
clu_det <- read_csv(here("output", "cluster_phillips_detail.csv"), show_col_types = FALSE)
stopifnot(all(c("cluster","period") %in% names(clu_det)))

# if refsize column isn’t present (older runs), fall back to raw
if (!VAR %in% names(clu_det)) {
  message("Requested VAR '", VAR, "' not found. Falling back to 'detail_phillips_raw'.")
  VAR <- "detail_phillips_raw"
  stopifnot(VAR %in% names(clu_det))
}

ccm <- read_csv(here("output", "county_cluster_membership.csv"), show_col_types = FALSE) %>%
  mutate(fips = str_pad(as.character(fips), 5, pad = "0")) %>%
  select(fips, period, cluster)

# ‣ Period → shapefile year (UPDATED periods) ------------------
shapefile_years <- c("1999_2006" = 2000,
                     "2007_2014" = 2010,
                     "2015_2022" = 2020)

# ‣ Helpers ----------------------------------------------------
.pick_fips <- function(df) {
  hits <- grep("^GEOID", names(df), value = TRUE)
  if (length(hits) >= 1) return(df[[hits[1]]])
  if (all(c("STATEFP","COUNTYFP") %in% names(df))) return(paste0(df$STATEFP, df$COUNTYFP))
  stop("No GEOID or STATEFP/COUNTYFP columns found in counties() output. Names: ",
       paste(names(df), collapse = ", "))
}

build_cluster_sf <- function(period_name, shp_year, ccm_df) {
  counties_raw <- tigris::counties(year = shp_year, cb = TRUE, class = "sf") %>%
    sf::st_zm(drop = TRUE, what = "ZM") %>%
    sf::st_transform(crs_proj)

  counties_sf <- counties_raw %>%
    mutate(fips = str_pad(.pick_fips(counties_raw), 5, pad = "0")) %>%
    select(fips, geometry)

  m <- ccm_df %>% filter(period == period_name)
  if (nrow(m) == 0) stop("No county→cluster rows for period ", period_name)

  j <- counties_sf %>% left_join(m, by = "fips") %>% filter(!is.na(cluster))

  clusters_sf <- j %>%
    group_by(cluster) %>%
    summarise(geometry = sf::st_union(geometry), .groups = "drop") %>%
    suppressWarnings()
  clusters_sf
}

make_map <- function(sf_data, var, title = NULL) {
  states <- tigris::states(cb = TRUE, class = "sf") %>% sf::st_transform(crs_proj)
  lims <- c(50, 70)  # fixed scale range
  plot_title <- if (is.null(title)) var else title

  ggplot() +
    geom_sf(data = sf_data, aes(fill = .data[[var]]), colour = NA) +
    geom_sf(data = states, fill = NA, colour = "white", linewidth = 0.3) +
    scale_fill_distiller(palette = "RdBu", direction = 1,
                         limits = lims, oob = scales::squish, na.value = "grey90") +
    coord_sf(xlim = c(-2500000, 2500000),
             ylim = c(-2200000, 730000),
             expand = FALSE) +
    labs(title = plot_title, fill = NULL) +
    theme_void() +
    theme(
      plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
      legend.text = element_text(size = 9)
    )
}
# ‣ Output dir -------------------------------------------------
out_dir_clu <- here("figures","phillips_maps",
                    paste0("cluster_", if (VAR=="detail_phillips_refsize") "refsize" else "raw"))
dir.create(out_dir_clu, recursive = TRUE, showWarnings = FALSE)
dev_png <- "png"  # switch to ragg if you like

# ‣ Build + save (cluster‑only) --------------------------------
imap(shapefile_years, function(yr, win) {
  message("Mapping clusters — ", win, "…")

  cl_sf  <- build_cluster_sf(win, yr, ccm)
  dat_cl <- clu_det %>% filter(period == win) %>% select(cluster, !!VAR)
  sf_cl  <- cl_sf %>% left_join(dat_cl, by = "cluster")

  pC <- make_map(sf_cl, VAR, glue("Phillips detail — {if (VAR=='detail_phillips_refsize') 'm=2000' else 'raw'} — {win}"))
  ggsave(filename = file.path(out_dir_clu, glue("cluster_phillips_{if (VAR=='detail_phillips_refsize') 'refsize' else 'raw'}_{win}.png")),
         plot = pC, width = 8, height = 6, dpi = 320, device = dev_png)
})
```
Build public health spending
```{r}
# ──────────────────────────────────────────────────────────────
# Build `fin_all` from fixed-width FinEstDAT (2017/2022) + PID crosswalk
# ──────────────────────────────────────────────────────────────
suppressPackageStartupMessages({
  library(readr); library(dplyr); library(stringr); library(tidyr); library(purrr); library(here)
})

std_fips <- function(x) stringr::str_pad(as.character(x), 5, pad = "0")

# ---- 1) Parser for FinEst Individual Unit fixed-width file ----
# Layout (robustly inferred):
# [govt_id (14 chars)] [item_code (3 chars)] [amount (digits, right-justified)] [year (4)] [flag (1)]
# We parse from the RIGHT to avoid depending on space counts.
parse_finest_fixed <- function(path) {
  stopifnot(file.exists(path))
  lines <- read_lines(path, progress = FALSE)
  # drop blanks
  lines <- lines[nzchar(lines)]
  if (!length(lines)) return(tibble())

  # RIGHT-anchored extraction
  year  <- as.integer(str_sub(lines, -5, -2))
  flag  <- str_sub(lines, -1, -1)
  left1 <- str_sub(lines,  1, -6)                       # everything before year+flag
  # amount is trailing digits on left1
  m <- regexpr("(\\d+)\\s*$", left1, perl = TRUE)
  amt_str <- ifelse(m > 0, regmatches(left1, m), NA_character_)
  amount  <- suppressWarnings(as.numeric(amt_str))
  left2   <- ifelse(m > 0, substr(left1, 1, m - 1L), left1)
  left2   <- rtrim <- sub("\\s+$", "", left2)           # trim right spaces

  # last 3 chars of left2 are the raw item code (alpha+2digits OR 2digits+alpha)
  raw_item <- str_sub(left2, -3, -1)
  govt_id  <- str_sub(left2,  1, -4)

  # Normalize item code to LETTER+2DIGITS (e.g., "E32", "T01")
  item_code <- ifelse(grepl("^[A-Z][0-9]{2}$", raw_item, ignore.case = TRUE),
                      toupper(raw_item),
               ifelse(grepl("^[0-9]{2}[A-Z]$", raw_item, ignore.case = TRUE),
                      paste0(toupper(str_sub(raw_item, -1, -1)), str_sub(raw_item, 1, 2)),
                      toupper(raw_item)))

  tibble(
    govt_id  = govt_id,
    item_code = item_code,
    amount   = amount,
    year     = year,
    flag     = flag
  ) %>%
    filter(!is.na(amount), !is.na(year), nchar(govt_id) >= 10)
}

# ---- 2) PID crosswalk (maps govt_id → county FIPS if available) ----
# PID files vary; try TSV/CSV; look for columns like GOVTID/GOVT_ID and FIPS/GEOID/COUNTYFIPS.
# ──────────────────────────────────────────────────────────────
# Robust PID crosswalk for fixed‑width PID (e.g., Fin_PID_2022.txt)
# Extracts: govt_id (leading digits), county_ihme (stateFIPS + county)
# Lines look like:
# 011003160514BALDWIN COUNTY … 99003   22928722             093022
# ──────────────────────────────────────────────────────────────
read_pid_xwalk <- function(path) {
  if (!file.exists(path)) return(NULL)
  lines <- readr::read_lines(path, progress = FALSE)
  lines <- lines[nzchar(lines)]
  if (!length(lines)) return(NULL)

  # helper: leading digits = GOVTID
  lead_digits <- function(x) sub("^([0-9]+).*$", "\\1", x)

  # find the right-most 5-digit token that starts with "99" (e.g., 99015)
  find_99_code <- function(x) {
    m <- gregexpr("\\b99\\d{3}\\b", x, perl = TRUE)
    if (m[[1]][1] == -1) return(NA_character_)
    # take the last match on the line
    ix <- tail(m[[1]], 1)
    substr(x, ix, ix + attr(m[[1]], "match.length")[length(m[[1]])] - 1)
  }

  tib <- tibble::tibble(raw = lines) %>%
    dplyr::mutate(
      govt_id_raw = lead_digits(raw),
      state_fips  = substr(govt_id_raw, 1, 2),
      code_99     = vapply(raw, find_99_code, character(1)),
      county_ihme = dplyr::if_else(
        !is.na(code_99),
        paste0(state_fips, substr(code_99, 3, 5)),
        NA_character_
      )
    ) %>%
    dplyr::filter(!is.na(county_ihme), nchar(county_ihme) == 5) %>%
    dplyr::transmute(
      govt_id = govt_id_raw,
      county_ihme = stringr::str_pad(county_ihme, 5, pad = "0")
    ) %>%
    dplyr::distinct()

  if (!nrow(tib)) return(NULL)
  tib
}
# ---- 3) Locate files and build fin_all ----
find_first <- function(fname) {
  c(here("data_raw/finance", fname), fname) |> {\(p) p[file.exists(p)][1]}()
}

fin2017_path <- find_first("2017FinEstDAT_09202024modp_pu.txt")
fin2022_path <- find_first("2022FinEstDAT_09202024modp_pu.txt")
pid2017_path <- find_first("Fin_PID_2017.txt")
pid2022_path <- find_first("Fin_PID_2022.txt")

fin_tbls <- list()
if (!is.na(fin2017_path)) fin_tbls <- append(fin_tbls, list(parse_finest_fixed(fin2017_path)))
if (!is.na(fin2022_path)) fin_tbls <- append(fin_tbls, list(parse_finest_fixed(fin2022_path)))
stopifnot(length(fin_tbls) > 0)

fin_raw <- bind_rows(fin_tbls)

# PID crosswalks (optional but recommended for county mapping)
pid_xw <- bind_rows(
  list(read_pid_xwalk(pid2017_path), read_pid_xwalk(pid2022_path)) |> compact()
) %>% distinct()

# ---- 4) Filter to Public Health Expenditures (E32) and summarize by county ----
# item_code "E32" = expenditures, function 32 (Public Health)
fin_e32 <- fin_raw %>% filter(item_code == "E32")

if (nrow(fin_e32) == 0) {
  stop("Parsed finance files but found ZERO rows with item_code == 'E32'. ",
       "Double-check files and item code list; if needed, print a sample of item_code counts.")
}

# Map to counties
if (!nrow(pid_xw)) {
  stop("PID crosswalk not found or had no usable (govt_id, FIPS) columns. ",
       "Please provide Fin_PID_2017.txt / Fin_PID_2022.txt (or their actual paths).")
}

fin_all <- fin_e32 %>%
  left_join(pid_xw, by = "govt_id") %>%
  filter(!is.na(county_ihme)) %>%
  transmute(
    county_ihme = county_ihme,
    fin_year    = as.integer(year),
    ph_exp_total = as.numeric(amount)
  ) %>%
  group_by(county_ihme, fin_year) %>%
  summarise(ph_exp_total = sum(ph_exp_total, na.rm = TRUE), .groups = "drop")

# Sanity check
cat("# fin_all rows:", nrow(fin_all), "\n")
print(fin_all %>% count(fin_year, sort = TRUE))
```
Build pop and other needed things for next chunk
```{r}
# ──────────────────────────────────────────────────────────────
# REPAIR BLOCK — ensure components + build pop_join
# Run this ONCE before the PH-spend plotting code
# ──────────────────────────────────────────────────────────────
suppressPackageStartupMessages({
  library(dplyr); library(tidyr); library(stringr); library(purrr)
})

std_fips <- function(x) stringr::str_pad(as.character(x), 5, pad = "0")
as_county_ihme <- function(df) {
  nm <- names(df)
  id_col <- dplyr::case_when(
    "county_ihme" %in% nm ~ "county_ihme",
    "GEOID"       %in% nm ~ "GEOID",
    "fips"        %in% nm ~ "fips",
    "FIPS"        %in% nm ~ "FIPS",
    "countyrs"    %in% nm ~ "countyrs",
    TRUE ~ NA_character_
  )
  if (is.na(id_col)) stop("No county ID column found. Expect one of county_ihme/GEOID/fips/FIPS/countyrs.")
  df %>% mutate(county_ihme = std_fips(.data[[id_col]]))
}

# 1) Ensure direction_year has component z-cols
ensure_direction_components <- function(direction_year, df) {
  needed_comp <- c("z_DQ_prop_garbage","z_prop_light","z_pct_overd_miss","z_pct_acc_miss")
  if (all(needed_comp %in% names(direction_year))) return(direction_year)

  # We need the raw inputs in `df` to (re)compute z-scores
  req <- c("DQ_prop_garbage","prop_light","pct_overd_miss","pct_acc_miss","year")
  if (!exists("df") || !all(req %in% names(df))) {
    stop("direction_year lacks z-components and `df` is missing required columns: ",
         paste(setdiff(req, names(df)), collapse = ", "))
  }
  df <- as_county_ihme(df)

  # Compute global means/sds once
  gs <- df %>%
    summarise(across(
      c(DQ_prop_garbage, prop_light, pct_overd_miss, pct_acc_miss),
      list(mean = ~mean(.x, na.rm = TRUE), sd = ~sd(.x, na.rm = TRUE)),
      .names = "{.col}_{.fn}"
    ))
  z <- function(x, m, s) (x - m) / s

  # Join raw inputs to direction_year, then add missing z-cols
  direction_year %>%
    left_join(df %>% select(county_ihme, year,
                            DQ_prop_garbage, prop_light, pct_overd_miss, pct_acc_miss),
              by = c("county_ihme","year")) %>%
    mutate(
      z_DQ_prop_garbage = if (!"z_DQ_prop_garbage" %in% names(direction_year))
                            z(DQ_prop_garbage, gs$DQ_prop_garbage_mean, gs$DQ_prop_garbage_sd)
                          else z_DQ_prop_garbage,
      z_prop_light      = if (!"z_prop_light" %in% names(direction_year))
                            z(prop_light, gs$prop_light_mean, gs$prop_light_sd)
                          else z_prop_light,
      z_pct_overd_miss  = if (!"z_pct_overd_miss" %in% names(direction_year))
                            z(pct_overd_miss, gs$pct_overd_miss_mean, gs$pct_overd_miss_sd)
                          else z_pct_overd_miss,
      z_pct_acc_miss    = if (!"z_pct_acc_miss" %in% names(direction_year))
                            z(pct_acc_miss, gs$pct_acc_miss_mean, gs$pct_acc_miss_sd)
                          else z_pct_acc_miss
    ) %>%
    select(-DQ_prop_garbage, -prop_light, -pct_overd_miss, -pct_acc_miss)
}

direction_year <- ensure_direction_components(direction_year, df)

# 2) Build pop_join (ACS preferred; pid_all fallback)
build_pop_join <- function(fin_years) {
  # Reuse if already valid
  if (exists("pop_join", inherits = TRUE)) {
    pj <- get("pop_join", inherits = TRUE)
    if (all(c("county_ihme","fin_year","pop") %in% names(pj))) return(pj)
  }

  # Try ACS via tidycensus
  if (requireNamespace("tidycensus", quietly = TRUE)) {
    message("Building pop_join from ACS (B01001_001, ACS5) …")
    out <- purrr::map_dfr(sort(unique(stats::na.omit(as.integer(fin_years)))), function(y) {
      yy <- max(2009L, min(2023L, as.integer(y)))  # ACS5 window
      tidycensus::get_acs(
        geography = "county", variables = "B01001_001",
        year = yy, survey = "acs5", cache_table = TRUE, show_call = FALSE
      ) %>%
        transmute(county_ihme = GEOID, fin_year = y, pop = estimate)
    })
    if (nrow(out)) return(out)
  }

  # Fallback: pid_all (must exist and have year/pop)
  if (exists("pid_all", inherits = TRUE)) {
    message("Falling back to pid_all for population …")
    pid <- get("pid_all", inherits = TRUE)
    stopifnot(all(c("county_ihme","year","pop") %in% names(pid)))
    return(pid %>% transmute(county_ihme, fin_year = as.integer(year), pop))
  }

  stop("Could not build pop_join: neither ACS nor pid_all available.")
}

# Need fin_years from fin_all
if (!exists("fin_all", inherits = TRUE)) stop("fin_all not found — run the finance builder first.")
fin_all <- as_county_ihme(fin_all)
stopifnot(all(c("county_ihme","fin_year","ph_exp_total") %in% names(fin_all)))

avail_fin_years <- sort(unique(stats::na.omit(as.integer(fin_all$fin_year))))
if (!length(avail_fin_years)) stop("No valid fin_year values in fin_all.")

pop_join <- build_pop_join(avail_fin_years)

# Finally: build ph_pc for downstream chunk
ph_pc <- fin_all %>%
  filter(!is.na(fin_year)) %>%
  left_join(pop_join, by = c("county_ihme","fin_year")) %>%
  mutate(ph_pc = ph_exp_total / pop) %>%
  filter(is.finite(ph_pc))
```
Validation: direction score by public health spending
```{r}
# ──────────────────────────────────────────────────────────────
# 12 · Robust: build direction_year (if missing) + PH spend time series
#     (now also plots each component of the direction score)
# ──────────────────────────────────────────────────────────────
suppressPackageStartupMessages({
  library(dplyr); library(readr); library(stringr); library(tidyr)
  library(ggplot2); library(scales); library(purrr); library(here)
})

options(tigris_use_cache = TRUE)

# -- Helper: standardize to 5-digit county_ihme ----------------
std_fips <- function(x) stringr::str_pad(as.character(x), 5, pad = "0")
as_county_ihme <- function(df) {
  nm <- names(df)
  id_col <- dplyr::case_when(
    "county_ihme" %in% nm ~ "county_ihme",
    "GEOID"       %in% nm ~ "GEOID",
    "fips"        %in% nm ~ "fips",
    "FIPS"        %in% nm ~ "FIPS",
    "countyrs"    %in% nm ~ "countyrs",
    TRUE ~ NA_character_
  )
  if (is.na(id_col)) stop("No county ID column found. Expect one of county_ihme/GEOID/fips/FIPS/countyrs.")
  df %>% mutate(county_ihme = std_fips(.data[[id_col]]))
}

# -- 0) Build direction_year if it doesn't exist ----------------
if (!exists("direction_year")) {
  stopifnot(exists("df"))
  needed <- c("DQ_prop_garbage","prop_light","pct_overd_miss","pct_acc_miss","year")
  if (!all(needed %in% names(df))) stop("df is missing required columns: ", paste(setdiff(needed, names(df)), collapse=", "))
  df <- as_county_ihme(df)

  global_stats <- df %>%
    summarise(across(
      c(DQ_prop_garbage, prop_light, pct_overd_miss, pct_acc_miss),
      list(mean = ~mean(.x, na.rm = TRUE),
           sd   = ~sd(.x, na.rm = TRUE)),
      .names = "{.col}_{.fn}"
    ))
  zscore_global <- function(x, m, s) (x - m) / s

  direction_year <- df %>%
    filter(year >= 1999, year <= 2022) %>%
    mutate(
      z_DQ_prop_garbage  = zscore_global(DQ_prop_garbage, global_stats$DQ_prop_garbage_mean, global_stats$DQ_prop_garbage_sd),
      z_prop_light       = zscore_global(prop_light,      global_stats$prop_light_mean,      global_stats$prop_light_sd),
      z_pct_overd_miss   = zscore_global(pct_overd_miss,  global_stats$pct_overd_miss_mean,  global_stats$pct_overd_miss_sd),
      z_pct_acc_miss     = zscore_global(pct_acc_miss,    global_stats$pct_acc_miss_mean,    global_stats$pct_acc_miss_sd),
      direction_score    = (z_prop_light + z_pct_overd_miss + z_pct_acc_miss + z_DQ_prop_garbage) / 4
    ) %>%
    # KEEP component columns (changed from your original select)
    select(county_ihme, year, direction_score,
           z_DQ_prop_garbage, z_prop_light, z_pct_overd_miss, z_pct_acc_miss) %>%
    filter(!is.na(county_ihme))
}

# -- 1) Finance + population for per-capita ---------------------
fin_all <- as_county_ihme(fin_all)

# Assume you already computed pop_join upstream (ACS or PID fallback)
# (If not, plug in your existing pop_join block here.)

ph_pc <- fin_all %>%
  filter(!is.na(fin_year)) %>%
  left_join(pop_join, by = c("county_ihme","fin_year")) %>%
  mutate(ph_pc = ph_exp_total / pop) %>%
  filter(is.finite(ph_pc))

# --- helpers ---------------------------------------------------
nearest_fin_year <- function(y, avail) {
  if (!length(avail) || is.na(y)) return(NA_integer_)
  avail[ which.min(abs(avail - y)) ]
}
safe_quintile <- function(x) {
  labs <- c("Q1 lowest","Q2","Q3","Q4","Q5 highest")
  v <- x[is.finite(x)]
  if (length(v) < 5L || length(unique(v)) < 5L) {
    return(factor(rep(NA_character_, length(x)), levels = labs))
  }
  qs <- quantile(v, probs = seq(0, 1, 0.2), na.rm = TRUE, names = FALSE, type = 7)
  qs[1] <- min(v) - 1e-9; qs[length(qs)] <- max(v) + 1e-9
  cut(x, breaks = qs, include.lowest = TRUE, right = FALSE, labels = labs)
}

avail_fin_years <- sort(unique(na.omit(fin_all$fin_year)))
if (length(avail_fin_years) == 0L) stop("No valid fin_year values in fin_all.")

# --- build dir_ph with guarded quintiles -----------------------
dir_ph <- direction_year %>%
  mutate(fin_year = vapply(year, nearest_fin_year, integer(1), avail = avail_fin_years)) %>%
  left_join(ph_pc, by = c("county_ihme","fin_year")) %>%
  group_by(fin_year) %>%
  mutate(ph_quintile = safe_quintile(ph_pc)) %>%
  ungroup()

# ---- A) Original overall time series by spend quintile --------
ts_ph <- dir_ph %>%
  filter(!is.na(ph_quintile)) %>%
  group_by(year, ph_quintile) %>%
  summarise(avg_direction = mean(direction_score, na.rm = TRUE), .groups = "drop")

g_ph <- ggplot(ts_ph, aes(year, avg_direction, colour = ph_quintile)) +
  geom_line(linewidth = 1) +
  labs(title = "Direction-score trend by public health spend quintile",
       subtitle = paste0("Per-capita (func 32), snapped to nearest finance year: ",
                         paste(avail_fin_years, collapse = ", ")),
       x = NULL, y = "Mean direction score", colour = "Spend quintile") +
  scale_y_continuous(labels = number_format(accuracy = 0.01)) +
  theme_bw()

ggsave(here("figures", "timeseries_direction_by_ph_spend_quintile.png"),
       g_ph, width = 7, height = 4, dpi = 300)
print(g_ph)

# ---- B) NEW: Component time series by spend quintile ----------
component_lookup <- c(
  z_DQ_prop_garbage = "z(DQ_prop_garbage)",
  z_prop_light      = "z(prop_light)",
  z_pct_overd_miss  = "z(pct_overdose_unspecified)",
  z_pct_acc_miss    = "z(pct_accident_unspecified)"
)

ts_ph_comp <- dir_ph %>%
  filter(!is.na(ph_quintile)) %>%
  pivot_longer(
    cols = c(z_DQ_prop_garbage, z_prop_light, z_pct_overd_miss, z_pct_acc_miss),
    names_to = "component", values_to = "z_value"
  ) %>%
  mutate(component = recode(component, !!!component_lookup)) %>%
  group_by(year, ph_quintile, component) %>%
  summarise(avg_z = mean(z_value, na.rm = TRUE), .groups = "drop")

g_ph_comp <- ggplot(ts_ph_comp, aes(year, avg_z, colour = ph_quintile)) +
  geom_line(linewidth = 0.9) +
  facet_wrap(~ component, ncol = 2, scales = "free_y") +
  labs(title = "Component trends by public health spend quintile",
       subtitle = "Each panel is a z-scored component of the direction score",
       x = NULL, y = "Mean z-score", colour = "Spend quintile") +
  theme_bw()

ggsave(here("figures", "timeseries_components_by_ph_spend_quintile.png"),
       g_ph_comp, width = 8.5, height = 6.5, dpi = 300)
print(g_ph_comp)

# ---- C) Cross-section R² (county averages) vs spend -----------
if (!exists("res")) {
  res <- direction_year %>%
    group_by(county_ihme) %>%
    summarise(direction_score = mean(direction_score, na.rm = TRUE), .groups = "drop")
}
res <- as_county_ihme(res)

latest_fin <- max(avail_fin_years)
var_ph <- res %>%
  select(county_ihme, direction_score) %>%
  left_join(ph_pc %>% filter(fin_year == latest_fin) %>% select(county_ihme, ph_pc),
            by = "county_ihme") %>%
  filter(is.finite(direction_score), is.finite(ph_pc)) %>%
  mutate(log_ph_pc = log10(pmax(ph_pc, 1e-6)))

fit_ph <- lm(direction_score ~ log_ph_pc, data = var_ph)
r2_ph <- summary(fit_ph)$r.squared
cat("Public health spend R² (overall):", sprintf("%.3f", r2_ph),
    "→", round(r2_ph*100,1), "% of variance explained by log10(per-capita PH spend).\n")

# ---- D) NEW: Cross-section R² for each component vs spend ----
# Build county averages for components
comp_avg <- direction_year %>%
  group_by(county_ihme) %>%
  summarise(across(c(z_DQ_prop_garbage, z_prop_light, z_pct_overd_miss, z_pct_acc_miss),
                   ~ mean(.x, na.rm = TRUE), .names = "{.col}"),
            .groups = "drop") %>%
  left_join(ph_pc %>% filter(fin_year == latest_fin) %>% select(county_ihme, ph_pc),
            by = "county_ihme") %>%
  filter(is.finite(ph_pc)) %>%
  mutate(log_ph_pc = log10(pmax(ph_pc, 1e-6)))

for (nm in c("z_DQ_prop_garbage","z_prop_light","z_pct_overd_miss","z_pct_acc_miss")) {
  ff <- as.formula(paste(nm, "~ log_ph_pc"))
  r2 <- summary(lm(ff, data = comp_avg))$r.squared
  cat(sprintf("Public health spend R² (%s): %.3f (%.1f%%)\n",
              component_lookup[[nm]], r2, 100*r2))
}
```
Direction score by reporting type
```{r}
# ──────────────────────────────────────────────────────────────
# 5) Reporting type (ME/Coroner/Mixed/etc.) — time series & R²
#     (now also plots each component of the direction score)
# ──────────────────────────────────────────────────────────────

# 5.1 · Read reporting-type lookup (robust column detection)
reporting_path_opts <- c(
  here("data_raw", "County-Death-Investigation-System-2018-1-9-2024.csv"),
  "/mnt/data/County-Death-Investigation-System-2018-1-9-2024.csv"
)
reporting_path <- reporting_path_opts[file.exists(reporting_path_opts)][1]
if (is.na(reporting_path)) stop("Could not find the reporting-type CSV at either default path.")

rep_raw <- readr::read_csv(reporting_path, show_col_types = FALSE)

get_colname <- function(df, patterns) {
  nm <- names(df)
  hits <- which(Reduce(`|`, lapply(patterns, function(p) grepl(p, nm, ignore.case = TRUE))))
  if (length(hits) == 0) return(NA_character_)
  nm[hits[1]]
}

fips_col <- get_colname(rep_raw, c("^fips$", "^fips_?code$", "geoid", "county_?fips", "countyrs", "fips.*5"))
type_col <- get_colname(rep_raw, c("reporting.*type", "investigation.*type", "death.*investigation.*system", "^type$", "system"))

if (is.na(fips_col) || is.na(type_col)) {
  message("Columns in reporting CSV:\n- ", paste(names(rep_raw), collapse = "\n- "))
  stop("Could not detect FIPS and/or reporting-type column. ",
       "Set them manually, e.g.: fips_col <- 'FIPS'; type_col <- 'Reporting Type'")
}

message("Using columns → FIPS: '", fips_col, "' | reporting type: '", type_col, "'")

rep_lu <- rep_raw %>%
  mutate(
    county_ihme    = std_fips(.data[[fips_col]]),
    reporting_type = trimws(as.character(.data[[type_col]]))
  ) %>%
  filter(!is.na(county_ihme), nchar(county_ihme) == 5, !is.na(reporting_type), reporting_type != "") %>%
  mutate(
    reporting_type = dplyr::recode(tolower(reporting_type),
      "medical examiner" = "Medical Examiner",
      "me"               = "Medical Examiner",
      "coroner"          = "Coroner",
      "mixed"            = "Mixed",
      "hybrid"           = "Mixed",
      .default = stringr::str_to_title(reporting_type)
    )
  ) %>%
  select(county_ihme, reporting_type) %>%
  distinct()

print(rep_lu %>% count(reporting_type, sort = TRUE))

# 5.2 · Time series: mean direction_score by reporting type -----
ts_rep <- direction_year %>%
  left_join(rep_lu, by = "county_ihme") %>%
  filter(!is.na(reporting_type)) %>%
  group_by(year, reporting_type) %>%
  summarise(avg_direction = mean(direction_score, na.rm = TRUE), .groups = "drop")

g_rep <- ggplot(ts_rep, aes(year, avg_direction, colour = reporting_type)) +
  geom_line(linewidth = 1) +
  labs(
    title = "Direction-score trend by death investigation reporting type",
    x = NULL, y = "Mean direction score", colour = "Reporting type"
  ) +
  scale_y_continuous(labels = scales::number_format(accuracy = 0.01)) +
  theme_bw()

ggsave(here("figures", "timeseries_direction_by_reporting_type.png"),
       g_rep, width = 7, height = 4, dpi = 300)
print(g_rep)

# 5.2b · NEW: Components by reporting type (faceted) -----------
component_lookup <- c(
  z_DQ_prop_garbage = "z(DQ_prop_garbage)",
  z_prop_light      = "z(prop_light)",
  z_pct_overd_miss  = "z(pct_overdose_unspecified)",
  z_pct_acc_miss    = "z(pct_accident_unspecified)"
)

ts_rep_comp <- direction_year %>%
  left_join(rep_lu, by = "county_ihme") %>%
  filter(!is.na(reporting_type)) %>%
  pivot_longer(
    cols = c(z_DQ_prop_garbage, z_prop_light, z_pct_overd_miss, z_pct_acc_miss),
    names_to = "component", values_to = "z_value"
  ) %>%
  mutate(component = recode(component, !!!component_lookup)) %>%
  group_by(year, reporting_type, component) %>%
  summarise(avg_z = mean(z_value, na.rm = TRUE), .groups = "drop")

g_rep_comp <- ggplot(ts_rep_comp, aes(year, avg_z, colour = reporting_type)) +
  geom_line(linewidth = 0.9) +
  facet_wrap(~ component, ncol = 2, scales = "free_y") +
  labs(
    title = "Component trends by death investigation reporting type",
    subtitle = "Each panel is a z-scored component of the direction score",
    x = NULL, y = "Mean z-score", colour = "Reporting type"
  ) +
  theme_bw()

ggsave(here("figures", "timeseries_components_by_reporting_type.png"),
       g_rep_comp, width = 8.5, height = 6.5, dpi = 300)
print(g_rep_comp)

# 5.3 · Cross-section R² of county averages vs reporting type --
var_rep <- (if (exists("res")) res else {
  direction_year %>% group_by(county_ihme) %>%
    summarise(direction_score = mean(direction_score, na.rm = TRUE), .groups = "drop")
}) %>%
  left_join(rep_lu, by = "county_ihme") %>%
  filter(!is.na(reporting_type))

if (nrow(var_rep) > 0) {
  fit_rep <- lm(direction_score ~ reporting_type, data = var_rep)
  r2_rep  <- summary(fit_rep)$r.squared
  cat("Reporting-type R² (overall):", sprintf("%.3f", r2_rep),
      "→", round(r2_rep * 100, 1), "% of variance explained.\n")
} else {
  message("No overlap between county averages and reporting-type lookup.")
}

# 5.3b · NEW: Component R² vs reporting type -------------------
comp_rep <- direction_year %>%
  group_by(county_ihme) %>%
  summarise(across(c(z_DQ_prop_garbage, z_prop_light, z_pct_overd_miss, z_pct_acc_miss),
                   ~ mean(.x, na.rm = TRUE), .names = "{.col}"),
            .groups = "drop") %>%
  left_join(rep_lu, by = "county_ihme") %>%
  filter(!is.na(reporting_type))

if (nrow(comp_rep) > 0) {
  for (nm in c("z_DQ_prop_garbage","z_prop_light","z_pct_overd_miss","z_pct_acc_miss")) {
    ff <- as.formula(paste(nm, "~ reporting_type"))
    r2 <- summary(lm(ff, data = comp_rep))$r.squared
    cat(sprintf("Reporting-type R² (%s): %.3f (%.1f%%)\n",
                component_lookup[[nm]], r2, 100*r2))
  }
}
```
Correlation between the metrics
```{r}
# ──────────────────────────────────────────────────────────────
# Correlation between z-score metrics (overall, across all county-years)
# ──────────────────────────────────────────────────────────────
library(dplyr)

# Make sure these columns exist
z_cols <- c("z_DQ_prop_garbage", "z_prop_light", "z_pct_overd_miss", "z_pct_acc_miss")
stopifnot(all(z_cols %in% names(direction_year)))

# Pearson correlation
cor_pearson <- cor(direction_year %>% select(all_of(z_cols)),
                   use = "pairwise.complete.obs", method = "pearson")

# Spearman correlation
cor_spearman <- cor(direction_year %>% select(all_of(z_cols)),
                    use = "pairwise.complete.obs", method = "spearman")

cat("Pearson correlations:\n")
print(round(cor_pearson, 3))

cat("\nSpearman correlations:\n")
print(round(cor_spearman, 3))
```








