---
title: "Mapping Notebook"
output: html_notebook
---

Temporality stable groupings
```{r}
# ──────────────────────────────────────────────────────────────
# 0 · Setup – packages, paths, helpers
# ──────────────────────────────────────────────────────────────
required <- c("sf", "tigris", "readr", "dplyr", "stringr", "tidyr",
              "purrr", "ggplot2", "viridis", "patchwork")
missing <- setdiff(required, rownames(installed.packages()))
if (length(missing)) install.packages(missing)
invisible(lapply(required, library, character.only = TRUE))
options(tigris_use_cache = TRUE, tigris_class = "sf")

comp_vars_pct <- c("pct_marstat_comp_k", "pct_placdth_comp_k", "pct_educ_comp_k", "pct_mandeath_comp_k","pct_age_comp_k", "pct_sex_comp_k", "pct_race_comp_k", "DQ_overall")

vars_to_map <- c(
  "prop_garbage", "prop_light",  "pct_acc_miss", "pct_overd_miss",  "pct_gc_I64", "pct_gc_C_misc", "pct_gc_I10", "pct_gc_R_misc", "pct_gc_N19", "pct_gc_J80", comp_vars_pct
)

# ──────────────────────────────────────────────────────────────
# 2 · Load & preprocess county-year data
# ──────────────────────────────────────────────────────────────
dq <- read_csv(here::here("data", "county_year_quality_metrics.csv"), show_col_types = FALSE)

dq <- dq |>
  mutate(
    year        = as.integer(year),
    time_window = case_when(
      year >= 1999 & year <= 2004 ~ "1999_2004",
      year >= 2005 & year <= 2010 ~ "2005_2010",
      year >= 2011 & year <= 2017 ~ "2011_2017",
      year >= 2020 & year <= 2022 ~ "2020_2022",
      TRUE ~ NA_character_
    )
  )

dq <- dq |>
  mutate(across(
    ends_with("_comp_k"),
    ~ 100 * (.x / n_cert),
    .names = "pct_{.col}"
  ))


# ──────────────────────────────────────────────────────────────
# 5 · 5-year averages by IHME stable FIPS
# ──────────────────────────────────────────────────────────────
dq_avg <- dq %>%
  group_by(fips_ihme, time_window) %>%
  summarise(
    across(all_of(vars_to_map), ~ mean(.x, na.rm = TRUE)),
    n_years = n(),
    .groups = "drop"
  )

# ──────────────────────────────────────────────────────────────
# 6 · Time-window-specific shapefiles + merge with dq_avg
# ──────────────────────────────────────────────────────────────
crs_proj <- 2163
shapefile_years <- c("1999_2004" = 2000, "2005_2010" = 2010, "2011_2017" = 2015, "2020_2022" = 2020)

# Helper: shift and scale
st_shift <- function(x, offset) {
  st_geometry(x) <- st_geometry(x) + offset
  x
}

st_scale <- function(x, factor) {
  centroid <- st_centroid(st_union(x))
  st_geometry(x) <- (st_geometry(x) - centroid) * factor + centroid
  x
}

county_shapes_by_window <- purrr::map(shapefile_years, function(year) {
  us_counties <- counties(year = year, cb = TRUE, class = "sf") |>
    st_zm(drop = TRUE, what = "ZM") |>
    st_transform(crs_proj)
  
  if ("GEOID" %in% names(us_counties)) {
    us_counties <- us_counties |> mutate(countyrs = GEOID)
  } else {
    us_counties <- us_counties |> mutate(
      countyrs = paste0(
        str_pad(as.character(STATE), 2, pad = "0"),
        str_pad(as.character(COUNTY), 3, pad = "0")
      )
    )
  }

  us_counties <- us_counties |>
    select(countyrs, geometry) |>
    left_join(ihme_xwalk, by = c("countyrs" = "fips")) |>
    mutate(
      countyrs = as.character(countyrs),
      stable_fips = coalesce(stable_fips, countyrs)
    ) |>
    group_by(stable_fips) |>
    summarise(geometry = st_union(geometry), .groups = "drop") |>
    st_set_crs(crs_proj)

  alaska <- us_counties |> filter(substr(stable_fips, 1, 2) == "02") |>
    st_scale(0.4) |> st_shift(c(1300000, -4900000)) |> st_set_crs(crs_proj)

  hawaii <- us_counties |> filter(substr(stable_fips, 1, 2) == "15") |>
    st_scale(1.5) |> st_shift(c(5200000, -1400000)) |> st_set_crs(crs_proj)

  mainland <- us_counties |> filter(!substr(stable_fips, 1, 2) %in% c("02", "15", "72"))

  bind_rows(mainland, alaska, hawaii)
})

# Join to dq_avg for each time period
joined_by_period <- map2(
  county_shapes_by_window,
  names(county_shapes_by_window),
  ~ left_join(.x, dq_avg %>% filter(time_window == .y), by = "fips_ihme")
)

# ──────────────────────────────────────────────────────────────
# 7 · Mapping helper (identical to your original)
# ──────────────────────────────────────────────────────────────
make_map <- function(sf_data, var, title = NULL, palette = "RdBu") {
  states <- tigris::states(cb = TRUE, class = "sf") |> st_transform(2163)

  fill_scale <- if (var == "prop_garbage") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0.2, 0.4),
    oob = scales::squish,
    direction = -1,
    na.value = "grey90"
  )
} else if (var == "prop_light") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0.20, 0.40),
    oob = scales::squish,
    direction = -1,
    na.value = "grey90"
  )
} else if (var == "pct_overd_miss") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0, 0.75),
    oob = scales::squish,
    direction = -1,
    na.value = "grey90"
  )
} else if (var == "direction_score") {
  scale_fill_distiller(
    palette = palette,
    limits = c(-1,1),
    oob = scales::squish,
    direction = -1,
    na.value = "grey90"
  )
} else if (var == "DQ_overall") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0,0.40),
    oob = scales::squish,
    direction = 1,
    na.value = "grey90"
  )
} else if (var == "pct_acc_miss") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0, 0.75),
    oob = scales::squish,
    direction = -1,
    na.value = "grey90"
  )
} else if (var == "pct_mandeath_comp_k") {
  scale_fill_distiller(
    palette = palette,
    limits = c(5, 20),
    oob = scales::squish,
    direction = 1,
    na.value = "grey90"
  )
} else if (var == "pct_placdth_comp_k") {
  scale_fill_distiller(
    palette = palette,
    limits = c(98, 100),
    oob = scales::squish,
    direction = 1,
    na.value = "grey90"
  )
} else if (var == "pct_educ_comp_k") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0, 100),
    oob = scales::squish,
    direction = 1,
    na.value = "grey90"
  )
} else if (var == "pct_marstat_comp_k") {
  scale_fill_distiller(
    palette = palette,
    limits = c(97, 100),
    oob = scales::squish,
    direction = 1,
    na.value = "grey90"
  )
} else if (var %in% c("pct_age_comp_k", "pct_sex_comp_k", "pct_race_comp_k")) {
  scale_fill_distiller(
    palette = palette,
    limits = c(99.5, 100),
    oob = scales::squish,
    direction = 1,
    na.value = "grey90"
  )
} else if (var == "pct_gc_I64") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0.01, 0.05),
    oob = scales::squish,
    direction = -1,
    na.value = "grey90"
  )
} else if (var == "pct_gc_C_misc") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0.005, 0.03),
    oob = scales::squish,
    direction = -1,
    na.value = "grey90"
  )
} else if (var == "pct_gc_I10") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0, 0.02),
    oob = scales::squish,
    direction = -1,
    na.value = "grey90"
  )
} else if (var == "pct_gc_R_misc") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0, 0.03),
    oob = scales::squish,
    direction = -1,
    na.value = "grey90"
  )
} else if (var == "pct_gc_N19") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0, 0.025),
    oob = scales::squish,
    direction = -1,
    na.value = "grey90"
  )
} else if (var == "pct_gc_J80") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0, 0.005),
    oob = scales::squish,
    direction = -1,
    na.value = "grey90"
  )
} else if (startsWith(var, "pct_") || var == "overall_completeness_pct") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0, 100),
    oob = scales::squish,
    direction = 1,
    na.value = "grey90"
  )
} else {
  scale_fill_distiller(
    palette = palette,
    oob = scales::squish,
    na.value = "grey90",
    direction = -1
  )
}

  ggplot() +
    geom_sf(data = sf_data, aes(fill = .data[[var]]), colour = NA) +
    geom_sf(data = states, fill = NA, colour = "white", linewidth = 0.3) +
    fill_scale +
    coord_sf(xlim = c(-2500000, 2500000),
             ylim = c(-2200000, 730000),
             expand = FALSE) +
    labs(title = title %||% var, fill = NULL) +
    theme_void() +
    theme(
      plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
      legend.text = element_text(size = 9)
    )
}

# ──────────────────────────────────────────────────────────────
# 8 · Save maps
# ──────────────────────────────────────────────────────────────
output_dir <- "/Users/amymann/Documents/Data Quality Project/data_quality/figures/5yr_avg_stable"
dir.create(output_dir, showWarnings = FALSE, recursive = TRUE)

walk(vars_to_map, function(var) {
  walk(names(joined_by_period), function(win) {
    map_plot <- make_map(joined_by_period[[win]], var, paste(var, win))
    ggsave(
      filename = file.path(output_dir, paste0(var, "_", win, ".png")),
      plot     = map_plot,
      width    = 8, height = 6, dpi = 320
    )
  })
})

```
Relationship between rural and garbage
```{r}
# 0 · Load packages -------------------------------------------------------
library(dplyr)
library(stringr)
library(ggplot2)
library(scales)
library(tigris)
library(tidycensus)
library(readr)
library(here)

# 1 · Load your data ------------------------------------------------------
dq <- read_csv(here("data", "county_year_quality_metrics.csv"),
               show_col_types = FALSE)

# 2 · Build FIPS lookup ---------------------------------------------------
fips_lookup <- fips_codes %>%
  mutate(
    county_code_3 = sprintf("%03d", as.integer(county_code)),
    custom_code   = paste0(state, county_code_3),
    GEOID         = paste0(state_code, county_code)
  ) %>%
  select(custom_code, GEOID)

# 3 · Convert county codes to GEOID ---------------------------------------
dq_all <- dq %>%
  mutate(
    GEOID = case_when(
      str_detect(countyrs, "^[0-9]{5}$") ~ countyrs,
      str_detect(countyrs, "^[A-Z]{2}[0-9]{3}$") ~ fips_lookup$GEOID[match(countyrs, fips_lookup$custom_code)],
      TRUE ~ NA_character_
    )
  )

cat("Counties without valid GEOID:", sum(is.na(dq_all$GEOID)), "\n")

# 4 · Get 2022 population estimates ---------------------------------------
census_api_key("671b64055fc192103cbc199d2dff91b46d9cc781", overwrite = FALSE)

pop22 <- get_acs(
  geography = "county",
  variables = "B01003_001",
  year = 2022,
  survey = "acs5"
) %>%
  select(GEOID, pop_2022 = estimate)

# 5 · Define county size buckets ------------------------------------------
cuts   <- c(0, 50000, 100000, 250000, 1000000, Inf)
labels <- c("<50k", "50–100k", "100–250k", "250k–1M", "≥1M")

county_sizes <- pop22 %>%
  mutate(size_bucket = cut(pop_2022, breaks = cuts, labels = labels, right = FALSE)) %>%
  select(GEOID, pop_2022, size_bucket)

# 6 · Join population and size to main data -------------------------------
dq_all <- dq_all %>%
  left_join(pop22, by = "GEOID") %>%
  left_join(county_sizes %>% select(GEOID, size_bucket), by = "GEOID")

# 7 · Indicators to plot --------------------------------------------------
indicators <- c(
  "prop_garbage",
  "prop_light",
  "pct_overd_miss",
  "pct_acc_miss")

# 8 · Time trends by size bucket ------------------------------------------
for (var in indicators) {
  ts_df <- dq_all %>%
    filter(!is.na(size_bucket)) %>%
    group_by(year, size_bucket) %>%
    summarise(avg_prop = mean(.data[[var]], na.rm = TRUE), .groups = "drop")
  
  g <- ggplot(ts_df, aes(year, avg_prop, colour = size_bucket)) +
    geom_line(linewidth = 1) +
    scale_y_continuous(labels = percent_format(accuracy = 0.1)) +
    labs(
      title = paste("Trend of", gsub("_", " ", var), "by county size"),
      x = NULL,
      y = "Mean percentage",
      colour = "2022 size bucket"
    ) +
    theme_bw()
  
  print(g)
  
  ggsave(
    filename = here("figures", paste0("trend_", var, "_by_size_bucket.png")),
    plot = g,
    width = 7, height = 4, dpi = 300
  )
}

# 9 · Scatterplots: pop vs indicator (2022) -------------------------------
scatter_data <- dq_all %>%
  filter(year == 2022, !is.na(pop_2022))

for (var in indicators) {
  scatter_df <- scatter_data %>%
    filter(!is.na(.data[[var]]))
  
  g <- ggplot(scatter_df, aes(x = pop_2022, y = .data[[var]])) +
    geom_point(alpha = 0.6) +
    scale_x_log10(labels = comma) +
    scale_y_continuous(labels = percent_format(accuracy = 0.1)) +
    labs(
      title = paste("County population vs", gsub("_", " ", var), "(2022)"),
      x = "County population (log10 scale)",
      y = paste("Proportion:", gsub("_", " ", var))
    ) +
    theme_bw()
  
  print(g)
  
  ggsave(
    filename = here("figures", paste0("scatter_", var, "_vs_population_2022.png")),
    plot = g,
    width = 7, height = 4, dpi = 300
  )
}
```
Clustering or anomaly detection: Uses unsupervised ML (clustering, autoencoders) to identify unusual records or patterns. If a county’s records are more often flagged as outliers, that suggests quality issues.
```{r}
# ------------------------------------------------------------------------------
# Title: County-Level Anomaly Detection for US Mortality Data Quality
#         (Average of 1999–2022, using IHME stable FIPS)
# Date: Sys.Date()
# Description: For each county, averages key data-quality indicators across
#              years 1999–2022, harmonizes FIPS naming, and applies unsupervised
#              anomaly detection (Isolation Forest + LOF).
# ------------------------------------------------------------------------------

# 0.  PACKAGES ------------------------------------------------------------------
library(tidyverse)
library(solitude)
library(dbscan)
library(factoextra)
library(patchwork)
library(here)

# 1.  LOAD IHME CROSSWALK ------------------------------------------------------
cw_path <- here("data_raw", "ihme_fips.rda")
cw_names <- load(cw_path)
ihme_xwalk <- get(cw_names[1]) %>%
  transmute(
    fips = str_pad(as.character(orig_fips), 5, pad = "0"),
    stable_fips = str_pad(as.character(ihme_fips), 5, pad = "0")
  ) %>%
  distinct()

# State lookups for pre-/post-2003 naming conventions
state_fips_lookup <- tibble::tribble(
  ~state_abbr, ~state_fips,
  "AL", "01", "AK", "02", "AZ", "04", "AR", "05", "CA", "06", "CO", "08", "CT", "09",
  "DE", "10", "DC", "11", "FL", "12", "GA", "13", "HI", "15", "ID", "16", "IL", "17",
  "IN", "18", "IA", "19", "KS", "20", "KY", "21", "LA", "22", "ME", "23", "MD", "24",
  "MA", "25", "MI", "26", "MN", "27", "MS", "28", "MO", "29", "MT", "30", "NE", "31",
  "NV", "32", "NH", "33", "NJ", "34", "NM", "35", "NY", "36", "NC", "37", "ND", "38",
  "OH", "39", "OK", "40", "OR", "41", "PA", "42", "RI", "44", "SC", "45", "SD", "46",
  "TN", "47", "TX", "48", "UT", "49", "VT", "50", "VA", "51", "WA", "53", "WV", "54",
  "WI", "55", "WY", "56", "PR", "72"
)

nchs_lookup <- tibble::tribble(
  ~nchs_code, ~fips_code,
  "01", "01", "02", "02", "03", "04", "04", "05", "05", "06", "06", "08", "07", "09",
  "08", "10", "09", "11", "10", "12", "11", "13", "12", "15", "13", "16", "14", "17",
  "15", "18", "16", "19", "17", "20", "18", "21", "19", "22", "20", "23", "21", "24",
  "22", "25", "23", "26", "24", "27", "25", "28", "26", "29", "27", "30", "28", "31",
  "29", "32", "30", "33", "31", "34", "32", "35", "33", "36", "34", "37", "35", "38",
  "36", "39", "37", "40", "38", "41", "39", "42", "40", "44", "41", "45", "42", "46",
  "43", "47", "44", "48", "45", "49", "46", "50", "47", "51", "48", "53", "49", "54",
  "50", "55", "51", "56", "52", "72"
)

# 2. LOAD AND CLEAN DATA -------------------------------------------------------
df <- read_csv(here("data", "county_year_quality_metrics.csv"), show_col_types = FALSE) %>%
  mutate(
    year = as.integer(year),
    raw = str_pad(as.character(countyrs), 5, pad = "0"),
    nchs_state = if_else(year <= 2002, substr(raw, 1, 2), NA_character_),
    state_abbr = if_else(year >= 2003, substr(raw, 1, 2), NA_character_),
    county_part = substr(raw, 3, 5)
  ) %>%
  left_join(nchs_lookup, by = c("nchs_state" = "nchs_code")) %>%
  left_join(state_fips_lookup, by = "state_abbr") %>%
  mutate(
    state_fips = coalesce(fips_code, state_fips),
    fips = str_pad(paste0(state_fips, county_part), 5, pad = "0")
  ) %>%
  left_join(ihme_xwalk, by = "fips") %>%
  mutate(stable_fips = coalesce(stable_fips, fips)) %>%
  select(-raw, -nchs_state, -state_abbr, -county_part, -fips_code, -state_fips)

df <- df %>%
  filter(str_detect(fips, "^[0-9]{5}$"))  # only keep proper 5-digit numeric FIPS

# 3. CREATE prop_all_comp & AVERAGE OVER YEARS ---------------------------------
df <- df %>%
  mutate(prop_all_comp = (marstat_comp_k + placdth_comp_k + educ_comp_k +
                              age_comp_k + sex_comp_k + mandeath_comp_k) / (6*n_cert))

quality_vars <- c(
  "DQ_prop_garbage", "prop_light", "pct_overd_miss", "pct_acc_miss"
)

county_avg <- df %>%
  filter(year >= 1999 & year <= 2022) %>%
  group_by(stable_fips) %>%
  summarise(
    across(all_of(quality_vars), ~ mean(.x, na.rm = TRUE)),
    n_cert = mean(n_cert, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  filter(!is.na(stable_fips), !str_starts(stable_fips, "72"))

# 4. SCALE & RUN MODELS --------------------------------------------------------
mat <- county_avg %>%
  mutate(across(all_of(quality_vars), ~ ifelse(is.na(.x), median(.x, na.rm = TRUE), .x)))

X_df <- mat %>%
  select(all_of(quality_vars)) %>%
  scale() %>%
  as.data.frame()
row.names(X_df) <- mat$stable_fips

set.seed(123)
iso <- isolationForest$new(num_trees = 500, sample_size = 256)
iso$fit(X_df)
iso_scores <- iso$predict(X_df)$anomaly_score

k <- round(sqrt(nrow(X_df)))
lof_scores <- lof(as.matrix(X_df), k = k)

pca <- prcomp(X_df, scale. = FALSE)
pcs <- pca$x[, 1:3]
set.seed(123)
km <- kmeans(pcs, centers = 4, nstart = 25)

# 5. OUTPUT ---------------------------------------------------------------------
res <- mat %>%
  mutate(
    iso_score = iso_scores,
    lof_score = lof_scores,
    km_cluster = km$cluster,
    iso_rank = rank(-iso_score, ties.method = "first"),
    lof_rank = rank(-lof_score, ties.method = "first"),
    anomaly_flag = iso_rank <= ceiling(0.05 * n()) |
                   lof_rank <= ceiling(0.05 * n())
  )

write_csv(res, here("output", "county_outlier_scores_1999_2022_avg.csv"))

pca2 <- as_tibble(pca$x[, 1:2]) %>%
  set_names(c("PC1", "PC2")) %>%
  mutate(
    stable_fips = mat$stable_fips,
    iso_score = iso_scores,
    km_cluster = factor(km$cluster),
    anomaly_flag = res$anomaly_flag
  )

p1 <- ggplot(pca2, aes(PC1, PC2, colour = km_cluster)) +
  geom_point(alpha = 0.7, size = 3) +
  labs(title = "K-means clusters (k = 4)") +
  theme_minimal()

p2 <- ggplot(pca2, aes(PC1, PC2)) +
  geom_point(aes(size = iso_score), colour = "red", alpha = 0.5) +
  labs(title = "Isolation Forest anomaly score", size = "Score") +
  theme_minimal()

p_combined <- p1 + p2 + plot_layout(ncol = 2)

ggsave(here("figures", "outlier_plot_1999_2022_avg.png"), p_combined, width = 10, height = 5, dpi = 300)

# 6. PRINT ----------------------------------------------------------------------
message("Top 15 counties by Isolation Forest score (1999–2022 avg):\n")
print(res %>% arrange(desc(iso_score)) %>% slice_head(n = 15))

message("Top 15 counties by LOF score (1999–2022 avg):\n")
print(res %>% arrange(desc(lof_score)) %>% slice_head(n = 15))

message("Script complete. Outputs:\n  • county_outlier_scores_1999_2022_avg.csv\n  • outlier_plot_1999_2022_avg.png\n")

# add negative/positive signs
res <- res %>%
  mutate(across(all_of(quality_vars), ~ scale(.), .names = "z_{.col}"))
res <- res %>%
  mutate(
    direction_score =
      (z_prop_light + z_pct_overd_miss +
      z_pct_acc_miss + z_DQ_prop_garbage)/4
  )

```
map direction scores
```{r}
# map direction scores
# ──────────────────────────────────────────────────────────────
# 0 · Packages (load after your existing libs)
# ──────────────────────────────────────────────────────────────
library(sf)
library(tigris)      # for county & state shapes
library(ggplot2)
library(dplyr)
library(stringr)

options(tigris_use_cache = TRUE, tigris_class = "sf")
crs_proj <- 2163     # CONUS Albers (EPSG:2163)

# ──────────────────────────────────────────────────────────────
# 1 · Pull direction_score & stable_fips from your results
# ──────────────────────────────────────────────────────────────
dir_scores <- res %>%       
  select(stable_fips, direction_score)

# ──────────────────────────────────────────────────────────────
# 2 · Build county geometry collapsed to stable_fips
# ──────────────────────────────────────────────────────────────
# 2015 TIGER/CB shapefile is a good compromise between detail & performance
county_sf <- counties(cb = TRUE, year = 2015, class = "sf") %>%
  st_transform(crs_proj) %>%
  mutate(fips = GEOID,
         fips = str_pad(fips, 5, pad = "0")) %>%
  # Attach IHME mapping and collapse to temporally-stable polygons
  left_join(ihme_xwalk, by = c("fips" = "fips")) %>%
  mutate(stable_fips = coalesce(stable_fips, fips)) %>%
  select(stable_fips, geometry) %>%
  group_by(stable_fips) %>%
  summarise(geometry = st_union(geometry), .groups = "drop")  # dissolve splits


# ──────────────────────────────────────────────────────────────
# 3 · Join scores ➜ geometry
# ──────────────────────────────────────────────────────────────
map_sf <- left_join(county_sf, dir_scores, by = "stable_fips")

# ──────────────────────────────────────────────────────────────
# 4 · Build colour scale limits (symmetric diverging)
# ──────────────────────────────────────────────────────────────
max_abs <- max(abs(map_sf$direction_score), na.rm = TRUE)

# ─────────────────────────────────────────────────────────
# 0 · Prepare geometry for every stable_fips
# ─────────────────────────────────────────────────────────
library(sf)
library(tigris)
options(tigris_use_cache = TRUE, tigris_class = "sf")

crs_proj <- 2163

# county_sf: one polygon per IHME stable_fips
county_sf <- counties(year = 2015, cb = TRUE, class = "sf") |>
  st_transform(crs_proj) |>
  mutate(fips = GEOID) |>
  left_join(ihme_xwalk, by = c("fips" = "fips")) |>
  mutate(stable_fips = dplyr::coalesce(stable_fips, fips)) |>
  select(stable_fips, geometry) |>
  group_by(stable_fips) |>
  summarise(geometry = st_union(geometry), .groups = "drop")

# ─────────────────────────────────────────────────────────
# 1 · Join your results (res) to geometry
#    res must contain 'stable_fips' and the variable you want to map
# ─────────────────────────────────────────────────────────
map_sf <- county_sf |>
  left_join(res, by = "stable_fips")      # res is your data-frame

# ─────────────────────────────────────────────────────────
# 2 · Call make_map()
#    Example: visualise the direction_score column
# ─────────────────────────────────────────────────────────
map_plot <- make_map(
  sf_data = map_sf,
  var     = "direction_score",            # any column present in map_sf
  title   = "Average Z-score with direction (1999–2022)"
)

# Show in RStudio viewer
print(map_plot)

# Save to file (optional)
ggsave(here("figures", "direction_score_map.png"), map_plot,
       width = 9, height = 6, dpi = 320)
```
Direction scores by 5 year period with global mean and standard deviation
```{r}
# ──────────────────────────────────────────────────────────────
# 0 · Globals  (μ and σ from the overall 1999–2022 county means)
# ──────────────────────────────────────────────────────────────
quality_vars <- c("DQ_prop_garbage", "prop_light", "pct_overd_miss", "pct_acc_miss")

# If `mat` from the earlier chunk is in memory, use that; otherwise rebuild quickly
if (!exists("mat")) {
  mat <- df %>%                            # `df` was read in the anomaly-detection chunk
    filter(year >= 1999, year <= 2022) %>%
    mutate(prop_all_comp = (marstat_comp_k + placdth_comp_k +
                             age_comp_k + sex_comp_k ) /
                           (4 * n_cert)) %>%
    group_by(stable_fips) %>%
    summarise(across(all_of(quality_vars), ~ mean(.x, na.rm = TRUE)),
              .groups = "drop")
}

global_means <- sapply(mat[quality_vars], mean, na.rm = TRUE)
global_sds   <- sapply(mat[quality_vars], sd,   na.rm = TRUE)
library(R.utiles)
R.utils::gunzip("county_year_quality_metrics.csv.gz", remove = FALSE)

# ──────────────────────────────────────────────────────────────
# 1 · Period definitions
# ──────────────────────────────────────────────────────────────
periods <- list(
  `1999_2004` = 1999:2004,
  `2005_2010` = 2005:2010,
  `2011_2017` = 2011:2017,
  `2018_2022` = 2018:2022,
  `2020_2022` = 2020:2022
)

# ──────────────────────────────────────────────────────────────
# 2 · Geometry (one polygon per IHME stable_fips)
# ──────────────────────────────────────────────────────────────
if (!exists("county_sf")) {
  county_sf <- counties(cb = TRUE, year = 2015, class = "sf") %>%
    st_transform(2163) %>%
    mutate(fips = GEOID) %>%
    left_join(ihme_xwalk, by = "fips") %>%
    mutate(stable_fips = coalesce(stable_fips, fips)) %>%
    select(stable_fips, geometry) %>%
    group_by(stable_fips) %>%
    summarise(geometry = st_union(geometry), .groups = "drop")
}

# ──────────────────────────────────────────────────────────────
# 3 · Loop over periods: compute & map direction_score
# ──────────────────────────────────────────────────────────────
out_dir <- here::here("figures", "direction_score_periods")
dir.create(out_dir, showWarnings = FALSE, recursive = TRUE)

for (pname in names(periods)) {
  yrs <- periods[[pname]]

  # county means within the period
  peri_df <- df %>%
    filter(year %in% yrs) %>%
    mutate(prop_all_comp = (marstat_comp_k + placdth_comp_k + educ_comp_k +
                             age_comp_k + sex_comp_k + mandeath_comp_k) /
                           (6 * n_cert)) %>%
    group_by(stable_fips) %>%
    summarise(across(all_of(quality_vars), ~ mean(.x, na.rm = TRUE)),
              .groups = "drop")

  # fixed-reference Z-scores
  peri_df <- peri_df %>%
    mutate(across(all_of(quality_vars),
                  \(x, nm=cur_column()) (x - global_means[nm]) / global_sds[nm],
                  .names = "z_{.col}")) %>%
    mutate(
      direction_score = (z_prop_light + z_pct_overd_miss +
                         z_pct_acc_miss + z_DQ_prop_garbage) / 4
    )
  
    # county means within the period  ──────────────────────────────
  peri_df <- df %>%
    filter(year %in% yrs) %>%
    mutate(prop_all_comp = (marstat_comp_k + placdth_comp_k + educ_comp_k +
                            age_comp_k + sex_comp_k + mandeath_comp_k) /
                           (6 * n_cert)) %>%
    group_by(stable_fips) %>%
    summarise(across(all_of(quality_vars), ~ mean(.x, na.rm = TRUE)),
              .groups = "drop")

  ## NEW 1: guarantee every stable_fips appears (even if all NA)
  peri_df <- county_sf %>%                        # drop geometry, keep IDs
    st_drop_geometry() %>%
    select(stable_fips) %>%
    left_join(peri_df, by = "stable_fips")

  ## NEW 2: impute missing period means with the *same* values
  ##        we used for the overall map (global_means)
  peri_df <- peri_df %>%
    mutate(across(all_of(quality_vars),
                  \(x, nm = cur_column()) replace_na(x, global_means[nm])))

  # fixed-reference Z-scores  ───────────────────────────────────
  peri_df <- peri_df %>%
    mutate(across(all_of(quality_vars),
                  \(x, nm = cur_column()) (x - global_means[nm]) / global_sds[nm],
                  .names = "z_{.col}")) %>%
    ## NEW 3: compute score with all four components now present
    mutate(direction_score = (z_prop_light + z_pct_overd_miss +
                              z_pct_acc_miss  + z_DQ_prop_garbage) / 4)


  # join geometry ➜ map
  map_sf <- county_sf %>% left_join(peri_df, by = "stable_fips")

  p <- make_map(map_sf,
                var   = "direction_score",
                title = glue::glue("Direction score {pname} (scaled to 1999–2022 μ/σ)"))

  ggsave(file.path(out_dir, glue::glue("direction_score_{pname}.png")),
         plot = p, width = 9, height = 6, dpi = 320)
}
```


```{r}
# ──────────────────────────────────────────────────────────────
# 10 · Variance in direction_score explained by median income
# ──────────────────────────────────────────────────────────────
library(dplyr)

var_df <- res %>%                       # 1999–2022 average direction scores
  left_join(income22, by = "stable_fips") %>%    # median income 2022
  filter(!is.na(direction_score), !is.na(medhhinc_2022))

fit <- lm(direction_score ~ medhhinc_2022, data = var_df)

r2  <- summary(fit)$r.squared
r2p <- round(r2 * 100, 1)

cat("R² (variance explained):", sprintf("%.3f", r2), "\n")
cat("=>", r2p, "% of the variance in average direction_scores is explained by 2022 median household income.\n")
```
```{r}
# ──────────────────────────────────────────────────────────────
# 11 · Education: time-series + variance explained
# ──────────────────────────────────────────────────────────────
library(tidycensus)
library(dplyr)
library(ggplot2)
library(glue)
library(scales)

# 11-a · 2022 % bachelor’s+ from ACS table B15003
edu22_raw <- get_acs(
  geography = "county",
  table     = "B15003",
  year      = 2022,
  survey    = "acs5",
  cache_table = TRUE
)

# Keep total (001) and bachelor+ (022-025) → compute share
edu22 <- edu22_raw %>%
  select(GEOID, variable, estimate) %>%
  pivot_wider(names_from = variable, values_from = estimate) %>%
  transmute(
    stable_fips        = GEOID,
    total_25_plus      = B15003_001,
    bach_plus          = B15003_022 + B15003_023 + B15003_024 + B15003_025,
    pct_bachplus_2022  = bach_plus / total_25_plus
  )

# 11-b · Build education buckets (share bachelor’s+)
edu_breaks  <- c(-Inf, .20, .30, .40, .50, Inf)
edu_labels  <- c("<20%", "20–30%", "30–40%", "40–50%", "≥50%")

direction_year <- direction_year %>%   # from step 9
  left_join(edu22, by = "stable_fips") %>%
  mutate(edu_bucket = cut(pct_bachplus_2022,
                          breaks = edu_breaks,
                          labels = edu_labels,
                          right  = FALSE))

# 11-c · TIME-SERIES by education bucket
ts_edu <- direction_year %>%
  filter(!is.na(edu_bucket)) %>%
  group_by(year, edu_bucket) %>%
  summarise(avg_direction = mean(direction_score, na.rm = TRUE),
            .groups = "drop")

g_edu <- ggplot(ts_edu,
                aes(year, avg_direction, colour = edu_bucket)) +
  geom_line(linewidth = 1) +
  labs(title = "Direction-score trend by 2022 education bucket",
       x = NULL, y = "Mean direction score",
       colour = "% bachelor’s or higher") +
  theme_bw()

ggsave(here("figures", "timeseries_direction_by_edu_bucket.png"),
       g_edu, width = 7, height = 4, dpi = 300)
print(g_edu)

# 11-d · R²: variance explained by education
var_edu <- res %>%                       # 1999-2022 averages
  left_join(edu22, by = "stable_fips") %>%
  filter(!is.na(direction_score), !is.na(pct_bachplus_2022))

fit_edu <- lm(direction_score ~ pct_bachplus_2022, data = var_edu)
r2e  <- summary(fit_edu)$r.squared
r2ep <- round(r2e * 100, 1)

cat("Education R²:", sprintf("%.3f", r2e),
    "→", r2ep, "% of variance in average direction_scores explained by pct bachelor’s+.\n")

message("✓ Time-series plot saved: timeseries_direction_by_edu_bucket.png")
```
```{r}
# ──────────────────────────────────────────────────────────────
# 12 · Racial composition: time-series + variance explained
#     Metric: % non-Hispanic White alone (ACS table B03002)
# ──────────────────────────────────────────────────────────────
library(tidycensus)
library(dplyr)
library(tidyr)
library(ggplot2)
library(glue)
library(scales)

# 12-a · Pull 2022 race data (ACS 5-year, table B03002)
race22_raw <- get_acs(
  geography = "county",
  table     = "B03002",
  year      = 2022,
  survey    = "acs5",
  cache_table = TRUE
)

race22 <- race22_raw %>%
  select(GEOID, variable, estimate) %>%
  pivot_wider(names_from = variable, values_from = estimate) %>%
  transmute(
    stable_fips      = GEOID,
    total_pop        = B03002_001,
    nh_white_alone   = B03002_003,
    pct_nh_white_2022 = nh_white_alone / total_pop
  )

# 12-b · Define race buckets (share non-Hispanic White)
race_breaks <- c(-Inf, .20, .40, .60, .80, Inf)
race_labels <- c("<20%", "20–40%", "40–60%", "60–80%", "≥80%")

direction_year <- direction_year %>%   # from earlier steps
  left_join(race22, by = "stable_fips") %>%
  mutate(race_bucket = cut(pct_nh_white_2022,
                           breaks = race_breaks,
                           labels = race_labels,
                           right  = FALSE))

# 12-c · TIME-SERIES by race bucket
ts_race <- direction_year %>%
  filter(!is.na(race_bucket)) %>%
  group_by(year, race_bucket) %>%
  summarise(avg_direction = mean(direction_score, na.rm = TRUE),
            .groups = "drop")

g_race <- ggplot(ts_race,
                 aes(year, avg_direction, colour = race_bucket)) +
  geom_line(linewidth = 1) +
  labs(title = "Direction-score trend by 2022 racial composition",
       x = NULL, y = "Mean direction score",
       colour = "% non-Hispanic White") +
  theme_bw()

ggsave(here("figures", "timeseries_direction_by_race_bucket.png"),
       g_race, width = 7, height = 4, dpi = 300)
print(g_race)

# 12-d · R²: variance explained by racial composition
var_race <- res %>%                       # 1999-2022 averages
  left_join(race22, by = "stable_fips") %>%
  filter(!is.na(direction_score), !is.na(pct_nh_white_2022))

fit_race <- lm(direction_score ~ pct_nh_white_2022, data = var_race)
r2r  <- summary(fit_race)$r.squared
r2rp <- round(r2r * 100, 1)

cat("Race R²:", sprintf("%.3f", r2r),
    "→", r2rp, "% of variance in average direction_scores explained by % non-Hispanic White.\n")

message("✓ Time-series plot saved: timeseries_direction_by_race_bucket.png")
```
```{r}
# ──────────────────────────────────────────────────────────────
# 13 · % Black & % Hispanic: time-series + variance explained
# ──────────────────────────────────────────────────────────────
library(dplyr)
library(tidyr)
library(ggplot2)
library(glue)
library(scales)

# 13-a · Build race22 with additional % Black & % Hispanic
race22 <- race22_raw %>%                # pulled in step 12-a
  select(GEOID, variable, estimate) %>%
  pivot_wider(names_from = variable, values_from = estimate) %>%
  transmute(
    stable_fips        = GEOID,
    total_pop          = B03002_001,
    pct_nh_white_2022  = B03002_003 / total_pop,
    pct_nh_black_2022  = B03002_004 / total_pop,
    pct_hisp_2022      = B03002_012 / total_pop        # Hispanic of any race
  )

# -------------------------------------------------------------
# helper to build plots & R² for an arbitrary race metric
build_race_outputs <- function(var_col,
                               breaks, labels,
                               plot_fname, title_lab, legend_lab) {

  # join & bucket
  tmp_year <- direction_year %>%        # from step 9
    left_join(race22, by = "stable_fips") %>%
    mutate(bucket = cut(.data[[var_col]],
                        breaks = breaks,
                        labels = labels,
                        right  = FALSE))

  # time-series
  ts_df <- tmp_year %>%
    filter(!is.na(bucket)) %>%
    group_by(year, bucket) %>%
    summarise(avg_direction = mean(direction_score, na.rm = TRUE),
              .groups = "drop")

  g <- ggplot(ts_df,
              aes(year, avg_direction, colour = bucket)) +
    geom_line(linewidth = 1) +
    labs(title = title_lab,
         x = NULL, y = "Mean direction score",
         colour = legend_lab) +
    theme_bw()

  ggsave(here("figures", plot_fname),
         g, width = 7, height = 4, dpi = 300)
  print(g)

  # R²
  var_df <- res %>%                       # 1999-2022 averages
    left_join(race22, by = "stable_fips") %>%
    filter(!is.na(direction_score), !is.na(.data[[var_col]]))

  # build formula like "direction_score ~ pct_hisp_2022"
  fit <- lm(reformulate(var_col, response = "direction_score"), data = var_df)

  r2  <- summary(fit)$r.squared
  r2p <- round(r2 * 100, 1)

  cat(title_lab, "– R²:", sprintf("%.3f", r2),
      "→", r2p, "% variance explained.\n")

}

# 13-b · % non-Hispanic Black alone ------------------------------------------
build_race_outputs(
  var_col   = "pct_nh_black_2022",
  breaks    = c(-Inf, .10, .20, .40, .6, Inf),
  labels    = c("<10%", "10–20%", "20–40%", "40–60%", "≥60%"),
  plot_fname = "timeseries_direction_by_black_bucket.png",
  title_lab  = "Direction-score trend by 2022 % non-Hispanic Black",
  legend_lab = "% non-Hispanic Black"
)

# 13-c · % Hispanic/Latino (any race) ----------------------------------------
build_race_outputs(
  var_col   = "pct_hisp_2022",
  breaks    = c(-Inf, .10, .25, .50, .75, Inf),
  labels    = c("<10%", "10–25%", "25–50%", "50–75%", "≥75%"),
  plot_fname = "timeseries_direction_by_hisp_bucket.png",
  title_lab  = "Direction-score trend by 2022 % Hispanic",
  legend_lab = "% Hispanic"
)

message("✓ Time-series plots saved:",
        "\n  • timeseries_direction_by_black_bucket.png",
        "\n  • timeseries_direction_by_hisp_bucket.png")
```
```{r}
# ──────────────────────────────────────────────────────────────
# 0 · Load packages
# ──────────────────────────────────────────────────────────────
library(dplyr)
library(stringr)
library(ggplot2)
library(glue)
library(purrr)
library(scales)
library(here)
library(sf)
library(tigris)
library(tidyr)
library(tidycensus)

options(tigris_use_cache = TRUE, tigris_class = "sf")

# ──────────────────────────────────────────────────────────────
# 1 · Load data & construct stable_fips
# ──────────────────────────────────────────────────────────────
df <- read_csv(here("data", "county_year_quality_metrics.csv"),
               show_col_types = FALSE) %>%
  mutate(stable_fips = str_sub(countyrs, 1, 5))

# List of columns used in direction_score calculation
quality_vars <- c("prop_light", "pct_overd_miss", "pct_acc_miss", "DQ_prop_garbage")

# ──────────────────────────────────────────────────────────────
# 2 · Compute GLOBAL mean and SD for each quality indicator
# ──────────────────────────────────────────────────────────────
stats_tbl <- df %>%
  summarise(across(all_of(quality_vars),
                   list(mu = ~ mean(.x, na.rm = TRUE),
                        sd = ~  sd(.x, na.rm = TRUE))))

mu_vec <- stats_tbl %>%
  select(ends_with("_mu")) %>%
  setNames(str_remove(names(.), "_mu")) %>%
  unlist()

sd_vec <- stats_tbl %>%
  select(ends_with("_sd")) %>%
  setNames(str_remove(names(.), "_sd")) %>%
  unlist()

# ──────────────────────────────────────────────────────────────
# 3 · Load county shapefile
# ──────────────────────────────────────────────────────────────
crs_proj <- 2163
county_sf <- counties(year = 2015, cb = TRUE, class = "sf") %>%
  st_transform(crs_proj) %>%
  mutate(stable_fips = GEOID) %>%
  select(stable_fips, geometry)

# ──────────────────────────────────────────────────────────────
# 4 · Create county-level averages across all years (res)
# ──────────────────────────────────────────────────────────────
res <- df %>%
  group_by(stable_fips) %>%
  summarise(across(all_of(quality_vars), ~ mean(.x, na.rm = TRUE)), .groups = "drop") %>%
  mutate(across(all_of(quality_vars),
                ~ (.x - mu_vec[cur_column()]) / sd_vec[cur_column()],
                .names = "z_{.col}")) %>%
  mutate(direction_score =
           (z_prop_light + z_pct_overd_miss + z_pct_acc_miss + z_DQ_prop_garbage) /4)

# ──────────────────────────────────────────────────────────────
# 5 · Define periods and mapping function using global z-scores
# ──────────────────────────────────────────────────────────────
periods <- list(
  `1999_2004` = 1999:2004,
  `2005_2010` = 2005:2010,
  `2011_2017` = 2011:2017,
  `2018_2022` = 2018:2022,
  `2020_2022` = 2020:2022
)

calc_period_scores <- function(year_vec) {
  df %>%
    filter(year %in% year_vec) %>%
    group_by(stable_fips) %>%
    summarise(across(all_of(quality_vars), ~ mean(.x, na.rm = TRUE)), .groups = "drop") %>%
    mutate(across(all_of(quality_vars),
                  ~ (.x - mu_vec[cur_column()]) / sd_vec[cur_column()],
                  .names = "z_{.col}")) %>%
    mutate(direction_score =
             (z_prop_light + z_pct_overd_miss + z_pct_acc_miss + z_DQ_prop_garbage) / 4)
}

maps <- imap(periods, function(yrs, nm) {
  per_res <- calc_period_scores(yrs)

  per_map_sf <- county_sf %>%
    left_join(per_res %>% select(stable_fips, direction_score),
              by = "stable_fips")

  p_map <- make_map(
    sf_data = per_map_sf,
    var     = "direction_score",
    title   = glue("Direction score ({nm})")
  )

  ggsave(here("figures", glue("direction_score_map_{nm}.png")),
         p_map, width = 9, height = 6, dpi = 320)

  p_map
})

# ──────────────────────────────────────────────────────────────
# 6 · Time-series data frame (county-year global z-scores)
# ──────────────────────────────────────────────────────────────
direction_year <- df %>%
  mutate(across(all_of(quality_vars),
                ~ (.x - mu_vec[cur_column()]) / sd_vec[cur_column()],
                .names = "z_{.col}")) %>%
  mutate(direction_score =
           (z_prop_light + z_pct_overd_miss + z_pct_acc_miss + z_DQ_prop_garbage) / 4) %>%
  select(stable_fips, year, direction_score)

# ──────────────────────────────────────────────────────────────
# 7 · Get 2022 ACS race shares (used later in race bucket plots)
# ──────────────────────────────────────────────────────────────
census_api_key("671b64055fc192103cbc199d2dff91b46d9cc781", overwrite = FALSE)


race22 <- get_acs(geography = "county",
                  table = "B03002",
                  year = 2022,
                  survey = "acs5",
                  cache_table = TRUE) %>%
  select(GEOID, variable, estimate) %>%
  pivot_wider(names_from = variable, values_from = estimate) %>%
  transmute(
    stable_fips       = GEOID,
    total_pop         = B03002_001,
    pct_nh_white_2022 = B03002_003 / total_pop,
    pct_nh_black_2022 = B03002_004 / total_pop,
    pct_hisp_2022     = B03002_012 / total_pop
  )

# ──────────────────────────────────────────────────────────────
# 8 · Optional: Load 2020 election results for vote-share plot
# ──────────────────────────────────────────────────────────────
e20_raw <- read_csv(here("data", "county_election_2020.csv"),
                    show_col_types = FALSE)
```






