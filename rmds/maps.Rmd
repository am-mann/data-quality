---
title: "Mapping Notebook"
output: html_notebook
---

Temporality stable groupings
```{r}
# ──────────────────────────────────────────────────────────────
# 5-year average maps by temporally-stable IHME groups — NO EXTRA PROCESSING
# ──────────────────────────────────────────────────────────────

# 0) Packages & options
required <- c("sf","tigris","readr","dplyr","stringr","purrr","ggplot2","here","scales","patchwork")
missing <- setdiff(required, rownames(installed.packages()))
if (length(missing)) install.packages(missing)
invisible(lapply(required, library, character.only = TRUE))
options(tigris_use_cache = TRUE, tigris_class = "sf")

`%||%` <- function(a, b) if (!is.null(a)) a else b

# 1) Variables to map (no transformations will be applied)
vars_to_map <- c(
  "prop_garbage","pct_overd_miss", "DQ_rec_ig_frac_mean_garbage", "DQ_overall", "DQ_rec_ig_abs_mean", "foreman_garbage"
)

# 2) Load your county-year data (as-is); add time windows only
dq <- readr::read_csv(here::here("data","county_year_quality_metrics.csv.gz"), show_col_types = FALSE) %>%
  dplyr::mutate(
    year        = as.integer(year),
    county_ihme = stringr::str_pad(as.character(county_ihme), 5, pad = "0"),
    time_window = dplyr::case_when(
      year >= 1999 & year <= 2005 ~ "1999_2005",
      year >= 2006 & year <= 2012 ~ "2006_2012",
      year >= 2013 & year <= 2019 ~ "2013_2019",
      year >= 2020 & year <= 2022 ~ "2020_2022",
      TRUE ~ NA_character_
    )
  )

# Keep only variables that actually exist; no conversions
vars_present <- intersect(vars_to_map, names(dq))
if (!length(vars_present)) stop("None of vars_to_map exist in dq. Check column names.")
vars_missing <- setdiff(vars_to_map, vars_present)
if (length(vars_missing)) message("Skipping missing vars: ", paste(vars_missing, collapse = ", "))

# 3) 5-year averages by temporally-stable group (means of existing cols only)
dq_avg <- dq %>%
  dplyr::group_by(county_ihme, time_window) %>%
  dplyr::summarise(
    dplyr::across(dplyr::all_of(vars_present), ~ mean(.x, na.rm = TRUE)),
    n_years = dplyr::n(),
    .groups = "drop"
  )

# 4) Crosswalk for temporality-stable shapes (dissolve multi-GEOID groups)
load(here::here("data_raw","ihme_fips.rda"))  # provides ihme_fips
ihme_map <- ihme_fips %>%
  dplyr::transmute(
    GEOID       = stringr::str_pad(orig_fips, 5, pad = "0"),
    county_ihme = stringr::str_pad(ihme_fips, 5, pad = "0")
  ) %>%
  dplyr::distinct()

# 5) Build shapes per period & join averages
crs_proj <- 2163
shapefile_years <- c("1999_2005"=2000, "2006_2012"=2010, "2013_2019"=2019, "2020_2022"=2020)

st_shift <- function(x, offset) { sf::st_geometry(x) <- sf::st_geometry(x) + offset; x }
st_scale <- function(x, factor) { ctr <- sf::st_centroid(sf::st_union(x)); sf::st_geometry(x) <- (sf::st_geometry(x) - ctr) * factor + ctr; x }

# ——— Normalizer: ensure a 5-digit GEOID exists, with robust fallbacks ———
normalize_geoid <- function(sf, year) {
  nms <- names(sf)

  # A) If any column starts with GEOID (GEOID, GEOID10, GEOID20…), use it.
  geoid_col <- grep("^GEOID", nms, value = TRUE)[1]
  if (!is.na(geoid_col)) {
    message("[", year, "] Using GEOID column: ", geoid_col)
    sf <- dplyr::rename(sf, GEOID = !!rlang::sym(geoid_col))
    sf$GEOID <- stringr::str_pad(as.character(sf$GEOID), 5, pad = "0")
    return(sf)
  }

  # B) Else build from STATEFP + COUNTYFP (present in 2000 & 2010)
  state_col  <- grep("^STATE.*FP$", nms, value = TRUE)[1]
  county_col <- grep("^COUNTY.*FP$", nms, value = TRUE)[1]
  if (!is.na(state_col) && !is.na(county_col)) {
    message("[", year, "] Constructing GEOID from ", state_col, " + ", county_col)
    sf$GEOID <- paste0(
      stringr::str_pad(as.character(sf[[state_col]]),  2, pad = "0"),
      stringr::str_pad(as.character(sf[[county_col]]), 3, pad = "0")
    )
    return(sf)
  }

  # C) Last resort: legacy combined id
  combo_col <- grep("^CNTYIDFP$", nms, value = TRUE)[1]
  if (!is.na(combo_col)) {
    message("[", year, "] Using combined ID column: ", combo_col, " as GEOID")
    sf <- dplyr::rename(sf, GEOID = !!rlang::sym(combo_col))
    sf$GEOID <- stringr::str_pad(as.character(sf$GEOID), 5, pad = "0")
    return(sf)
  }

  stop("No GEOID-compatible columns found in shapefile for year ", year,
       ". Columns were: ", paste(nms, collapse = ", "))
}

# ——— Build groups: normalize → transform → dissolve by county_ihme ———
build_groups_sf <- function(year) {
  counties_raw <- tigris::counties(year = year, cb = TRUE, class = "sf") %>%
    sf::st_zm(drop = TRUE, what = "ZM")

  counties_norm <- normalize_geoid(counties_raw, year) %>%  # ← normalize FIRST
    sf::st_transform(crs_proj)

  groups_sf <- counties_norm %>%
    dplyr::left_join(ihme_map, by = "GEOID") %>%
    dplyr::mutate(county_ihme = dplyr::coalesce(county_ihme, GEOID)) %>%
    dplyr::group_by(county_ihme) %>%
    dplyr::summarise(.groups = "drop")  # st_union here

  # AK / HI layout (unchanged)
  alaska   <- groups_sf %>% dplyr::filter(substr(county_ihme,1,2)=="02") %>%
    { ctr <- sf::st_centroid(sf::st_union(.)); sf::st_geometry(.) <- (sf::st_geometry(.) - ctr)*0.40 + ctr; . } %>%
    { sf::st_geometry(.) <- sf::st_geometry(.) + c(1300000, -4900000); . } %>%
    sf::st_set_crs(crs_proj)
  hawaii   <- groups_sf %>% dplyr::filter(substr(county_ihme,1,2)=="15") %>%
    { ctr <- sf::st_centroid(sf::st_union(.)); sf::st_geometry(.) <- (sf::st_geometry(.) - ctr)*1.50 + ctr; . } %>%
    { sf::st_geometry(.) <- sf::st_geometry(.) + c(5200000, -1400000); . } %>%
    sf::st_set_crs(crs_proj)
  mainland <- groups_sf %>% dplyr::filter(!substr(county_ihme,1,2) %in% c("02","15"))

  dplyr::bind_rows(mainland, alaska, hawaii)
}


joined_by_period <- purrr::imap(
  shapefile_years,
  function(year, window) {
    shape_all <- build_groups_sf(year)
    dplyr::left_join(shape_all, dplyr::filter(dq_avg, time_window == window), by = "county_ihme")
  }
)

# 6) Your original make_map (unchanged)
make_map <- function(sf_data, var, title = NULL, palette = "RdBu") {
  states <- tigris::states(cb = TRUE, class = "sf") |> st_transform(2163)

  fill_scale <- if (var == "prop_garbage") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0.15, 0.4),
    oob = scales::squish,
    direction = -1,
    na.value = "grey90"
  )
} else if (var == "prop_light") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0.20, 0.40),
    oob = scales::squish,
    direction = -1,
    na.value = "grey90"
  )
} else if (var == "pct_overd_miss") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0, 0.75),
    oob = scales::squish,
    direction = -1,
    na.value = "grey90"
  )
} else if (var == "direction_score") {
  scale_fill_distiller(
    palette = palette,
    limits = c(-1,1.5),
    oob = scales::squish,
    direction = -1,
    na.value = "grey90"
  )
} else if (var == "DQ_overall") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0,0.9),
    oob = scales::squish,
    direction = 1,
    na.value = "grey90"
  )
} else if (var == "pct_acc_miss") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0, 0.75),
    oob = scales::squish,
    direction = -1,
    na.value = "grey90"
  )
} else if (var == "pct_mandeath_comp_k") {
  scale_fill_distiller(
    palette = palette,
    limits = c(5, 20),
    oob = scales::squish,
    direction = 1,
    na.value = "grey90"
  )

} else if (var %in% c("pct_age_comp_k", "pct_sex_comp_k", "pct_race_comp_k")) {
  scale_fill_distiller(
    palette = palette,
    limits = c(99.5, 100),
    oob = scales::squish,
    direction = 1,
    na.value = "grey90"
  )
} else if (var == "pct_gc_R_misc") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0, 0.03),
    oob = scales::squish,
    direction = -1,
    na.value = "grey90"
  )
} else if (var == "pct_gc_N19") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0, 0.025),
    oob = scales::squish,
    direction = -1,
    na.value = "grey90"
  )
} else if (var == "pct_gc_J80") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0, 0.005),
    oob = scales::squish,
    direction = -1,
    na.value = "grey90"
  )
} else if (startsWith(var, "pct_") || var == "overall_completeness_pct") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0, 100),
    oob = scales::squish,
    direction = 1,
    na.value = "grey90"
  )
} else {
  scale_fill_distiller(
    palette = palette,
    oob = scales::squish,
    na.value = "grey90",
    direction = -1
  )
}

  ggplot() +
    geom_sf(data = sf_data, aes(fill = .data[[var]]), colour = NA) +
    geom_sf(data = states, fill = NA, colour = "white", linewidth = 0.3) +
    fill_scale +
    coord_sf(xlim = c(-2500000, 2500000),
             ylim = c(-2200000, 730000),
             expand = FALSE) +
    labs(title = title %||% var, fill = NULL) +
    theme_void() +
    theme(
      plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
      legend.text = element_text(size = 9)
    )
}

# 7) Save maps
output_dir <- here::here("figures","5yr_avg_stable")
dir.create(output_dir, showWarnings = FALSE, recursive = TRUE)

# (A) Keep original per-period files (unchanged)
purrr::walk(vars_present, function(var) {
  purrr::walk(names(shapefile_years), function(win) {
    sf_dat  <- joined_by_period[[win]]
    if (!var %in% names(sf_dat)) return(invisible(NULL))
    map_plot <- make_map(sf_dat, var, paste(var, win))
    ggsave(filename = file.path(output_dir, paste0(var, "_", win, ".png")),
           plot = map_plot, width = 8, height = 6, dpi = 320)
  })
})

# (B) ALSO save a 4-panel figure per variable (one panel per time window)
purrr::walk(vars_present, function(var) {
  # Build the exact same four maps
  p_1999 <- make_map(joined_by_period[["1999_2005"]], var, paste(var, "1999_2005"))
  p_2006 <- make_map(joined_by_period[["2006_2012"]], var, paste(var, "2006_2012"))
  p_2013 <- make_map(joined_by_period[["2013_2017"]], var, paste(var, "2013_2017"))
  p_2020 <- make_map(joined_by_period[["2020_2022"]], var, paste(var, "2020_2022"))

  combined <- (p_1999 | p_2006) / (p_2013 | p_2020)

  ggsave(filename = file.path(output_dir, paste0(var, "_4panel.png")),
         plot = combined, width = 12, height = 9, dpi = 320)
})

message("✓ Done. Maps saved to: ", output_dir)

```
Make map of percentage missing overdoses with clusters
```{r}
# ──────────────────────────────────────────────────────────────
# Map pct_overd_miss with temporally-stable clusters (IHME groups)
# - Clusters are built on IHME-stable county groups (fixed over time)
# - Threshold: ≥ 50 overdose deaths in EVERY time window
# - Uses overd_n from county_year_quality_metrics.csv.gz (no Parquet)
# - NA pct_overd_miss → 0
# ──────────────────────────────────────────────────────────────

# 0) Packages & options
required <- c("sf","tigris","readr","dplyr","stringr","purrr","ggplot2",
              "here","scales","igraph","tidyr","tibble","patchwork")
missing <- setdiff(required, rownames(installed.packages()))
if (length(missing)) install.packages(missing)
invisible(lapply(required, library, character.only = TRUE))
options(tigris_use_cache = TRUE, tigris_class = "sf")
sf::sf_use_s2(FALSE)

`%||%` <- function(a, b) if (!is.null(a)) a else b

# 1) Windows & inputs
periods_list <- list(
  "1999_2005" = 1999:2005,
  "2006_2012" = 2006:2012,
  "2013_2019" = 2013:2019,
  "2020_2022" = 2020:2022
)
periods_vec <- names(periods_list)

dq <- readr::read_csv(here::here("data","county_year_quality_metrics.csv.gz"),
                      show_col_types = FALSE) %>%
  dplyr::mutate(
    year        = as.integer(year),
    county_ihme = stringr::str_pad(as.character(county_ihme), 5, pad = "0"),
    time_window = dplyr::case_when(
      year >= 1999 & year <= 2005 ~ "1999_2005",
      year >= 2006 & year <= 2012 ~ "2006_2012",
      year >= 2013 & year <= 2019 ~ "2013_2019",
      year >= 2020 & year <= 2022 ~ "2020_2022",
      TRUE ~ NA_character_
    )
  )

stopifnot(all(c("overd_n","pct_overd_miss") %in% names(dq)))

# 2) IHME crosswalk (temporality-stable groups)
load(here::here("data_raw","ihme_fips.rda"))  # -> ihme_fips
ihme_map <- ihme_fips %>%
  dplyr::transmute(
    GEOID       = stringr::str_pad(as.character(orig_fips), 5, pad = "0"),
    county_ihme = stringr::str_pad(as.character(ihme_fips), 5, pad = "0")
  ) %>%
  dplyr::distinct()

# 3) Compute robust overdose totals per IHME group
overd_by_window <- dq %>%
  dplyr::filter(!is.na(time_window)) %>%
  dplyr::group_by(county_ihme, time_window) %>%
  dplyr::summarise(overd = sum(overd_n, na.rm = TRUE), .groups = "drop")

overd_wide <- overd_by_window %>%
  tidyr::pivot_wider(names_from = time_window, values_from = overd, values_fill = 0)

for (w in periods_vec) if (!w %in% names(overd_wide)) overd_wide[[w]] <- 0

robust_tbl <- overd_wide %>%
  dplyr::mutate(
    robust_overd = pmin(`1999_2005`,`2006_2012`,`2013_2019`,`2020_2022`, na.rm = TRUE)
  ) %>%
  dplyr::transmute(county_ihme, robust_overd = as.numeric(robust_overd))

# 4) Helpers for shapefiles (same style as your IHME-stable code)
crs_proj <- 2163
shapefile_years <- c("1999_2005"=2000, "2006_2012"=2010, "2013_2019"=2019, "2020_2022"=2020)

normalize_geoid <- function(sf, year) {
  nms <- names(sf)
  geoid_col <- grep("^GEOID", nms, value = TRUE)[1]
  if (!is.na(geoid_col)) {
    sf <- dplyr::rename(sf, GEOID = !!rlang::sym(geoid_col))
    sf$GEOID <- stringr::str_pad(as.character(sf$GEOID), 5, pad = "0")
    return(sf)
  }
  state_col  <- grep("^STATE.*FP$", nms, value = TRUE)[1]
  county_col <- grep("^COUNTY.*FP$", nms, value = TRUE)[1]
  if (!is.na(state_col) && !is.na(county_col)) {
    sf$GEOID <- paste0(
      stringr::str_pad(as.character(sf[[state_col]]),  2, pad = "0"),
      stringr::str_pad(as.character(sf[[county_col]]), 3, pad = "0")
    )
    return(sf)
  }
  combo_col <- grep("^CNTYIDFP$", nms, value = TRUE)[1]
  if (!is.na(combo_col)) {
    sf <- dplyr::rename(sf, GEOID = !!rlang::sym(combo_col))
    sf$GEOID <- stringr::str_pad(as.character(sf$GEOID), 5, pad = "0")
    return(sf)
  }
  stop("No GEOID-compatible columns found in shapefile for year ", year)
}

# Build IHME-stable county geoms for a given year
build_ihme_groups_sf <- function(year) {
  counties_raw <- tigris::counties(year = year, cb = TRUE, class = "sf") %>%
    sf::st_zm(drop = TRUE, what = "ZM")
  counties_norm <- normalize_geoid(counties_raw, year) %>% sf::st_transform(crs_proj)
  groups_sf <- counties_norm %>%
    dplyr::left_join(ihme_map, by = "GEOID") %>%
    dplyr::mutate(county_ihme = dplyr::coalesce(county_ihme, GEOID)) %>%
    dplyr::group_by(county_ihme) %>%
    dplyr::summarise(.groups = "drop")
  groups_sf
}

# 5) Build adjacency on IHME-stable geoms (use 2020 as base)
ihme2020 <- build_ihme_groups_sf(2020) %>% sf::st_transform(5070)
adj_list <- sf::st_touches(ihme2020)
neighbors_of <- setNames(
  lapply(seq_len(nrow(ihme2020)), function(i) ihme2020$county_ihme[adj_list[[i]]]),
  ihme2020$county_ihme
)

# 6) Greedy clustering on IHME-stable nodes using robust_overd
min_overd <- 50L

nodes_tbl <- ihme2020 %>%
  sf::st_drop_geometry() %>%
  dplyr::select(county_ihme) %>%
  dplyr::left_join(robust_tbl, by = "county_ihme") %>%
  dplyr::mutate(robust_overd = dplyr::coalesce(robust_overd, 0))

overd_vec <- setNames(nodes_tbl$robust_overd, nodes_tbl$county_ihme)
to_assign <- names(overd_vec)
clusters  <- setNames(rep(NA_character_, length(overd_vec)), to_assign)
cid <- 1L

while (length(to_assign) > 0) {
  seed  <- to_assign[which.max(overd_vec[to_assign])]
  cur   <- seed
  total <- overd_vec[seed]
  avail <- setdiff(to_assign, seed)

  repeat {
    nbrs <- unique(unlist(neighbors_of[cur], use.names = FALSE))
    nbrs <- setdiff(intersect(nbrs, avail), cur)
    if (!length(nbrs)) break
    cand_totals <- total + overd_vec[nbrs]
    best_idx <- if (any(cand_totals < min_overd)) which.max(cand_totals) else which.min(abs(cand_totals - min_overd))
    best <- nbrs[best_idx]
    new_total <- total + overd_vec[best]
    if (is.na(new_total)) break
    if (new_total <= min_overd * 1.6 || total < min_overd) {
      cur   <- c(cur, best)
      total <- new_total
      avail <- setdiff(avail, best)
      if (total >= min_overd) break
    } else break
  }

  clusters[cur] <- paste0("CL", cid)
  to_assign <- setdiff(to_assign, cur)
  cid <- cid + 1L
}

# Merge any undersized clusters to nearest big cluster (on IHME-stable centroids)
centroids <- sf::st_centroid(ihme2020) %>% dplyr::mutate(county_ihme = ihme2020$county_ihme)
xy <- sf::st_coordinates(centroids)[,c("X","Y"), drop=FALSE]; rownames(xy) <- centroids$county_ihme

cluster_sum <- tapply(overd_vec[names(clusters)], clusters, sum, na.rm = TRUE)
small <- names(cluster_sum)[cluster_sum < min_overd]
big   <- names(cluster_sum)[cluster_sum >= min_overd]

if (length(small) && length(big)) {
  get_center <- function(cid) {
    memb <- names(clusters)[clusters == cid]
    colMeans(xy[memb,,drop=FALSE])
  }
  small_xy <- t(vapply(small, get_center, numeric(2L)))
  big_xy   <- t(vapply(big,   get_center, numeric(2L)))
  assign_to <- vapply(seq_len(nrow(small_xy)), function(i) {
    dif <- t(big_xy) - small_xy[i,]; which.min(colSums(dif*dif))
  }, integer(1L))
  for (i in seq_along(small)) {
    members <- names(clusters)[clusters == small[i]]
    clusters[members] <- big[assign_to[i]]
  }
}

cluster_map <- tibble::tibble(
  county_ihme = names(clusters),
  cluster_id  = unname(clusters)
)

# 7) Sanity: each cluster has ≥ 50 overdoses in EVERY window
check_df <- overd_by_window %>%
  dplyr::inner_join(cluster_map, by = "county_ihme") %>%
  dplyr::group_by(cluster_id, time_window) %>%
  dplyr::summarise(overdoses = sum(overd, na.rm = TRUE), .groups = "drop")
if (any(check_df$overdoses < min_overd, na.rm = TRUE)) {
  warning("Some clusters are still < 50 overdoses in a window. Consider tweaking heuristics.")
}

# 8) Aggregate pct_overd_miss to cluster×window with NA→0
dq_cluster <- dq %>%
  dplyr::filter(!is.na(time_window)) %>%
  dplyr::inner_join(cluster_map, by = "county_ihme") %>%
  dplyr::group_by(cluster_id, time_window) %>%
  dplyr::summarise(
    pct_overd_miss = mean(replace_na(pct_overd_miss, 0)),
    n_counties     = dplyr::n_distinct(county_ihme),
    .groups = "drop"
  ) %>%
  tidyr::complete(cluster_id, time_window = periods_vec,
                  fill = list(pct_overd_miss = 0, n_counties = 0L))

# 9) Shapes per period: IHME-stable dissolve → cluster dissolve
build_clusters_sf <- function(year) {
  base <- build_ihme_groups_sf(year)  # county_ihme geometry for that year
  base %>%
    dplyr::left_join(cluster_map, by = "county_ihme") %>%
    dplyr::filter(!is.na(cluster_id)) %>%
    dplyr::group_by(cluster_id) %>%
    dplyr::summarise(.groups = "drop") %>%
    sf::st_transform(crs_proj)
}

joined_by_period <- purrr::imap(shapefile_years, function(year, window) {
  shape_all <- build_clusters_sf(year)
  dplyr::left_join(shape_all,
                   dplyr::filter(dq_cluster, time_window == window),
                   by = "cluster_id")
})

# 10) Map helper (unchanged except limits tuned for pct_overd_miss)
make_map <- function(sf_data, var = "pct_overd_miss", title = NULL, palette = "RdBu") {
  states <- tigris::states(cb = TRUE, class = "sf") |> sf::st_transform(crs_proj)
  fill_scale <- scale_fill_distiller(
    palette = palette,
    limits = if (var == "pct_overd_miss") c(0, 0.75) else NULL,
    oob = scales::squish,
    direction = -1,
    na.value = "grey90"
  )
  ggplot() +
    geom_sf(data = sf_data, aes(fill = .data[[var]]), colour = NA) +
    geom_sf(data = states, fill = NA, colour = "white", linewidth = 0.3) +
    fill_scale +
    coord_sf(xlim = c(-2500000, 2500000),
             ylim = c(-2200000, 730000),
             expand = FALSE) +
    labs(title = title %||% var, fill = NULL) +
    theme_void() +
    theme(
      plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
      legend.text = element_text(size = 9)
    )
}

# 11) Save one combined 2×2 figure (exact same maps, assembled)
output_dir <- here::here("figures","5yr_avg_IHMEstable_clusters50_overd")
dir.create(output_dir, showWarnings = FALSE, recursive = TRUE)

p_1999 <- make_map(joined_by_period[["1999_2005"]], "pct_overd_miss", "pct_overd_miss  1999_2005")
p_2006 <- make_map(joined_by_period[["2006_2012"]], "pct_overd_miss", "pct_overd_miss  2006_2012")
p_2013 <- make_map(joined_by_period[["2013_2019"]], "pct_overd_miss", "pct_overd_miss  2013_2019")
p_2020 <- make_map(joined_by_period[["2020_2022"]], "pct_overd_miss", "pct_overd_miss  2020_2022")

combined <- (p_1999 | p_2006) / (p_2013 | p_2020)

ggsave(filename = file.path(output_dir, "pct_overd_miss_4panel.png"),
       plot = combined, width = 12, height = 9, dpi = 320)

message("✓ Done. Single 4-panel map saved to: ",
        file.path(output_dir, "pct_overd_miss_4panel.png"))
```

Relationship between rural and garbage
```{r}
# 0 · Load packages -------------------------------------------------------
library(dplyr)
library(stringr)
library(ggplot2)
library(scales)
library(tigris)
library(tidycensus)
library(readr)
library(here)

# 1 · Load your data ------------------------------------------------------
dq <- read_csv(here("data", "county_year_quality_metrics.csv.gz"),
               show_col_types = FALSE)

# 3 · Convert county codes to GEOID ---------------------------------------
dq_all <- dq %>%
  mutate(
    GEOID = county_ihme
  )

cat("Counties without valid GEOID:", sum(is.na(dq_all$GEOID)), "\n")

# 4 · Get 2022 population estimates ---------------------------------------
census_api_key("671b64055fc192103cbc199d2dff91b46d9cc781", overwrite = TRUE, install=TRUE)

pop22 <- get_acs(
  geography = "county",
  variables = "B01003_001",
  year = 2022,
  survey = "acs5"
) %>%
  select(GEOID, pop_2022 = estimate)

# 5 · Define county size buckets ------------------------------------------
cuts   <- c(0, 50000, 100000, 250000, 1000000, Inf)
labels <- c("<50k", "50–100k", "100–250k", "250k–1M", "≥1M")

county_sizes <- pop22 %>%
  mutate(size_bucket = cut(pop_2022, breaks = cuts, labels = labels, right = FALSE)) %>%
  select(GEOID, pop_2022, size_bucket)

# 6 · Join population and size to main data -------------------------------
dq_all <- dq_all %>%
  left_join(pop22, by = "GEOID") %>%
  left_join(county_sizes %>% select(GEOID, size_bucket), by = "GEOID")

# 7 · Indicators to plot --------------------------------------------------
indicators <- c(
  "prop_garbage",
  "prop_light",
  "pct_overd_miss",
  "pct_acc_miss")

# 8 · Time trends by size bucket ------------------------------------------
for (var in indicators) {
  ts_df <- dq_all %>%
    filter(!is.na(size_bucket)) %>%
    group_by(year, size_bucket) %>%
    summarise(avg_prop = mean(.data[[var]], na.rm = TRUE), .groups = "drop")
  
  g <- ggplot(ts_df, aes(year, avg_prop, colour = size_bucket)) +
    geom_line(linewidth = 1) +
    scale_y_continuous(labels = percent_format(accuracy = 0.1)) +
    labs(
      title = paste("Trend of", gsub("_", " ", var), "by county size"),
      x = NULL,
      y = "Mean percentage",
      colour = "2022 size bucket"
    ) +
    theme_bw()
  
  print(g)
  
  ggsave(
    filename = here("figures", paste0("trend_", var, "_by_size_bucket.png")),
    plot = g,
    width = 7, height = 4, dpi = 300
  )
}

# 9 · Scatterplots: pop vs indicator (2022) -------------------------------
scatter_data <- dq_all %>%
  filter(year == 2022, !is.na(pop_2022))

for (var in indicators) {
  scatter_df <- scatter_data %>%
    filter(!is.na(.data[[var]]))
  
  g <- ggplot(scatter_df, aes(x = pop_2022, y = .data[[var]])) +
    geom_point(alpha = 0.6) +
    scale_x_log10(labels = comma) +
    scale_y_continuous(labels = percent_format(accuracy = 0.1)) +
    labs(
      title = paste("County population vs", gsub("_", " ", var), "(2022)"),
      x = "County population (log10 scale)",
      y = paste("Proportion:", gsub("_", " ", var))
    ) +
    theme_bw()
  
  print(g)
  
  ggsave(
    filename = here("figures", paste0("scatter_", var, "_vs_population_2022.png")),
    plot = g,
    width = 7, height = 4, dpi = 300
  )
}
```
Clustering or anomaly detection: Uses unsupervised ML (clustering, autoencoders) to identify unusual records or patterns. If a county’s records are more often flagged as outliers, that suggests quality issues.
```{r}
# 0.  PACKAGES ------------------------------------------------------------------
library(tidyverse)
library(solitude)
library(dbscan)
library(factoextra)
library(patchwork)
library(here)

# 1.  LOAD & CLEAN DATA --------------------------------------------------------
df <- read_csv(
  here("data", "county_year_quality_metrics.csv.gz"),
  show_col_types = FALSE
) %>%
  mutate(
    year = as.integer(year)
  )

# 2.  COMPUTE prop_all_comp & DEFINE VARIABLES --------------------------------
df <- df %>%
  mutate(
    prop_all_comp = (
      marstat_comp_k + placdth_comp_k + educ_comp_k +
      age_comp_k     + sex_comp_k      + mandeath_comp_k
    ) / (6 * n_cert)
  )

quality_vars <- c(
  "DQ_prop_garbage", "prop_light", "pct_overd_miss", "pct_acc_miss"
)

# 3.  AVERAGE OVER YEARS BY COUNTY_ihme ----------------------------------------
county_avg <- df %>%
  filter(year >= 1999, year <= 2022) %>%
  group_by(county_ihme) %>%
  summarise(
    across(all_of(quality_vars), ~ mean(.x, na.rm = TRUE)),
    n_cert = mean(n_cert, na.rm = TRUE),
    .groups = "drop"
  )

# ------------------------------------------------------------------
# 4 · SCALE, FIT MODELS & ADD SCORES   (corrected)
# ------------------------------------------------------------------

## 4.1  Impute medians for NAs -------------------------------------
mat <- county_avg %>%
  mutate(across(all_of(quality_vars), 
                ~ ifelse(is.na(.x), median(.x, na.rm = TRUE), .x)))

## 4.2  Drop any rows missing county_ihme --------------------------
if (any(is.na(mat$county_ihme))) {
  dropped <- mat %>% filter(is.na(county_ihme)) %>% nrow()
  message("Dropping ", dropped, " rows with missing county_ihme.")
  mat <- mat %>% filter(!is.na(county_ihme))
}

## 4.3  Build scaled predictor matrix ------------------------------
X_df <- mat %>%
  select(all_of(quality_vars)) %>%
  scale() %>%
  as.data.frame()
row.names(X_df) <- mat$county_ihme

# 4.4  Isolation Forest (solitude) ------------------------------
iso_mod <- solitude::isolationForest$new(num_trees = 100)
iso_mod$fit(X_df)
iso_scores <- iso_mod$predict(X_df)$anomaly_score

# 4.5  Local Outlier Factor (dbscan) -----------------------------
lof_scores <- dbscan::lof(as.matrix(X_df),
                          minPts = round(sqrt(nrow(X_df))))

# 4.6  k-means clustering (k = 3) --------------------------------
set.seed(123)
km <- kmeans(X_df, centers = 3, nstart = 25)

# 4.7  Bind results back to mat ----------------------------------
mat <- mat %>%
  mutate(
    iso_score  = iso_scores,
    lof_score  = lof_scores,
    km_cluster = km$cluster,
    iso_rank   = rank(-iso_score, ties.method = "first"),
    lof_rank   = rank(-lof_score,  ties.method = "first"),
    anomaly_flag = iso_rank <= ceiling(0.05 * n()) |
                   lof_rank <= ceiling(0.05 * n())
  )

# store final results
res <- mat

# 5.  PCA FOR PLOTTING --------------------------------------------------------
pca <- prcomp(X_df, center = TRUE, scale. = FALSE)
pca2 <- as_tibble(pca$x[,1:2]) %>%
  set_names(c("PC1", "PC2")) %>%
  mutate(
    county_ihme  = row.names(pca$x),
    iso_score    = res$iso_score,
    km_cluster   = factor(res$km_cluster),
    anomaly_flag = res$anomaly_flag
  )

# 6.  SAVE OUTPUTS ------------------------------------------------------------
write_csv(res,
          here("output", "county_outlier_scores_1999_2022_avg.csv"))

p1 <- ggplot(pca2, aes(PC1, PC2, colour = km_cluster)) +
  geom_point(alpha = 0.7, size = 3) +
  labs(title = "K-means clusters (k = 3)") +
  theme_minimal()

p2 <- ggplot(pca2, aes(PC1, PC2, size = iso_score)) +
  geom_point(colour = "red", alpha = 0.5) +
  labs(title = "Isolation Forest anomaly score") +
  theme_minimal()

(p1 + p2) %>%
  ggsave(
    filename = here("figures", "outlier_plot_1999_2022_avg.png"),
    width    = 10, height = 5, dpi = 300
  )

# 7.  PRINT TOP OUTLIERS & COMPUTE DIRECTION_SCORE ---------------------------
message("Top 15 counties by Isolation Forest score:\n")
print(res %>% arrange(desc(iso_score)) %>% slice_head(n = 15))

message("Top 15 counties by LOF score:\n")
print(res %>% arrange(desc(lof_score)) %>% slice_head(n = 15))

res <- res %>%
  mutate(
    across(all_of(quality_vars),
           ~ as.numeric(scale(.x)),
           .names = "z_{.col}"),
    direction_score = (
      z_prop_light     +
      z_pct_overd_miss +
      z_pct_acc_miss   +
      z_DQ_prop_garbage
    ) / 4
  )

message("Script complete.")
```
map direction scores
```{r}

# map direction scores
# ──────────────────────────────────────────────────────────────
# 0 · Packages (load after your existing libs)
# ──────────────────────────────────────────────────────────────
library(sf)
library(tigris)      # for county & state shapes
library(ggplot2)
library(dplyr)
library(stringr)

options(tigris_use_cache = TRUE, tigris_class = "sf")
crs_proj <- 2163     # CONUS Albers (EPSG:2163)

# ──────────────────────────────────────────────────────────────
# 1 · Pull direction_score & county_ihme from your results
# ──────────────────────────────────────────────────────────────
dir_scores <- res %>%       
  select(county_ihme, direction_score)

# ──────────────────────────────────────────────────────────────
# 2 · Build county geometry collapsed to county_ihme
# ──────────────────────────────────────────────────────────────
# 2015 TIGER/CB shapefile is a good compromise between detail & performance
county_sf <- counties(cb = TRUE, year = 2015, class = "sf") %>%
  st_transform(crs_proj) %>%
  mutate(fips = GEOID,
         fips = str_pad(fips, 5, pad = "0")) %>%
  # Attach IHME mapping and collapse to temporally-stable polygons
  left_join(ihme_xwalk, by = c("fips" = "fips")) %>%
  mutate(county_ihme = coalesce(county_ihme, fips)) %>%
  select(county_ihme, geometry) %>%
  group_by(county_ihme) %>%
  summarise(geometry = st_union(geometry), .groups = "drop")  # dissolve splits


# ──────────────────────────────────────────────────────────────
# 3 · Join scores ➜ geometry
# ──────────────────────────────────────────────────────────────
map_sf <- left_join(county_sf, dir_scores, by = "county_ihme")

# ──────────────────────────────────────────────────────────────
# 4 · Build colour scale limits (symmetric diverging)
# ──────────────────────────────────────────────────────────────
max_abs <- max(abs(map_sf$direction_score), na.rm = TRUE)

# ─────────────────────────────────────────────────────────
# 0 · Prepare geometry for every county_ihme
# ─────────────────────────────────────────────────────────
library(sf)
library(tigris)
options(tigris_use_cache = TRUE, tigris_class = "sf")

crs_proj <- 2163

# county_sf: one polygon per IHME county_ihme
county_sf <- counties(year = 2015, cb = TRUE, class = "sf") |>
  st_transform(crs_proj) |>
  mutate(fips = GEOID) |>
  left_join(ihme_xwalk, by = c("fips" = "fips")) |>
  mutate(county_ihme = dplyr::coalesce(county_ihme, fips)) |>
  select(county_ihme, geometry) |>
  group_by(county_ihme) |>
  summarise(geometry = st_union(geometry), .groups = "drop")

# ─────────────────────────────────────────────────────────
# 1 · Join your results (res) to geometry
#    res must contain 'county_ihme' and the variable you want to map
# ─────────────────────────────────────────────────────────
map_sf <- county_sf |>
  left_join(res, by = "county_ihme")      # res is your data-frame

# ─────────────────────────────────────────────────────────
# 2 · Call make_map()
#    Example: visualise the direction_score column
# ─────────────────────────────────────────────────────────
map_plot <- make_map(
  sf_data = map_sf,
  var     = "direction_score",            # any column present in map_sf
  title   = "Average Z-score with direction (1999–2022)"
)

# Show in RStudio viewer
print(map_plot)

# Save to file (optional)
ggsave(here("figures", "direction_score_map.png"), map_plot,
       width = 9, height = 6, dpi = 320)
```
Direction scores by 5 year period with global mean and standard deviation
```{r}
# ──────────────────────────────────────────────────────────────
# 0 · Globals  (μ and σ from the overall 1999–2022 county means)
# ──────────────────────────────────────────────────────────────
quality_vars <- c("DQ_prop_garbage", "prop_light", "pct_overd_miss", "pct_acc_miss")

# If `mat` from the earlier chunk is in memory, use that; otherwise rebuild quickly
if (!exists("mat")) {
  mat <- df %>%                            # `df` was read in the anomaly-detection chunk
    filter(year >= 1999, year <= 2022) %>%
    mutate(prop_all_comp = (marstat_comp_k + placdth_comp_k +
                             age_comp_k + sex_comp_k ) /
                           (4 * n_cert)) %>%
    group_by(county_ihme) %>%
    summarise(across(all_of(quality_vars), ~ mean(.x, na.rm = TRUE)),
              .groups = "drop")
}

global_means <- sapply(mat[quality_vars], mean, na.rm = TRUE)
global_sds   <- sapply(mat[quality_vars], sd,   na.rm = TRUE)

# ──────────────────────────────────────────────────────────────
# 1 · Period definitions
# ──────────────────────────────────────────────────────────────
periods <- list(
  `1999_2004` = 1999:2004,
  `2005_2010` = 2005:2010,
  `2011_2017` = 2011:2017,
  `2018_2022` = 2018:2022,
  `2020_2022` = 2020:2022
)

# ──────────────────────────────────────────────────────────────
# 2 · Geometry (one polygon per IHME county_ihme)
# ──────────────────────────────────────────────────────────────
if (!exists("county_sf")) {
  county_sf <- counties(cb = TRUE, year = 2015, class = "sf") %>%
    st_transform(2163) %>%
    mutate(fips = GEOID) %>%
    left_join(ihme_xwalk, by = "fips") %>%
    mutate(county_ihme = coalesce(county_ihme, fips)) %>%
    select(county_ihme, geometry) %>%
    group_by(county_ihme) %>%
    summarise(geometry = st_union(geometry), .groups = "drop")
}

# ──────────────────────────────────────────────────────────────
# 3 · Loop over periods: compute & map direction_score
# ──────────────────────────────────────────────────────────────
out_dir <- here::here("figures", "direction_score_periods")
dir.create(out_dir, showWarnings = FALSE, recursive = TRUE)

for (pname in names(periods)) {
  yrs <- periods[[pname]]

  # county means within the period
  peri_df <- df %>%
    filter(year %in% yrs) %>%
    mutate(prop_all_comp = (marstat_comp_k + placdth_comp_k + educ_comp_k +
                             age_comp_k + sex_comp_k + mandeath_comp_k) /
                           (6 * n_cert)) %>%
    group_by(county_ihme) %>%
    summarise(across(all_of(quality_vars), ~ mean(.x, na.rm = TRUE)),
              .groups = "drop")

  # fixed-reference Z-scores
  peri_df <- peri_df %>%
    mutate(across(all_of(quality_vars),
                  \(x, nm=cur_column()) (x - global_means[nm]) / global_sds[nm],
                  .names = "z_{.col}")) %>%
    mutate(
      direction_score = (z_prop_light + z_pct_overd_miss +
                         z_pct_acc_miss + z_DQ_prop_garbage) / 4
    )
  
    # county means within the period  ──────────────────────────────
  peri_df <- df %>%
    filter(year %in% yrs) %>%
    mutate(prop_all_comp = (marstat_comp_k + placdth_comp_k + educ_comp_k +
                            age_comp_k + sex_comp_k + mandeath_comp_k) /
                           (6 * n_cert)) %>%
    group_by(county_ihme) %>%
    summarise(across(all_of(quality_vars), ~ mean(.x, na.rm = TRUE)),
              .groups = "drop")

  ## NEW 1: guarantee every county_ihme appears (even if all NA)
  peri_df <- county_sf %>%                        # drop geometry, keep IDs
    st_drop_geometry() %>%
    select(county_ihme) %>%
    left_join(peri_df, by = "county_ihme")

  ## NEW 2: impute missing period means with the *same* values
  ##        we used for the overall map (global_means)
  peri_df <- peri_df %>%
    mutate(across(all_of(quality_vars),
                  \(x, nm = cur_column()) replace_na(x, global_means[nm])))

  # fixed-reference Z-scores  ───────────────────────────────────
  peri_df <- peri_df %>%
    mutate(across(all_of(quality_vars),
                  \(x, nm = cur_column()) (x - global_means[nm]) / global_sds[nm],
                  .names = "z_{.col}")) %>%
    ## NEW 3: compute score with all four components now present
    mutate(direction_score = (z_prop_light + z_pct_overd_miss +
                              z_pct_acc_miss  + z_DQ_prop_garbage) / 4)


  # join geometry ➜ map
  map_sf <- county_sf %>% left_join(peri_df, by = "county_ihme")

  p <- make_map(map_sf,
                var   = "direction_score",
                title = glue::glue("Direction score {pname} (scaled to 1999–2022 μ/σ)"))

  ggsave(file.path(out_dir, glue::glue("direction_score_{pname}.png")),
         plot = p, width = 9, height = 6, dpi = 320)
}
```

```{r}
# Assign each county its 2022 size bucket (once per county)
size_r2 <- direction_year %>%
  filter(year >= 1999, year <= 2022) %>%
  group_by(county_ihme) %>%
  summarise(
    mean_direction = mean(direction_score, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  left_join(pop22, by = "county_ihme") %>%
  mutate(size_bucket = cut(pop_2022, breaks = size_breaks, labels = size_labels, right = FALSE))

# Fit one-way ANOVA
fit_size <- lm(mean_direction ~ size_bucket, data = size_r2)
r2_size <- summary(fit_size)$r.squared
r2p_size <- round(r2_size * 100, 1)

cat("County size R²:", sprintf("%.3f", r2_size),
    "→", r2p_size, "% of variance in average direction_scores explained by 2022 county size bucket.\n")

income_r2 <- direction_year %>%
  filter(year >= 1999, year <= 2022) %>%
  group_by(county_ihme) %>%
  summarise(
    mean_direction = mean(direction_score, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  left_join(income22, by = "county_ihme") %>%
  mutate(income_bucket = cut(medhhinc_2022, breaks = income_breaks, labels = income_labels, right = FALSE))

fit_income <- lm(mean_direction ~ income_bucket, data = income_r2)
r2_income <- summary(fit_income)$r.squared
r2p_income <- round(r2_income * 100, 1)

cat("Income R²:", sprintf("%.3f", r2_income),
    "→", r2p_income, "% of variance in average direction_scores explained by 2022 income bucket.\n")

```

```{r}
# ──────────────────────────────────────────────────────────────
# 11 · Education: time-series + variance explained
# ──────────────────────────────────────────────────────────────
library(tidycensus)
library(dplyr)
library(ggplot2)
library(glue)
library(scales)

# 1. Get global means and sds for each variable across all years
global_stats <- df %>%
  summarise(across(
    c(DQ_prop_garbage, prop_light, pct_overd_miss, pct_acc_miss),
    list(mean = ~mean(.x, na.rm = TRUE),
         sd   = ~sd(.x, na.rm = TRUE)),
    .names = "{.col}_{.fn}"
  ))

# 2. Define a function to compute global z-score
zscore_global <- function(x, m, s) (x - m) / s

# 3. Calculate direction_score for every county-year using global stats
direction_year <- df %>%
  filter(year >= 1999, year <= 2022) %>%
  mutate(
    z_DQ_prop_garbage  = zscore_global(DQ_prop_garbage, global_stats$DQ_prop_garbage_mean, global_stats$DQ_prop_garbage_sd),
    z_prop_light       = zscore_global(prop_light,      global_stats$prop_light_mean,      global_stats$prop_light_sd),
    z_pct_overd_miss   = zscore_global(pct_overd_miss,  global_stats$pct_overd_miss_mean,  global_stats$pct_overd_miss_sd),
    z_pct_acc_miss     = zscore_global(pct_acc_miss,    global_stats$pct_acc_miss_mean,    global_stats$pct_acc_miss_sd),
    direction_score    = (z_prop_light + z_pct_overd_miss + z_pct_acc_miss + z_DQ_prop_garbage) / 4
  ) %>%
  select(county_ihme, year, direction_score) %>%
  filter(!is.na(county_ihme), !is.na(direction_score))


# 11-a · 2022 % bachelor’s+ from ACS table B15003
edu22_raw <- get_acs(
  geography = "county",
  table     = "B15003",
  year      = 2022,
  survey    = "acs5",
  cache_table = TRUE
)

# Keep total (001) and bachelor+ (022-025) → compute share
edu22 <- edu22_raw %>%
  select(GEOID, variable, estimate) %>%
  pivot_wider(names_from = variable, values_from = estimate) %>%
  transmute(
    county_ihme        = GEOID,
    total_25_plus      = B15003_001,
    bach_plus          = B15003_022 + B15003_023 + B15003_024 + B15003_025,
    pct_bachplus_2022  = bach_plus / total_25_plus
  )

# 11-b · Build education buckets (share bachelor’s+)
edu_breaks  <- c(-Inf, .20, .30, .40, .50, Inf)
edu_labels  <- c("<20%", "20–30%", "30–40%", "40–50%", "≥50%")

direction_year <- direction_year %>%
  left_join(edu22, by = "county_ihme") %>%
  mutate(edu_bucket = cut(pct_bachplus_2022,
                          breaks = edu_breaks,
                          labels = edu_labels,
                          right  = FALSE))

# Join education data to annual direction scores
direction_year <- direction_year %>%
  left_join(edu22, by = "county_ihme")


# Proceed with analysis and plotting
ts_edu <- direction_year %>%
  filter(!is.na(edu_bucket)) %>%
  group_by(year, edu_bucket) %>%
  summarise(avg_direction = mean(direction_score, na.rm = TRUE),
            .groups = "drop")

g_edu <- ggplot(ts_edu,
                aes(year, avg_direction, colour = edu_bucket)) +
  geom_line(linewidth = 1) +
  labs(title = "Direction-score trend by 2022 education bucket",
       x = NULL, y = "Mean direction score",
       colour = "% bachelor’s or higher") +
  theme_bw()

ggsave(here("figures", "timeseries_direction_by_edu_bucket.png"),
       g_edu, width = 7, height = 4, dpi = 300)
print(g_edu)

res <- res %>%
  mutate(
    z_DQ_prop_garbage  = as.numeric(scale(DQ_prop_garbage)),
    z_prop_light       = as.numeric(scale(prop_light)),
    z_pct_overd_miss   = as.numeric(scale(pct_overd_miss)),
    z_pct_acc_miss     = as.numeric(scale(pct_acc_miss)),
    direction_score    = (z_prop_light + z_pct_overd_miss + z_pct_acc_miss + z_DQ_prop_garbage) / 4
  )

# 11-d · R²: variance explained by education
var_edu <- res %>%                       # 1999-2022 averages
  left_join(edu22, by = "county_ihme") %>%
  filter(!is.na(direction_score), !is.na(pct_bachplus_2022))

fit_edu <- lm(direction_score ~ pct_bachplus_2022, data = var_edu)
r2e  <- summary(fit_edu)$r.squared
r2ep <- round(r2e * 100, 1)

cat("Education R²:", sprintf("%.3f", r2e),
    "→", r2ep, "% of variance in average direction_scores explained by pct bachelor’s+.\n")

message("✓ Time-series plot saved: timeseries_direction_by_edu_bucket.png")
```

```{r}
# ---- County population size bucket (using 2022) ----
library(tidycensus)

# Get 2022 population for each county
pop22_raw <- get_acs(
  geography = "county",
  variables = "B01003_001",
  year = 2022,
  survey = "acs5",
  cache_table = TRUE
)

pop22 <- pop22_raw %>%
  transmute(
    county_ihme = GEOID,
    pop_2022 = estimate
  )

# Create population size buckets
size_breaks <- c(-Inf, 50000, 100000, 250000, 1e6, Inf)
size_labels <- c("<50k", "50–100k", "100–250k", "250k–1M", "≥1M")

direction_year_size <- direction_year %>%
  left_join(pop22, by = "county_ihme") %>%
  mutate(
    size_bucket = cut(pop_2022, breaks = size_breaks, labels = size_labels, right = FALSE)
  )

ts_size <- direction_year_size %>%
  filter(!is.na(size_bucket)) %>%
  group_by(year, size_bucket) %>%
  summarise(avg_direction = mean(direction_score, na.rm = TRUE), .groups = "drop")

g_size <- ggplot(ts_size, aes(year, avg_direction, colour = size_bucket)) +
  geom_line(linewidth = 1) +
  labs(
    title = "Direction-score trend by 2022 county size bucket",
    x = NULL, y = "Mean direction score",
    colour = "2022 size bucket"
  ) +
  theme_bw()

ggsave(here("figures", "timeseries_direction_by_size_bucket.png"),
       g_size, width = 7, height = 4, dpi = 300)
print(g_size)

# ---- County household income bucket (using 2022) ----

# Get 2022 median household income (ACS table B19013)
income22_raw <- get_acs(
  geography = "county",
  variables = "B19013_001",
  year = 2022,
  survey = "acs5",
  cache_table = TRUE
)

income22 <- income22_raw %>%
  transmute(
    county_ihme = GEOID,
    medhhinc_2022 = estimate
  )

# Create household income buckets (edit cutpoints as desired)
income_breaks <- c(-Inf, 45000, 55000, 65000, 75000, Inf)
income_labels <- c("<$45k", "$45–55k", "$55–65k", "$65–75k", "$75k+")

direction_year_income <- direction_year %>%
  left_join(income22, by = "county_ihme") %>%
  mutate(
    income_bucket = cut(medhhinc_2022, breaks = income_breaks, labels = income_labels, right = FALSE)
  )

ts_income <- direction_year_income %>%
  filter(!is.na(income_bucket)) %>%
  group_by(year, income_bucket) %>%
  summarise(avg_direction = mean(direction_score, na.rm = TRUE), .groups = "drop")

g_income <- ggplot(ts_income, aes(year, avg_direction, colour = income_bucket)) +
  geom_line(linewidth = 1) +
  labs(
    title = "Direction-score trend by 2022 income bucket",
    x = NULL, y = "Mean direction score",
    colour = "Household income"
  ) +
  theme_bw()

ggsave(here("figures", "timeseries_direction_by_income_bucket.png"),
       g_income, width = 7, height = 4, dpi = 300)
print(g_income)

```
map diversity
```{r}
# ──────────────────────────────────────────────────────────────
# Map cluster-level diversity (S, Rao's Q) — with NA fill-in
# ──────────────────────────────────────────────────────────────
suppressPackageStartupMessages({
  library(dplyr); library(readr); library(stringr); library(purrr)
  library(sf); library(tigris); library(ggplot2); library(glue); library(here)
})

options(tigris_use_cache = TRUE, tigris_class = "sf")
crs_proj <- 2163

# 0) Inputs -----------------------------------------------------
div_tbl <- read_csv(here("output", "cluster_cod_diversity.csv"), show_col_types = FALSE)
stopifnot(all(c("cluster","period","deaths","S","I") %in% names(div_tbl)))

ccm <- read_csv(here("output", "county_cluster_membership_all_periods.csv"),
                show_col_types = FALSE) %>%
  mutate(fips = stringr::str_pad(as.character(fips), 5, pad = "0")) %>%
  select(fips, period, cluster)

# match your analysis/mapping vintages
shapefile_years <- c("1999_2004" = 2000,
                     "2005_2010" = 2010,
                     "2011_2017" = 2015,
                     "2018_2022" = 2020)

# 1) Helpers ----------------------------------------------------
.pick_fips <- function(df) {
  hits <- grep("^GEOID", names(df), value = TRUE)
  if (length(hits) >= 1) return(df[[hits[1]]])
  if (all(c("STATEFP","COUNTYFP") %in% names(df))) return(paste0(df$STATEFP, df$COUNTYFP))
  stop("No GEOID or STATEFP/COUNTYFP in counties() output: ", paste(names(df), collapse = ", "))
}

build_graph_for_year <- function(year) {
  sf_raw <- tigris::counties(year = year, cb = TRUE, class = "sf") |>
    sf::st_zm(drop = TRUE, what = "ZM") |>
    sf::st_transform(crs_proj)

  # compute FIPS outside mutate (no '.' pronoun issues)
  fips_vec <- stringr::str_pad(.pick_fips(sf_raw), 5, pad = "0")

  sf <- sf_raw |>
    dplyr::mutate(fips = fips_vec) |>
    dplyr::select(fips, geometry)

  adj <- sf::st_touches(sf)
  edges <- tibble::tibble(
    from = rep(sf$fips, lengths(adj)),
    to   = sf$fips[unlist(adj)]
  ) |>
    dplyr::filter(from < to)

  g <- igraph::graph_from_data_frame(edges, directed = FALSE, vertices = sf$fips)
  list(sf = sf, g = g)
}

# Fill missing cluster labels by assigning each unlabeled county
# to the modal cluster among its neighboring counties (or the largest cluster overall if isolated).
fix_na_by_nearest <- function(clu_named_vec, g) {
  stopifnot(!is.null(names(clu_named_vec)))
  unlabeled <- names(clu_named_vec)[is.na(clu_named_vec)]
  if (!length(unlabeled)) return(clu_named_vec)

  for (u in unlabeled) {
    # neighbors present in the vector
    neigh <- igraph::neighbors(g, u) |> names()
    neigh <- neigh[neigh %in% names(clu_named_vec)]
    nclu  <- clu_named_vec[neigh]
    nclu  <- nclu[!is.na(nclu)]

    if (length(nclu)) {
      tab <- sort(table(nclu), decreasing = TRUE)
      clu_named_vec[u] <- names(tab)[1]
    } else {
      # total fallback: largest cluster overall among labeled
      tab <- sort(table(clu_named_vec[!is.na(clu_named_vec)]), decreasing = TRUE)
      if (length(tab)) clu_named_vec[u] <- names(tab)[1]
    }
  }
  clu_named_vec
}

make_cluster_map <- function(sf_data, var, title = NULL) {
  states <- tigris::states(cb = TRUE, class = "sf") %>% sf::st_transform(crs_proj)
  vr   <- range(sf_data[[var]], na.rm = TRUE)
  lims <- c(floor(min(vr)*1000)/1000, ceiling(max(vr)*1000)/1000)
  plot_title <- if (is.null(title)) var else title

  ggplot() +
    geom_sf(data = sf_data, aes(fill = .data[[var]]), colour = NA) +
    geom_sf(data = states, fill = NA, colour = "white", linewidth = 0.3) +
    scale_fill_distiller(palette = "RdBu", direction = -1,
                         limits = lims, oob = scales::squish, na.value = "grey90") +
    coord_sf(xlim = c(-2500000, 2500000),
             ylim = c(-2200000, 730000),
             expand = FALSE) +
    labs(title = plot_title, fill = NULL) +
    theme_void() +
    theme(
      plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
      legend.text = element_text(size = 9)
    )
}

# 2) Build & plot (fills NAs before union) ---------------------
out_dir <- here("figures", "cluster_diversity_maps")
dir.create(out_dir, showWarnings = FALSE, recursive = TRUE)

imap(shapefile_years, function(yr, win) {
  message("Mapping ", win, " …")

  # counties + graph for THIS period's geography
  geo <- build_graph_for_year(yr)
  counties_sf <- geo$sf
  g_period    <- geo$g

  # membership for this period; ensure vector named by fips
  mem <- ccm %>% filter(period == win) %>% select(fips, cluster)

  # start from all counties in that year — join, then fill missing cluster labels
  all_cty <- counties_sf %>% left_join(mem, by = "fips")
  clu_vec <- setNames(all_cty$cluster, all_cty$fips)
  clu_vec <- fix_na_by_nearest(clu_vec, g_period)

  # attach filled clusters back
  all_cty$cluster <- unname(clu_vec[all_cty$fips])

  # build cluster polygons
  clusters_sf <- all_cty %>%
    group_by(cluster) %>%
    summarise(geometry = sf::st_union(geometry), .groups = "drop")

  # attach diversity for this period
  div_p <- div_tbl %>% filter(period == win) %>% select(cluster, S, I, deaths)

  sf_dat <- clusters_sf %>% left_join(div_p, by = "cluster")

  # plot & save
  pS <- make_cluster_map(sf_dat, "S", glue("Cause-of-death diversity S — {win}"))
  pI <- make_cluster_map(sf_dat, "I", glue("CoD inequality (Rao’s Q) — {win}"))

  ggsave(filename = file.path(out_dir, glue("cluster_S_{win}.png")), plot = pS, width = 8, height = 6, dpi = 320)
  ggsave(filename = file.path(out_dir, glue("cluster_I_{win}.png")), plot = pI, width = 8, height = 6, dpi = 320)

  print(pS); print(pI)
})
```
Correlate Phillips detail with age-bucket shares by period
```{r}
suppressPackageStartupMessages({
  library(arrow); library(dplyr); library(stringr); library(readr)
  library(tidyr); library(here)
})

parquet_dir <- if (fs::dir_exists(here("data_private","mcod"))) here("data_private","mcod") else here("data_private","mcod_sample")
county_var  <- "county_ihme"

detail_df <- read_csv(here("output","cluster_phillips_detail.csv"), show_col_types = FALSE) %>%
  transmute(period, cluster, detail = coalesce(detail_phillips_refsize, detail_phillips_raw)) %>%
  distinct()

ccm <- read_csv(here("output","county_cluster_membership.csv"), show_col_types = FALSE) %>%
  mutate(fips = str_pad(as.character(fips), 5, pad = "0")) %>%
  select(fips, period, cluster)

period_of <- function(y){
  dplyr::case_when(
    y >= 1999 & y <= 2006 ~ "1999_2006",
    y >= 2007 & y <= 2014 ~ "2007_2014",
    y >= 2015 & y <= 2022 ~ "2015_2022",
    TRUE ~ NA_character_
  )
}

ds <- open_dataset(parquet_dir)
stopifnot(all(c("year", county_var, "age") %in% names(ds$schema)))

age_levels <- c("0-14","15-24","25-44","45-64","65-74","75-84","85+")
cuts <- c(-Inf, 14, 24, 44, 64, 74, 84, Inf)

# --- ONE row per (period, cluster, age_bucket) ---
ages_clu <- ds %>%
  filter(!is.na(year), !is.na(!!sym(county_var)), !is.na(age)) %>%
  select(year, !!sym(county_var), age) %>%
  collect() %>%
  mutate(fips = str_pad(as.character(.data[[county_var]]), 5, pad = "0"),
         period = period_of(year)) %>%
  filter(!is.na(period), is.finite(age), age >= 0, age <= 110) %>%
  inner_join(ccm, by = c("fips","period")) %>%
  mutate(age_bucket = cut(age, breaks = cuts, labels = age_levels, right = TRUE)) %>%
  count(period, cluster, age_bucket, name = "n") %>%
  group_by(period, cluster) %>%
  mutate(share = n / sum(n)) %>%
  ungroup() %>%
  distinct(period, cluster, age_bucket, share)   # <- dedupe

ages_det <- ages_clu %>% inner_join(detail_df, by = c("period","cluster"))

# sanity: cluster counts should match detail_df
ages_det %>% distinct(period, cluster) %>% count(period, name = "n_clusters_actual")
# compare to:
detail_df %>% count(period, name = "n_clusters_expected")

# --- Cross-cluster correlations (per period × bucket) ---
age_corr <- ages_det %>%
  group_by(period, age_bucket) %>%
  summarise(
    n_clusters = n_distinct(cluster[is.finite(share) & is.finite(detail)]),
    cor_pearson  = {x<-share; y<-detail; if (sd(x,na.rm=TRUE)==0 || sd(y,na.rm=TRUE)==0) NA_real_
                    else suppressWarnings(cor(x,y,use="complete.obs"))},
    cor_spearman = {x<-share; y<-detail; if (sd(x,na.rm=TRUE)==0 || sd(y,na.rm=TRUE)==0) NA_real_
                    else suppressWarnings(cor(x,y,use="complete.obs",method="spearman"))},
    .groups = "drop"
  ) %>% arrange(age_bucket, period)

write_csv(age_corr, here("output","corr_detail_by_agebucket_timeseries_FIXED.csv"))
age_corr
```
maps phillips detail metric
```{r}


# ──────────────────────────────────────────────────────────────
# Map Phillips detail — CLUSTERS ONLY (by period)
# ──────────────────────────────────────────────────────────────
suppressPackageStartupMessages({
  library(dplyr); library(readr); library(stringr); library(purrr)
  library(sf); library(tigris); library(ggplot2); library(glue); library(here)
})

options(tigris_use_cache = TRUE, tigris_class = "sf")
crs_proj <- 2163

# ◼ Choose which metric to plot: "detail_phillips_refsize" or "detail_phillips_raw"
VAR <- "detail_phillips_refsize"

# ‣ Inputs -----------------------------------------------------
clu_det <- read_csv(here("output", "cluster_phillips_detail.csv"), show_col_types = FALSE)
stopifnot(all(c("cluster","period") %in% names(clu_det)))

# if refsize column isn’t present (older runs), fall back to raw
if (!VAR %in% names(clu_det)) {
  message("Requested VAR '", VAR, "' not found. Falling back to 'detail_phillips_raw'.")
  VAR <- "detail_phillips_raw"
  stopifnot(VAR %in% names(clu_det))
}

ccm <- read_csv(here("output", "county_cluster_membership.csv"), show_col_types = FALSE) %>%
  mutate(fips = str_pad(as.character(fips), 5, pad = "0")) %>%
  select(fips, period, cluster)

# ‣ Period → shapefile year (UPDATED periods) ------------------
shapefile_years <- c("1999_2006" = 2000,
                     "2007_2014" = 2010,
                     "2015_2022" = 2020)

# ‣ Helpers ----------------------------------------------------
.pick_fips <- function(df) {
  hits <- grep("^GEOID", names(df), value = TRUE)
  if (length(hits) >= 1) return(df[[hits[1]]])
  if (all(c("STATEFP","COUNTYFP") %in% names(df))) return(paste0(df$STATEFP, df$COUNTYFP))
  stop("No GEOID or STATEFP/COUNTYFP columns found in counties() output. Names: ",
       paste(names(df), collapse = ", "))
}

build_cluster_sf <- function(period_name, shp_year, ccm_df) {
  counties_raw <- tigris::counties(year = shp_year, cb = TRUE, class = "sf") %>%
    sf::st_zm(drop = TRUE, what = "ZM") %>%
    sf::st_transform(crs_proj)

  counties_sf <- counties_raw %>%
    mutate(fips = str_pad(.pick_fips(counties_raw), 5, pad = "0")) %>%
    select(fips, geometry)

  m <- ccm_df %>% filter(period == period_name)
  if (nrow(m) == 0) stop("No county→cluster rows for period ", period_name)

  j <- counties_sf %>% left_join(m, by = "fips") %>% filter(!is.na(cluster))

  clusters_sf <- j %>%
    group_by(cluster) %>%
    summarise(geometry = sf::st_union(geometry), .groups = "drop") %>%
    suppressWarnings()
  clusters_sf
}

make_map <- function(sf_data, var, title = NULL) {
  states <- tigris::states(cb = TRUE, class = "sf") %>% sf::st_transform(crs_proj)
  lims <- c(50, 70)  # fixed scale range
  plot_title <- if (is.null(title)) var else title

  ggplot() +
    geom_sf(data = sf_data, aes(fill = .data[[var]]), colour = NA) +
    geom_sf(data = states, fill = NA, colour = "white", linewidth = 0.3) +
    scale_fill_distiller(palette = "RdBu", direction = 1,
                         limits = lims, oob = scales::squish, na.value = "grey90") +
    coord_sf(xlim = c(-2500000, 2500000),
             ylim = c(-2200000, 730000),
             expand = FALSE) +
    labs(title = plot_title, fill = NULL) +
    theme_void() +
    theme(
      plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
      legend.text = element_text(size = 9)
    )
}
# ‣ Output dir -------------------------------------------------
out_dir_clu <- here("figures","phillips_maps",
                    paste0("cluster_", if (VAR=="detail_phillips_refsize") "refsize" else "raw"))
dir.create(out_dir_clu, recursive = TRUE, showWarnings = FALSE)
dev_png <- "png"  # switch to ragg if you like

# ‣ Build + save (cluster‑only) --------------------------------
imap(shapefile_years, function(yr, win) {
  message("Mapping clusters — ", win, "…")

  cl_sf  <- build_cluster_sf(win, yr, ccm)
  dat_cl <- clu_det %>% filter(period == win) %>% select(cluster, !!VAR)
  sf_cl  <- cl_sf %>% left_join(dat_cl, by = "cluster")

  pC <- make_map(sf_cl, VAR, glue("Phillips detail — {if (VAR=='detail_phillips_refsize') 'm=2000' else 'raw'} — {win}"))
  ggsave(filename = file.path(out_dir_clu, glue("cluster_phillips_{if (VAR=='detail_phillips_refsize') 'refsize' else 'raw'}_{win}.png")),
         plot = pC, width = 8, height = 6, dpi = 320, device = dev_png)
})
```

```{r}
# ──────────────────────────────────────────────────────────────
# DIAGNOSTICS for early-year blanks
# ──────────────────────────────────────────────────────────────
suppressPackageStartupMessages({
  library(dplyr); library(readr); library(stringr); library(tidyr); library(purrr); library(here)
})

finance_dir <- here("data_raw", "finance")

# helper readers (same signatures as the builder)
std_fips <- function(x) stringr::str_pad(as.character(x), 5, pad = "0")
digits   <- function(x) stringr::str_replace_all(as.character(x), "[^0-9]", "")

read_pid_line_map <- function(path) {
  ln <- read_lines(path, progress = FALSE); ln <- ln[grepl("^\\d{12}", ln)]
  if (!length(ln)) return(tibble())
  tibble(raw = ln) |>
    mutate(govsid12 = substr(raw, 1, 12),
           state2 = substr(govsid12, 1, 2),
           ten_tokens = str_extract_all(raw, "\\d{10}")) |>
    rowwise() |>
    mutate(token10 = { toks <- ten_tokens; pick <- toks[substr(toks,1,2)==state2]; if(!length(pick) && length(toks)) pick <- toks[1]; if(!length(pick)) NA_character_ else pick[1]},
           county_ihme = if (!is.na(token10)) substr(token10,1,5) else NA_character_) |>
    ungroup() |>
    transmute(govsid12, county_ihme = std_fips(county_ihme)) |>
    filter(!is.na(county_ihme), grepl("^\\d{5}$", county_ihme)) |>
    distinct()
}

# Fin_GID_* files have a very similar structure to Fin_PID_* for our purpose
read_gid_line_map <- function(path) {
  ln <- read_lines(path, progress = FALSE); ln <- ln[grepl("^\\d{12}", ln)]
  if (!length(ln)) return(tibble())
  tibble(raw = ln) |>
    mutate(govsid12 = substr(raw, 1, 12),
           state2   = substr(govsid12, 1, 2),
           ten_tokens = str_extract_all(raw, "\\d{10}")) |>
    rowwise() |>
    mutate(token10 = { toks <- ten_tokens; pick <- toks[substr(toks,1,2)==state2]; if(!length(pick) && length(toks)) pick <- toks[1]; if(!length(pick)) NA_character_ else pick[1]},
           county_ihme = if (!is.na(token10)) substr(token10,1,5) else NA_character_) |>
    ungroup() |>
    transmute(govsid12, county_ihme = std_fips(county_ihme)) |>
    filter(!is.na(county_ihme), grepl("^\\d{5}$", county_ihme)) |>
    distinct()
}

smart_read_any <- function(path, guess_max = 200000) {
  for (f in list(
    function() read_delim(path, delim = "\t", show_col_types = FALSE, guess_max = guess_max, progress = FALSE),
    function() read_csv(path, show_col_types = FALSE, guess_max = guess_max, progress = FALSE),
    function() read_table(path, show_col_types = FALSE, guess_max = guess_max, col_names = TRUE, na = c("", "NA"))
  )) {
    out <- try(f(), silent = TRUE)
    if (!inherits(out, "try-error") && is.data.frame(out) && ncol(out) >= 1) return(out)
  }
  stop("Could not parse file: ", basename(path))
}

# very light reader: just enough to see item_code + ids + year
peek_fin <- function(path) {
  df <- try(smart_read_any(path), silent = TRUE)
  if (!inherits(df, "try-error")) {
    id_col   <- names(df)[grepl("^govs?id$|^id$|^unit(id|_id)$|^govsid$|^lgid$|^pid$", names(df), ignore.case = TRUE)][1]
    item_col <- names(df)[grepl("^[A-Za-z]?[0-9]{2,3}[A-Za-z]?$|^item(code)?$|^code$|^variable$", names(df), ignore.case = TRUE)][1]
    amt_col  <- names(df)[grepl("^amount$|^value$|^data$|^amnt$|^val$", names(df), ignore.case = TRUE)][1]
    yr <- as.integer(stringr::str_match(basename(path), "^(\\d{4})")[,2])
    tibble(
      src = basename(path),
      fin_year = yr,
      item_code = if (!is.na(item_col)) as.character(df[[item_col]]) else NA_character_,
      id = if (!is.na(id_col)) as.character(df[[id_col]]) else NA_character_,
      amount = suppressWarnings(as.numeric(gsub(",", "", if (!is.na(amt_col)) df[[amt_col]] else NA)))
    )
  } else {
    tibble(src = basename(path), fin_year = NA_integer_, item_code = NA_character_, id = NA_character_, amount = NA_real_)
  }
}

fin_files <- c(
  list.files(finance_dir, pattern = "^\\d{4}FinEstDAT_.*_pu\\.txt$", full.names = TRUE),
  list.files(finance_dir, pattern = "^\\d{4}FinInddiv.*\\.txt$",      full.names = TRUE)
)

cat("# files found:", length(fin_files), "\n")
peek <- purrr::map_dfr(fin_files, peek_fin) %>%
  mutate(code_shape = case_when(
    grepl("^[0-9]{2}[A-Z]$", item_code) ~ "NNL (e.g., 19T)",
    grepl("^[A-Z][0-9]{2}$", item_code) ~ "LNN (e.g., T19/E32)",
    TRUE ~ "other/NA"
  ))

cat("\n## Item code shapes by year\n")
print(peek %>% count(fin_year, code_shape, sort = FALSE) %>% arrange(fin_year, code_shape))

# map sources → county
pid_paths <- list.files(finance_dir, pattern = "^Fin_PID_\\d{4}\\.txt$", full.names = TRUE)
gid_paths <- list.files(finance_dir, pattern = "^Fin_GID_\\d{4}\\.txt$", full.names = TRUE)
pid_map <- if (length(pid_paths)) map_dfr(pid_paths, read_pid_line_map) %>% distinct() else tibble()
gid_map <- if (length(gid_paths)) map_dfr(gid_paths, read_gid_line_map) %>% distinct() else tibble()
map_all <- bind_rows(pid_map, gid_map) %>% distinct()

cat("\n## GOVSID12 → county mapping rows\n")
print(tibble(source = c("Fin_PID_*", "Fin_GID_*", "combined"),
             rows = c(nrow(pid_map), nrow(gid_map), nrow(map_all))))

# coverage of county mapping by year (using ids in the files)
id_years <- peek %>%
  mutate(govsid12 = substr(digits(id), 1, 12)) %>%
  filter(!is.na(fin_year), nchar(govsid12) == 12) %>%
  distinct(fin_year, govsid12)

cov <- id_years %>%
  left_join(map_all, by = "govsid12") %>%
  mutate(status = if_else(is.na(county_ihme), "not_mapped", "mapped")) %>%
  count(fin_year, status, name = "n") %>%
  tidyr::pivot_wider(names_from = status, values_from = n, values_fill = 0) %>%
  { df <- .;                         # ensure both columns exist
    if (!"mapped" %in% names(df)) df$mapped <- 0L
    if (!"not_mapped" %in% names(df)) df$not_mapped <- 0L
    df
  } %>%
  arrange(fin_year) %>%
  select(fin_year, mapped, not_mapped)

print(cov, n = Inf)
```


Validation chunk
```{r}
# ──────────────────────────────────────────────────────────────
# FINANCE → POP → VALIDATION (E32-only, nearest-year snap, cluster PH)
#   • Reads ALL FinEstDAT/FinInddiv files in data_raw/finance
#   • Uses ONLY E32 (Public Health Expenditure) → NO Function 19 fallback
#   • Maps to counties via PID rosters + PID↔GID crosswalk
#   • Builds fin_all (E32 only) + pop_join + ph_pc_fin
#   • "Snap to nearest" finance year for each observation year
#   • Cluster-level PH per-capita (population-weighted) & quintiles by cluster
#   • Plots:
#        1) diversity_z_by_reporting_type_timeseries.png     (if rep_lu available)
#        2) direction_diversity_by_ph_spend_quintile_timeseries.png    (county-year, snapped)
#        3) codstd_phillips_diversity_by_ph_spend_quintile_timeseries.png (CLUSTER-level)
#        4) phillips_detail_cstd_by_reporting_type_timeseries.png  (if rep_lu available)
# ──────────────────────────────────────────────────────────────
suppressPackageStartupMessages({
  library(readr);  library(dplyr);  library(stringr); library(tidyr);  library(purrr)
  library(here);   library(ggplot2); library(scales); library(tidycensus)
})

finance_dir <- here("data_raw", "finance")
out_dir     <- here("figures", "phillips_validation")
dir.create(out_dir, recursive = TRUE, showWarnings = FALSE)

# -------------------- helpers --------------------
`%||%` <- function(a,b) if (is.null(a) || is.na(a)) b else a
std_fips   <- function(x) stringr::str_pad(as.character(x), 5, pad = "0")
std_pid6   <- function(x) stringr::str_pad(stringr::str_replace_all(as.character(x), "[^0-9]", ""), 6, pad = "0")
digits     <- function(x) stringr::str_replace_all(as.character(x), "[^0-9]", "")
msg        <- function(...) cat(paste0("[finance] ", sprintf(...), "\n"))
get_colname <- function(df, patterns) {
  nm <- names(df)
  hit <- which(Reduce(`|`, lapply(patterns, \(p) grepl(p, nm, ignore.case = TRUE))))
  if (length(hit)) nm[hit[1]] else NA_character_
}

smart_read_any <- function(path, guess_max = 200000) {
  suppressWarnings({
    for (f in list(
      function() readr::read_delim(path, delim = "\t", show_col_types = FALSE, guess_max = guess_max, progress = FALSE),
      function() readr::read_csv(path, show_col_types = FALSE, guess_max = guess_max, progress = FALSE),
      function() readr::read_table(path, show_col_types = FALSE, guess_max = guess_max, col_names = TRUE, na = c("", "NA"))
    )) {
      out <- try(f(), silent = TRUE)
      if (!inherits(out, "try-error") && is.data.frame(out) && ncol(out) >= 1) return(out)
    }
  })
  stop("Could not parse file: ", basename(path))
}

# -------------------- 1) PID↔GID crosswalk + PID rosters → FIPS --------------------
cw_path <- file.path(finance_dir, "PID_GID_Crosswalk.txt")
if (!file.exists(cw_path)) stop("Missing crosswalk: ", cw_path)

cw_lines <- readr::read_lines(cw_path, locale = readr::locale(encoding = "Windows-1252"), progress = FALSE)
cw <- tibble(line = cw_lines) %>%
  mutate(
    pid6  = str_extract(line, "^\\s*\\d{6}\\b"),
    gid14 = str_extract(line, "\\b\\d{14}\\b"),
    gid12 = dplyr::coalesce(gid14, str_extract(line, "\\b\\d{12}\\b"))
  ) %>%
  transmute(pid6 = std_pid6(pid6), govsid12 = substr(gid12, 1, 12)) %>%
  filter(!is.na(pid6), !is.na(govsid12), nchar(pid6) == 6, nchar(govsid12) == 12) %>%
  distinct()
msg("Crosswalk PID→GID12 rows: %d", nrow(cw))

read_pid_line_map <- function(path) {
  ln <- readr::read_lines(path, progress = FALSE)
  ln <- ln[grepl("^\\d{12}", ln)]
  if (!length(ln)) return(tibble())
  tibble(raw = ln) %>%
    mutate(
      govsid12   = substr(raw, 1, 12),
      state2     = substr(govsid12, 1, 2),
      ten_tokens = stringr::str_extract_all(raw, "\\d{10}")
    ) %>%
    rowwise() %>%
    mutate(
      token10 = {
        toks <- ten_tokens
        pick <- toks[substr(toks, 1, 2) == state2]
        if (length(pick) == 0 && length(toks) > 0) pick <- toks[1]
        if (length(pick) == 0) NA_character_ else pick[1]
      },
      county_ihme = if (!is.na(token10)) substr(token10, 1, 5) else NA_character_
    ) %>%
    ungroup() %>%
    transmute(govsid12, county_ihme = std_fips(county_ihme)) %>%
    filter(!is.na(county_ihme), grepl("^\\d{5}$", county_ihme)) %>%
    distinct()
}
pid_paths <- list.files(finance_dir, pattern = "^Fin_PID_\\d{4}\\.txt$", full.names = TRUE)
if (!length(pid_paths)) msg("WARNING: No Fin_PID_*.txt files found; county mapping may be incomplete.")
govsid12_to_county <- if (length(pid_paths)) purrr::map_dfr(pid_paths, read_pid_line_map) %>% distinct() else tibble()
msg("GOVSID12→county rows (from PID rosters): %d", nrow(govsid12_to_county))

# -------------------- 2) Readers for FinEstDAT / FinInddiv --------------------
parse_finest_compact <- function(path) {
  ln <- readr::read_lines(path, progress = FALSE)
  ln <- ln[nzchar(ln)]
  if (!length(ln)) return(tibble())

  ln <- sub("\\s+$", "", ln)
  m_year <- stringr::str_match(ln, "(\\d{4})([A-Z])\\s*$")
  keep <- which(!is.na(m_year[,1]))
  if (!length(keep)) stop("Compact parser could not match lines in ", basename(path))

  year <- as.integer(m_year[keep, 2])
  prefix <- stringr::str_trim(stringr::str_sub(ln[keep], end = -(nchar(m_year[keep, 1]) + 1)), side = "right")

  govsid14 <- stringr::str_sub(prefix, 1, 14)
  rest     <- stringr::str_trim(stringr::str_sub(prefix, 15))

  is_sep <- grepl("^[0-9]{2}[A-Z]", rest)
  is_inl <- !is_sep & grepl("^[A-Z]", rest)

  item_num_sep <- ifelse(is_sep, stringr::str_sub(rest, 1, 2), NA_character_)
  item_let_sep <- ifelse(is_sep, stringr::str_sub(rest, 3, 3), NA_character_)
  amt_sep_str  <- ifelse(is_sep, stringr::str_trim(stringr::str_sub(rest, 4)), NA_character_)

  item_num_inl <- ifelse(is_inl, stringr::str_sub(govsid14, 13, 14), NA_character_)
  item_let_inl <- ifelse(is_inl, stringr::str_sub(rest, 1, 1), NA_character_)
  amt_inl_str  <- ifelse(is_inl, stringr::str_trim(stringr::str_sub(rest, 2)), NA_character_)

  item_num <- dplyr::coalesce(item_num_sep, item_num_inl)
  item_let <- dplyr::coalesce(item_let_sep, item_let_inl)
  amt_str  <- dplyr::coalesce(amt_sep_str,  amt_inl_str)

  amount  <- suppressWarnings(as.numeric(gsub("[^0-9\\.-]", "", amt_str)))
  ok      <- !is.na(item_num) & !is.na(item_let) & !is.na(amount)

  tibble(
    pid6       = NA_character_,
    govsid12   = stringr::str_sub(govsid14[ok], 1, 12),
    item_code  = paste0(item_num[ok], item_let[ok]),   # e.g., "19T"
    amount     = amount[ok],
    fin_year   = year[ok],
    county_ihme = NA_character_
  )
}

parse_finest_fixed <- function(path) {
  lines <- readr::read_lines(path, progress = FALSE)
  lines <- lines[nzchar(lines)]
  if (!length(lines)) return(tibble())
  year  <- as.integer(stringr::str_sub(lines, -5, -2))
  flag  <- stringr::str_sub(lines, -1, -1)
  left1 <- stringr::str_sub(lines,  1, -6)
  m <- regexpr("(\\d+)\\s*$", left1, perl = TRUE)
  amt_str <- ifelse(m > 0, regmatches(left1, m), NA_character_)
  amount  <- suppressWarnings(as.numeric(amt_str))
  left2   <- ifelse(m > 0, substr(left1, 1, m - 1L), left1)
  left2   <- sub("\\s+$", "", left2)
  raw_item <- stringr::str_sub(left2, -3, -1)
  govt_id  <- stringr::str_sub(left2,  1, -4)
  item_code <- ifelse(grepl("^[A-Z][0-9]{2}$", raw_item, ignore.case = TRUE),
                      toupper(raw_item),
               ifelse(grepl("^[0-9]{2}[A-Z]$", raw_item, ignore.case = TRUE),
                      paste0(toupper(stringr::str_sub(raw_item, -1, -1)), stringr::str_sub(raw_item, 1, 2)),
                      toupper(raw_item)))
  tibble(
    pid6        = NA_character_,
    govsid12    = substr(govt_id, 1, 12),
    item_code   = item_code,                        # e.g., "E32"
    amount      = amount,
    fin_year    = year,
    county_ihme = NA_character_
  ) %>%
    filter(!is.na(amount), !is.na(fin_year), nchar(govsid12) == 12)
}

read_fin_one <- function(path) {
  dd <- try(parse_finest_compact(path), silent = TRUE)
  if (!inherits(dd, "try-error") && nrow(dd)) return(dd)
  dd2 <- try(parse_finest_fixed(path), silent = TRUE)
  if (!inherits(dd2, "try-error") && nrow(dd2)) return(dd2)
  df <- smart_read_any(path)
  item_col  <- get_colname(df, c("^item$","^code$","^variable$","^category$","^itemcode$"))
  amt_col   <- get_colname(df, c("^amount$","^value$","^amnt$","^val$","^data$"))
  id_col    <- get_colname(df, c("^govs?id$","^id$","^unit(id|_id)$","^govsid$","^lgid$","^pid$"))
  fips_col  <- get_colname(df, c("county.*fips","^fips$","county.?id","geoid.*5","cnty.*fips"))
  if (is.na(item_col) || is.na(amt_col)) {
    wide_candidates <- names(df)[grepl("^[A-Z][0-9]{2,3}[A-Z]?$", names(df))]
    if (length(wide_candidates)) {
      df <- df %>% pivot_longer(cols = all_of(wide_candidates), names_to = "item", values_to = "amount")
      item_col <- "item"; amt_col <- "amount"
      if (is.na(id_col))   id_col   <- get_colname(df, c("^govs?id$","^id$","^unit(id|_id)$","^pid$"))
      if (is.na(fips_col)) fips_col <- get_colname(df, c("county.*fips","^fips$","geoid.*5"))
    }
  }
  if (is.na(item_col) || is.na(amt_col)) stop("Cannot detect item/amount columns in: ", basename(path))
  id_raw    <- if (!is.na(id_col)) as.character(df[[id_col]]) else NA_character_
  id_digits <- digits(id_raw)
  pid6  <- ifelse(nchar(id_digits) <= 6 & nchar(id_digits) > 0, std_pid6(id_digits), NA_character_)
  gid12 <- ifelse(nchar(id_digits) >= 12, substr(id_digits, 1, 12), NA_character_)
  tibble(
    pid6        = pid6,
    govsid12    = gid12,
    county_ihme = if (!is.na(fips_col)) std_fips(df[[fips_col]]) else NA_character_,
    item_code   = as.character(df[[item_col]]),
    amount      = suppressWarnings(as.numeric(gsub(",", "", df[[amt_col]]))),
    fin_year    = as.integer(str_match(basename(path), "^(\\d{4})")[,2])
  )
}
# ─────────────────────────────────────────────────────────────────────────────
# Public Health (E32) spending → snap to nearest finance year → cluster averages
# ─────────────────────────────────────────────────────────────────────────────
suppressPackageStartupMessages({
  library(dplyr); library(readr); library(stringr); library(tidyr); library(purrr); library(here)
})

# ========================= USER CONFIG =======================================
finance_dir     <- here("data_raw","finance")
membership_guess <- c(
  here("output","county_cluster_membership.csv.gz"),
  here("output","county_cluster_membership.csv")
)
USE_PER_CAPITA  <- TRUE   # if FALSE, cluster spending uses *totals*
# If you already have a 2022 ACS population tibble like:
#   pop22 <- tibble(GEOID=<chr 5>, pop_2022=<num>)
# this script can fall back to it when tidycensus isn’t available.
# ============================================================================

# --------------------------- Small helpers -----------------------------------
std_fips <- function(x) stringr::str_pad(as.character(x), 5, pad = "0")
msg <- function(...) message(sprintf(...))

NA_chr <- function(n) rep(NA_character_, n)
NA_int <- function(n) rep(NA_integer_, n)
NA_num <- function(n) rep(NA_real_, n)

nearest_year <- function(y, avail) {
  if (!length(avail) || is.na(y)) return(NA_integer_)
  avail[ which.min(abs(avail - y)) ]
}

safe_quintile <- function(x, n = 5, min_unique = 2) {
  v <- x[is.finite(x)]
  u <- length(unique(v))
  if (u < min_unique) return(factor(rep("All", length(x)), levels = "All"))
  k <- min(n, u)
  qs <- stats::quantile(v, probs = seq(0, 1, length.out = k + 1), na.rm = TRUE, type = 7)
  qs[1] <- min(v, na.rm = TRUE) - 1e-9
  qs[length(qs)] <- max(v, na.rm = TRUE) + 1e-9
  cut(x, breaks = qs, include_lowest = TRUE, right = FALSE, labels = paste0("Q", seq_len(k)))
}

normalize_item_code <- function(x) {
  x <- toupper(trimws(as.character(x)))
  x <- gsub("[^A-Z0-9]", "", x)
  # Normalize variants like "E032", "32E", "E32something"
  x <- ifelse(grepl("^[0-9]{1,3}E$", x), paste0("E", sub("^0*([0-9]{1,3})E$", "\\1", x)), x)
  x <- gsub("^E0*([0-9]{1,3}).*$", "E\\1", x)
  x
}

# --------------------------- Finance reader (typed) ---------------------------
# Return: county_ihme (chr), fin_year (int), amount (num), item_code_norm (chr),
#         func_code (chr), trans_code (chr)
smart_read_fin <- function(path) {
  # Fixed-width-ish quick parse (best effort)
  lines <- try(read_lines(path, progress = FALSE), silent = TRUE)
  if (!inherits(lines, "try-error") && length(lines)) {
    yrm <- stringr::str_match(lines, "(\\d{4})\\s*[A-Za-z]?$")
    if (any(!is.na(yrm[,1]))) {
      keep <- which(!is.na(yrm[,1]))
      n <- length(keep)
      year <- suppressWarnings(as.integer(yrm[keep, 2]))
      pref <- stringr::str_trim(stringr::str_sub(lines[keep], end = -(nchar(yrm[keep,1]) + 1)), side = "right")
      amount   <- suppressWarnings(as.numeric(gsub(".*?([0-9][0-9,\\.]+)\\s*$", "\\1", pref)))
      item_raw <- toupper(sub("^\\s*([A-Z0-9]{1,6}).*$", "\\1", pref))
      out <- tibble::tibble(
        county_ihme    = NA_chr(n),
        fin_year       = year,
        amount         = amount,
        item_code_norm = normalize_item_code(item_raw),
        func_code      = NA_chr(n),
        trans_code     = NA_chr(n)
      ) |>
        filter(!is.na(fin_year), is.finite(amount))
      if (nrow(out)) return(out)
    }
  }

  # Delimited: TSV → CSV → whitespace
  parsers <- list(
    function() readr::read_delim(path, delim = "\t", show_col_types = FALSE, progress = FALSE),
    function() readr::read_csv(path, show_col_types = FALSE, progress = FALSE),
    function() readr::read_table(path, show_col_types = FALSE)
  )
  df <- NULL
  for (f in parsers) {
    tmp <- try(f(), silent = TRUE)
    if (!inherits(tmp, "try-error") && is.data.frame(tmp) && ncol(tmp)) { df <- tmp; break }
  }
  if (is.null(df)) stop("Could not parse: ", basename(path))

  nm <- names(df); n <- nrow(df)
  year_col <- nm[grepl("^f(isc|is_)?year$|^year$|^fy$", nm, ignore.case = TRUE)][1]
  amt_col  <- nm[grepl("^amount$|^value$|^expend", nm, ignore.case = TRUE)][1]
  item_col <- nm[grepl("^item$|^code$|^object$|^acct|^func.?obj|^function.?object|^objcode", nm, ignore.case = TRUE)][1]
  func_col <- nm[grepl("^function$|^func(code)?$", nm, ignore.case = TRUE)][1]
  tran_col <- nm[grepl("^trans|^type$|^object.?type$", nm, ignore.case = TRUE)][1]
  fips_col <- nm[grepl("county.*fips$|^fips$|geoid|cntyid", nm, ignore.case = TRUE)][1]

  fin_year <- if (!is.na(year_col)) suppressWarnings(as.integer(df[[year_col]])) else NA_int(n)
  amount   <- if (!is.na(amt_col))  suppressWarnings(as.numeric(gsub(",", "", df[[amt_col]]))) else NA_num(n)
  item_raw <- if (!is.na(item_col)) toupper(as.character(df[[item_col]])) else NA_chr(n)
  func_raw <- if (!is.na(func_col)) as.character(df[[func_col]]) else NA_chr(n)
  tran_raw <- if (!is.na(tran_col)) as.character(df[[tran_col]]) else NA_chr(n)
  geoid    <- if (!is.na(fips_col)) std_fips(df[[fips_col]]) else NA_chr(n)

  tibble::tibble(
    county_ihme    = geoid,
    fin_year       = fin_year,
    amount         = amount,
    item_code_norm = normalize_item_code(item_raw),
    func_code      = func_raw,
    trans_code     = tran_raw
  ) |>
    filter(!is.na(fin_year), is.finite(amount))
}

# --------------------------- Load finance files -------------------------------
fin_files <- c(
  list.files(finance_dir, pattern = "^\\d{4}.*FinEst.*\\.txt$", full.names = TRUE),
  list.files(finance_dir, pattern = "^\\d{4}.*FinInd.*\\.txt$", full.names = TRUE),
  list.files(finance_dir, pattern = "\\.csv$", full.names = TRUE),
  list.files(finance_dir, pattern = "\\.txt$", full.names = TRUE)
) |> unique()

if (!length(fin_files)) stop("No finance files found in: ", finance_dir)

msg("[finance] Reading %d finance files ...", length(fin_files))

fin_list <- purrr::map(fin_files, \(p) {
  out <- try(smart_read_fin(p), silent = TRUE)
  if (inherits(out, "try-error") || is.null(out) || !nrow(out)) return(NULL)
  out |>
    mutate(
      county_ihme    = as.character(county_ihme),
      fin_year       = as.integer(fin_year),
      amount         = as.numeric(amount),
      item_code_norm = as.character(item_code_norm),
      func_code      = as.character(func_code),
      trans_code     = as.character(trans_code)
    )
})
fin_list <- fin_list[!vapply(fin_list, is.null, logical(1))]
if (!length(fin_list)) stop("All finance files failed to parse.")

fin_raw <- dplyr::bind_rows(fin_list)

# --------------------------- Detect E32 (vectorized, no schema mixing) -------
# Rule:
#  • If combined item code is present → use it (E32 True if == "E32")
#  • Else if only FUNC+TRANS schema present → E32 when FUNC==32 & TRANS is expenditure-ish
#
# This avoids mixing (we don't cross-confirm or merge schemas on the same row).
trans_is_exp <- function(x) {
  xu <- toupper(trimws(as.character(x)))
  xu %in% c("E","EXP","EXPEND","EXPENDITURE","EXPENDITURES","EXPENSE","EXPENSES")
}
func_to_int <- function(x) suppressWarnings(as.integer(gsub("[^0-9]", "", as.character(x))))

fin_raw <- fin_raw |>
  mutate(
    has_ic     = !is.na(item_code_norm) & nzchar(item_code_norm),
    is_e32_ic  = has_ic & (item_code_norm == "E32"),
    func_i     = func_to_int(func_code),
    is_e32_ft  = (!has_ic) & !is.na(func_i) & func_i == 32 &
                 trans_is_exp(trans_code),
    is_E32     = is_e32_ic | is_e32_ft
  )

fin_e32 <- fin_raw |>
  filter(is_E32) |>
  filter(!is.na(county_ihme), grepl("^\\d{5}$", county_ihme), is.finite(amount)) |>
  group_by(county_ihme, fin_year) |>
  summarise(ph_exp_total = sum(amount, na.rm = TRUE), .groups = "drop")

if (!nrow(fin_e32)) {
  msg("No E32 rows found. Top signals observed (first 20):")
  print(fin_raw |>
          transmute(sig = paste0(
            "item=", ifelse(is.na(item_code_norm), "NA", item_code_norm),
            "; func=", ifelse(is.na(func_code), "NA", func_code),
            "; trans=", ifelse(is.na(trans_code), "NA", toupper(trans_code))
          )) |>
          count(sig, sort = TRUE) |>
          head(20))
  stop("E32 not detected in your finance files.")
}

fin_all <- fin_e32
yrs_fin <- sort(unique(fin_all$fin_year))
msg("[finance] Built fin_all (E32 only): %,d rows across finance years: %s",
    nrow(fin_all), paste(yrs_fin, collapse = ", "))

# --------------------------- Population for finance years ---------------------
ph_pc_fin <- NULL
using_pop22 <- FALSE

if (USE_PER_CAPITA) {
  can_census <- FALSE
  if (requireNamespace("tidycensus", quietly = TRUE)) {
    if (nzchar(Sys.getenv("CENSUS_API_KEY", ""))) {
      tidycensus::census_api_key(Sys.getenv("CENSUS_API_KEY"), install = FALSE, overwrite = FALSE)
    }
    get_fin_pop <- function(y) {
      pep <- try(
        tidycensus::get_estimates(geography = "county", product = "population",
                                  year = y, variables = "POP", cache_table = TRUE),
        silent = TRUE
      )
      if (!inherits(pep, "try-error") && nrow(pep)) {
        return(pep |> transmute(county_ihme = GEOID, year = y, pop = as.numeric(value)))
      }
      acs_year <- pmin(pmax(y, 2009L), 2023L)
      acs <- try(
        tidycensus::get_acs(geography = "county", variables = "B01001_001",
                            year = acs_year, survey = "acs5", cache_table = TRUE, show_call = FALSE),
        silent = TRUE
      )
      if (!inherits(acs, "try-error") && nrow(acs)) {
        return(acs |> transmute(county_ihme = GEOID, year = y, pop = as.numeric(estimate)))
      }
      tibble::tibble()
    }
    pop_fin <- purrr::map_dfr(yrs_fin, get_fin_pop) |> distinct()
    if (nrow(pop_fin)) {
      can_census <- TRUE
      ph_pc_fin <- fin_all |>
        inner_join(pop_fin, by = c("county_ihme","fin_year" = "year")) |>
        mutate(ph_pc = ph_exp_total / pop) |>
        select(county_ihme, fin_year, ph_pc)
      msg("[pop] Using PEP/ACS population for finance years.")
    }
  }
  if (!can_census) {
    if (exists("pop22") && all(c("GEOID","pop_2022") %in% names(pop22))) {
      using_pop22 <- TRUE
      ph_pc_fin <- fin_all |>
        inner_join(pop22 |> transmute(county_ihme = std_fips(GEOID),
                                      fin_year = NA_integer_, pop = pop_2022),
                   by = "county_ihme") |>
        mutate(ph_pc = ph_exp_total / pop) |>
        select(county_ihme, fin_year, ph_pc)
      msg("[pop] No tidycensus. Falling back to your 2022 ACS population (pop22).")
    } else {
      msg("[pop] No tidycensus key and no 'pop22' object → using TOTAL dollars (not per-capita).")
      USE_PER_CAPITA <- FALSE
    }
  }
}

# --------------------------- Direction score (county-year) --------------------
# Provide either `direction_year` or raw `df` to rebuild it.
if (!exists("direction_year")) {
  if (!exists("df"))
    stop("Provide `direction_year` or a `df` with county_ihme, year and the four inputs to build it.")
  req <- c("county_ihme","year","DQ_prop_garbage","prop_light","pct_overd_miss","pct_acc_miss")
  stopifnot(all(req %in% names(df)))
  df <- df |>
    mutate(county_ihme = std_fips(county_ihme), year = as.integer(year))
  gs <- df |>
    summarise(across(
      c(DQ_prop_garbage, prop_light, pct_overd_miss, pct_acc_miss),
      list(mean = ~mean(.x, na.rm = TRUE), sd = ~sd(.x, na.rm = TRUE)),
      .names = "{.col}_{.fn}"
    ))
  z <- function(x, m, s) (x - m) / s
  direction_year <- df |>
    mutate(
      z_DQ_prop_garbage = z(DQ_prop_garbage, gs$DQ_prop_garbage_mean, gs$DQ_prop_garbage_sd),
      z_prop_light      = z(prop_light,      gs$prop_light_mean,      gs$prop_light_sd),
      z_pct_overd_miss  = z(pct_overd_miss,  gs$pct_overd_miss_mean,  gs$pct_overd_miss_sd),
      z_pct_acc_miss    = z(pct_acc_miss,    gs$pct_acc_miss_mean,    gs$pct_acc_miss_sd),
      direction_score   = (z_prop_light + z_pct_overd_miss + z_pct_acc_miss + z_DQ_prop_garbage) / 4
    ) |>
    select(county_ihme, year, direction_score,
           z_DQ_prop_garbage, z_prop_light, z_pct_overd_miss, z_pct_acc_miss)
}

# --------------------------- Snap each obs-year to nearest finance year -------
avail_fin_years <- sort(unique(fin_all$fin_year))
if (!length(avail_fin_years)) stop("No available finance years in fin_all (after E32 filter).")

if (USE_PER_CAPITA) {
  dir_spend <- direction_year |>
    mutate(fin_year = vapply(year, nearest_year, integer(1), avail = avail_fin_years)) |>
    left_join(ph_pc_fin, by = c("county_ihme","fin_year"))
} else {
  dir_spend <- direction_year |>
    mutate(fin_year = vapply(year, nearest_year, integer(1), avail = avail_fin_years)) |>
    left_join(fin_all, by = c("county_ihme","fin_year"))
}

# --------------------------- Membership + year→period map ---------------------
membership_path <- membership_guess[file.exists(membership_guess)][1]
if (is.na(membership_path)) stop("Could not find county→cluster membership CSV in output/.")

membership <- readr::read_csv(membership_path, show_col_types = FALSE) |>
  transmute(
    county_ihme = std_fips(fips),
    cluster = as.character(cluster),
    period  = as.character(period)
  ) |>
  distinct()

# Build year→period map from labels like "2015_2022"
parse_period <- function(lbl) {
  m <- stringr::str_match(lbl, "^(\\d{4})\\D+(\\d{4})$")
  if (is.na(m[1,1])) stop("Unparsable period label: ", lbl)
  c(start = as.integer(m[1,2]), end = as.integer(m[1,3]))
}
bounds <- unique(membership$period) |>
  tibble::tibble(period = _) |>
  mutate(bounds = map(period, parse_period)) |>
  tidyr::unnest_wider(bounds)
year_to_period <- purrr::pmap_dfr(list(bounds$period, bounds$start, bounds$end),
                                  \(p, s, e) tibble::tibble(year = seq.int(s, e), period = p))

# --------------------------- Observation-year population (weights) ------------
pop_obs <- NULL
if (USE_PER_CAPITA && requireNamespace("tidycensus", quietly = TRUE) &&
    nzchar(Sys.getenv("CENSUS_API_KEY","")) ) {
  tidycensus::census_api_key(Sys.getenv("CENSUS_API_KEY"), install = FALSE, overwrite = FALSE)
  get_obs_pop <- function(yy) {
    pep <- try(
      tidycensus::get_estimates(geography = "county", product = "population",
                                year = yy, variables = "POP", cache_table = TRUE),
      silent = TRUE
    )
    if (!inherits(pep, "try-error") && nrow(pep)) {
      return(pep |> transmute(county_ihme = GEOID, year = yy, pop = as.numeric(value)))
    }
    acs_year <- pmin(pmax(yy, 2009L), 2023L)
    acs <- try(
      tidycensus::get_acs(geography = "county", variables = "B01001_001",
                          year = acs_year, survey = "acs5", cache_table = TRUE, show_call = FALSE),
      silent = TRUE
    )
    if (!inherits(acs, "try-error") && nrow(acs)) {
      return(acs |> transmute(county_ihme = GEOID, year = yy, pop = as.numeric(estimate)))
    }
    tibble::tibble()
  }
  yrs_obs <- sort(unique(direction_year$year))
  pop_obs <- purrr::map_dfr(yrs_obs, get_obs_pop) |> distinct()
  if (!nrow(pop_obs)) pop_obs <- NULL
}

# --------------------------- Cluster-period averages --------------------------
if (USE_PER_CAPITA) {
  if (!is.null(pop_obs)) {
    # population-weighted mean of snapped per-capita across county-years within period
    clu_spend <- dir_spend |>
      left_join(year_to_period, by = "year") |>
      left_join(pop_obs, by = c("county_ihme","year")) |>
      filter(is.finite(ph_pc), is.finite(pop), !is.na(period)) |>
      inner_join(membership, by = c("county_ihme","period")) |>
      group_by(cluster, period) |>
      summarise(ph_pc_cluster = sum(ph_pc * pop, na.rm = TRUE)/sum(pop, na.rm = TRUE), .groups = "drop") |>
      group_by(period) |>
      mutate(spend_quintile = safe_quintile(ph_pc_cluster, n = 5)) |>
      ungroup()
  } else {
    # unweighted mean across county-years within period
    clu_spend <- dir_spend |>
      left_join(year_to_period, by = "year") |>
      filter(is.finite(ph_pc), !is.na(period)) |>
      inner_join(membership, by = c("county_ihme","period")) |>
      group_by(cluster, period) |>
      summarise(ph_pc_cluster = mean(ph_pc, na.rm = TRUE), .groups = "drop") |>
      group_by(period) |>
      mutate(spend_quintile = safe_quintile(ph_pc_cluster, n = 5)) |>
      ungroup()
  }
} else {
  # Totals (not per-capita): mean of snapped totals across county-years → cluster-period
  clu_spend <- dir_spend |>
    left_join(year_to_period, by = "year") |>
    filter(is.finite(ph_exp_total), !is.na(period)) |>
    inner_join(membership, by = c("county_ihme","period")) |>
    group_by(cluster, period) |>
    summarise(ph_total_avg = mean(ph_exp_total, na.rm = TRUE), .groups = "drop") |>
    group_by(period) |>
    mutate(spend_quintile = safe_quintile(ph_total_avg, n = 5)) |>
    ungroup()
}

# --------------------------- Save --------------------------------------------
dir.create(here("output"), showWarnings = FALSE, recursive = TRUE)
outfile <- here("output","cluster_spending_E32_snapped.csv")
readr::write_csv(clu_spend, outfile)
msg("✓ Wrote: %s", outfile)
# -------------------- 4) Population --------------------
census_api_key(Sys.getenv("CENSUS_API_KEY"), install = FALSE, overwrite = FALSE)

get_year_pop <- function(y) {
  pep <- try(
    tidycensus::get_estimates(geography = "county", product = "population", year = y, variables = "POP"),
    silent = TRUE
  )
  if (!inherits(pep, "try-error") && nrow(pep)) {
    return(pep %>% transmute(county_ihme = GEOID, year = y, pop = as.numeric(value)))
  }
  acs_year <- pmin(pmax(y, 2009L), 2023L)
  acs <- try(
    tidycensus::get_acs(geography = "county", variables = "B01001_001", year = acs_year,
                        survey = "acs5", cache_table = TRUE, show_call = FALSE),
    silent = TRUE
  )
  if (!inherits(acs, "try-error") && nrow(acs)) {
    return(acs %>% transmute(county_ihme = GEOID, year = y, pop = as.numeric(estimate)))
  }
  tibble()
}

# Pop for FINANCE years (to compute per-capita in finance space)
yrs_fin <- sort(unique(fin_all$fin_year))
pop_fin <- map_dfr(yrs_fin, get_year_pop) %>% distinct()
if (!nrow(pop_fin)) stop("Could not build population series for finance years; check your Census API key.")

# Per-capita for finance years
ph_pc_fin <- fin_all %>%
  inner_join(pop_fin, by = c("county_ihme","fin_year" = "year")) %>%
  transmute(county_ihme, fin_year, ph_pc = ph_exp_total / pop)

# -------------------- 5) Build/ensure direction_year with z-components --------------------
get_colname2 <- function(df, patterns) {
  nm <- names(df); hit <- which(Reduce(`|`, lapply(patterns, \(p) grepl(p, nm, ignore.case = TRUE))))
  if (length(hit)) nm[hit[1]] else NA_character_
}

ensure_direction_year <- function(direction_year = NULL, df = NULL) {
  if (!is.null(direction_year)) {
    need <- c("county_ihme","year","z_DQ_prop_garbage","z_prop_light","z_pct_overd_miss","z_pct_acc_miss","direction_score")
    has  <- need %in% names(direction_year)
    if (all(has)) return(direction_year %>% mutate(county_ihme = std_fips(county_ihme), year = as.integer(year)))
    # re-compute missing pieces if raw present
  }
  if (is.null(df)) stop("direction_year not supplied and 'df' is not available to rebuild it.")
  req <- c("county_ihme","year","DQ_prop_garbage","prop_light","pct_overd_miss","pct_acc_miss")
  stopifnot(all(req %in% names(df)))
  df <- df %>% mutate(county_ihme = std_fips(county_ihme), year = as.integer(year))
  gs <- df %>%
    summarise(across(
      c(DQ_prop_garbage, prop_light, pct_overd_miss, pct_acc_miss),
      list(mean = ~mean(.x, na.rm = TRUE), sd = ~sd(.x, na.rm = TRUE)),
      .names = "{.col}_{.fn}"
    ))
  z <- function(x, m, s) (x - m) / s
  out <- df %>%
    mutate(
      z_DQ_prop_garbage = z(DQ_prop_garbage, gs$DQ_prop_garbage_mean, gs$DQ_prop_garbage_sd),
      z_prop_light      = z(prop_light,      gs$prop_light_mean,      gs$prop_light_sd),
      z_pct_overd_miss  = z(pct_overd_miss,  gs$pct_overd_miss_mean,  gs$pct_overd_miss_sd),
      z_pct_acc_miss    = z(pct_acc_miss,    gs$pct_acc_miss_mean,    gs$pct_acc_miss_sd),
      direction_score   = (z_prop_light + z_pct_overd_miss + z_pct_acc_miss + z_DQ_prop_garbage) / 4
    ) %>%
    select(county_ihme, year, direction_score, z_DQ_prop_garbage, z_prop_light, z_pct_overd_miss, z_pct_acc_miss)
  out
}

# Use in-scope 'direction_year' if present, else rebuild from 'df'
direction_year <- if (exists("direction_year")) {
  ensure_direction_year(direction_year = get("direction_year"))
} else if (exists("df")) {
  ensure_direction_year(direction_year = NULL, df = get("df"))
} else {
  stop("Need either 'direction_year' or a raw 'df' with required columns.")
}

# -------------------- 6) Optional: reporting-type lookup --------------------
reporting_path_opts <- c(
  here("data_raw", "County-Death-Investigation-System-2018-1-9-2024.csv"),
  "/mnt/data/County-Death-Investigation-System-2018-1-9-2024.csv"
)
reporting_path <- reporting_path_opts[file.exists(reporting_path_opts)][1]
rep_lu <- NULL
if (!is.na(reporting_path)) {
  rep_raw <- readr::read_csv(reporting_path, show_col_types = FALSE)
  fips_col <- get_colname2(rep_raw, c("^fips$","^fips_?code$","geoid","county_?fips","countyrs","fips.*5"))
  type_col <- get_colname2(rep_raw, c("reporting.*type","investigation.*type","death.*investigation.*system","^type$","system"))
  if (!is.na(fips_col) && !is.na(type_col)) {
    rep_lu <- rep_raw %>%
      transmute(county_ihme = std_fips(.data[[fips_col]]),
                reporting_type = trimws(as.character(.data[[type_col]]))) %>%
      filter(nchar(county_ihme)==5, reporting_type!="") %>%
      mutate(reporting_type = dplyr::recode(tolower(reporting_type),
        "medical examiner"="Medical Examiner","me"="Medical Examiner",
        "coroner"="Coroner","mixed"="Mixed","hybrid"="Mixed",
        "sheriff"="Other County Official","justice of the peace"="Other County Official",
        .default = stringr::str_to_title(reporting_type))) %>%
      distinct()
  }
}

# =====================================================================
# ===================== VALIDATION PLOTTING ============================
# =====================================================================

# ---- Common helpers for plotting ----
safe_bucket <- function(x, n = 5, min_unique = 2) {
  v <- x[is.finite(x)]; u <- length(unique(v))
  if (u < min_unique) return(factor(rep("All", length(x)), levels = "All"))
  k <- min(n, u)
  qs <- quantile(v, probs = seq(0, 1, length.out = k + 1), na.rm = TRUE, type = 7)
  qs[1] <- min(v, na.rm = TRUE) - 1e-9
  qs[length(qs)] <- max(v, na.rm = TRUE) + 1e-9
  cut(x, breaks = qs, include_lowest = TRUE, right = FALSE, labels = paste0("Q", seq_len(k)))
}
pretty_label <- function(x) { x <- gsub("^z_", "", x); x <- gsub("_", " ", x); paste0("z(", x, ")") }

# -------------------- A) diversity by reporting type (unchanged logic) --------------------
if (!is.null(rep_lu) && nrow(rep_lu)) {
  div4 <- c("z_DQ_prop_garbage","z_prop_light","z_pct_overd_miss","z_pct_acc_miss")
  ts_type <- direction_year %>%
    select(county_ihme, year, all_of(div4)) %>%
    left_join(rep_lu, by="county_ihme") %>%
    filter(!is.na(reporting_type)) %>%
    pivot_longer(cols = all_of(div4), names_to="div_metric", values_to="z_value") %>%
    mutate(div_metric = recode(div_metric,
      z_DQ_prop_garbage="z(garbage share)",
      z_prop_light="z(light-garbage share)",
      z_pct_overd_miss="z(overdose detail missing)",
      z_pct_acc_miss="z(accident detail missing)")) %>%
    group_by(year, reporting_type, div_metric) %>%
    summarise(avg_z = mean(z_value, na.rm = TRUE), .groups="drop")

  g_type <- ggplot(ts_type, aes(year, avg_z, colour = reporting_type)) +
    geom_line(linewidth=0.9) + geom_point(size=1.6) +
    facet_wrap(~ div_metric, ncol=2, scales="free_y") +
    labs(title="Diversity components (z) by death investigation reporting type",
         x=NULL, y="Mean z-score", colour="Reporting type") +
    theme_bw() + theme(axis.text.x = element_text(angle=20, hjust=1))
  ggsave(here(out_dir, "diversity_z_by_reporting_type_timeseries.png"),
         g_type, width=10, height=7.5, dpi=300)
}

# -------------------- B) COUNTY-YEAR (snapped) quintiles by PH spend --------------------
# Revert to "snap to nearest finance year" per observation year (no global fill)
nearest_fin_year <- function(y, avail) {
  if (!length(avail) || is.na(y)) return(NA_integer_)
  avail[ which.min(abs(avail - y)) ]
}
avail_fin_years <- sort(unique(na.omit(ph_pc_fin$fin_year)))
if (!length(avail_fin_years)) stop("No available finance years after E32 filtering.")

# Attach snapped PH per-capita to each county-year in direction_year
dir_ph_snap <- direction_year %>%
  mutate(fin_year = vapply(year, nearest_fin_year, integer(1), avail = avail_fin_years)) %>%
  left_join(ph_pc_fin, by = c("county_ihme","fin_year"))

# Reverse sign so higher = better (for readability), but do not mutate originals
dir_ph_snap2 <- dir_ph_snap %>%
  mutate(
    z_DQ_prop_garbage_pos = -z_DQ_prop_garbage,
    z_prop_light_pos      = -z_prop_light,
    z_pct_overd_miss_pos  = -z_pct_overd_miss,
    z_pct_acc_miss_pos    = -z_pct_acc_miss
  )

div_map_pos <- c(
  z_DQ_prop_garbage_pos = "z(garbage share)  (higher=better)",
  z_prop_light_pos      = "z(light-garbage)  (higher=better)",
  z_pct_overd_miss_pos  = "z(overdose detail missing)  (higher=better)",
  z_pct_acc_miss_pos    = "z(accident detail missing)  (higher=better)"
)

ts_year <- dir_ph_snap2 %>%
  group_by(year) %>%
  mutate(ph_quintile = safe_bucket(ph_pc, n = 5)) %>%
  ungroup() %>%
  filter(!is.na(ph_quintile)) %>%
  pivot_longer(cols = c(z_DQ_prop_garbage_pos, z_prop_light_pos, z_pct_overd_miss_pos, z_pct_acc_miss_pos),
               names_to = "div_metric", values_to = "z_value") %>%
  mutate(div_metric = recode(div_metric, !!!div_map_pos)) %>%
  group_by(year, ph_quintile, div_metric) %>%
  summarise(avg_z = mean(z_value, na.rm = TRUE), .groups = "drop")

g_year <- ggplot(ts_year, aes(year, avg_z, group = ph_quintile, colour = ph_quintile)) +
  geom_line(linewidth = 0.9, na.rm = TRUE) +
  geom_point(size = 1.6, na.rm = TRUE) +
  facet_wrap(~ div_metric, ncol = 2, scales = "free_y") +
  labs(
    title    = "Diversity components (z, higher = better) by PH spend quintile (county-year, snapped)",
    subtitle = paste0("PH per-capita snapped to nearest finance year: ",
                      paste(avail_fin_years, collapse = ", ")),
    x = NULL, y = "Mean z-score", colour = "Spend quintile"
  ) +
  theme_bw() + theme(axis.text.x = element_text(angle = 20, hjust = 1))

ggsave(here(out_dir, "direction_diversity_by_ph_spend_quintile_timeseries.png"),
       g_year, width = 10, height = 7.5, dpi = 300)

# -------------------- C) CLUSTER-LEVEL PH spend (population-weighted) --------------------
# Load cluster metrics (COD-standardized) and membership
metrics_file    <- here("output", "cluster_metrics_ucr39_cstd.csv.gz")
membership_file <- here("output", "county_cluster_membership.csv.gz")
message("Reading cluster metrics: ", metrics_file)
metrics <- readr::read_csv(metrics_file, show_col_types = FALSE) %>%
  mutate(cluster = as.character(cluster), period = as.character(period))
message("Reading cluster membership: ", membership_file)
membership <- readr::read_csv(membership_file, show_col_types = FALSE) %>%
  transmute(county_ihme = std_fips(fips), cluster = as.character(cluster), period = as.character(period)) %>%
  distinct() %>% filter(!is.na(county_ihme), grepl("^\\d{5}$", county_ihme))

# Choose COD-std diversity columns
num_cols  <- names(metrics)[vapply(metrics, is.numeric, logical(1))]
cand_all  <- num_cols[grepl("detail|entropy|shannon|even|rich|eff(num|ective)|phillips", num_cols, ignore.case = TRUE)]
cand_cstd <- cand_all[grepl("cstd|cod.?std|cause.?standard", cand_all, ignore.case = TRUE)]
div_cols  <- if (length(cand_cstd)) cand_cstd else cand_all
stopifnot(length(div_cols) > 0)

# Z the chosen columns
metrics_z <- metrics
for (nm in div_cols) metrics_z[[paste0("z_", nm)]] <- as.numeric(scale(metrics[[nm]]))
div_z_cols <- paste0("z_", div_cols)
div_label  <- setNames(pretty_label(div_z_cols), div_z_cols)

# Period order + bounds
canonical4 <- c("1999_2005","2006_2012","2013_2019","2020_2022")
period_levels <- if (all(canonical4 %in% unique(metrics$period))) canonical4 else sort(unique(metrics$period))
parse_period_bounds <- function(p) {
  p2 <- trimws(as.character(p))
  m2 <- stringr::str_match(p2, "^(\\d{4})\\D+(\\d{4})$"); m1 <- stringr::str_match(p2, "^(\\d{4})$")
  start <- ifelse(!is.na(m2[,2]), as.integer(m2[,2]), ifelse(!is.na(m1[,2]), as.integer(m1[,2]), NA_integer_))
  end   <- ifelse(!is.na(m2[,3]), as.integer(m2[,3]), ifelse(!is.na(m1[,1]), as.integer(m1[,1]), NA_integer_))
  tibble(start = start, end = end)
}
bounds <- tibble(period = period_levels) %>%
  mutate(parsed = map(period, parse_period_bounds)) %>%
  unnest(parsed) %>%
  arrange(start, end)
bad <- bounds$period[is.na(bounds$start) | is.na(bounds$end)]
if (length(bad)) stop("Unparsable period labels: ", paste(bad, collapse = ", "))

# Map each observation year to a period
year_to_period <- pmap_dfr(list(bounds$period, bounds$start, bounds$end),
                           \(p,s,e) tibble(year = seq.int(s, e), period = p))

# Population for OBSERVATION years (for pop-weighted aggregation)
yrs_obs <- sort(unique(direction_year$year))
pop_obs <- map_dfr(yrs_obs, get_year_pop) %>% distinct()
if (!nrow(pop_obs)) stop("Could not build population series for observation years; check your Census API key.")

# Attach snapped per-capita & observation-year pop; convert to "snap expenditure" for that year
dir_ph_obs <- direction_year %>%
  mutate(fin_year = vapply(year, nearest_fin_year, integer(1), avail = avail_fin_years)) %>%
  left_join(ph_pc_fin, by = c("county_ihme","fin_year")) %>%
  left_join(pop_obs %>% rename(obs_year = year), by = c("county_ihme","year" = "obs_year")) %>%
  filter(is.finite(ph_pc), is.finite(pop)) %>%
  mutate(ph_exp_snap = ph_pc * pop) %>%
  left_join(year_to_period, by = "year") %>%
  filter(!is.na(period))

# Bring in cluster membership (county×period), then aggregate to cluster×period (pop-weighted)
clu_snap <- dir_ph_obs %>%
  inner_join(membership, by = c("county_ihme","period")) %>%
  group_by(cluster, period) %>%
  summarise(
    ph_exp_sum = sum(ph_exp_snap, na.rm = TRUE),
    pop_sum    = sum(pop, na.rm = TRUE),
    ph_pc_cluster = ph_exp_sum / pop_sum,
    .groups = "drop"
  ) %>%
  mutate(period = factor(period, levels = period_levels))

# Quintile CLUSTERS (not counties) within each period
clu_snap <- clu_snap %>%
  group_by(period) %>%
  mutate(ph_quintile = safe_bucket(ph_pc_cluster, n = 5)) %>%
  ungroup()

# Average cluster metrics by cluster PH quintile (equal cluster weight)
ts_spend <- metrics_z %>%
  select(cluster, period, all_of(div_z_cols)) %>%
  inner_join(clu_snap %>% select(cluster, period, ph_quintile), by = c("cluster","period")) %>%
  pivot_longer(cols = all_of(div_z_cols), names_to = "div_metric", values_to = "z_value") %>%
  mutate(div_metric = recode(div_metric, !!!div_label)) %>%
  group_by(period, ph_quintile, div_metric) %>%
  summarise(avg_z = mean(z_value, na.rm = TRUE), .groups = "drop") %>%
  mutate(period = factor(period, levels = period_levels)) %>%
  complete(period, ph_quintile, div_metric)

g_ts <- ggplot(ts_spend, aes(period, avg_z, group = ph_quintile, colour = ph_quintile)) +
  geom_line(linewidth = 0.9, na.rm = TRUE) +
  geom_point(size = 1.6, na.rm = TRUE) +
  facet_wrap(~ div_metric, ncol = 2, scales = "free_y") +
  scale_x_discrete(drop = FALSE) +
  labs(title = "COD-standardized Phillips diversity (z) by CLUSTER PH spend quintile",
       subtitle = "Cluster PH per-capita = sum(E32 $)/sum(pop) over counties & years within each period",
       x = NULL, y = "Mean z-score", colour = "Cluster spend quintile") +
  theme_bw() + theme(axis.text.x = element_text(angle = 20, hjust = 1))
ggsave(here(out_dir, "codstd_phillips_diversity_by_ph_spend_quintile_timeseries.png"),
       g_ts, width = 10, height = 7.5, dpi = 300)

# -------------------- D) Phillips “detail” (COD-std) by reporting type (optional) --------------------
if (!is.null(rep_lu) && nrow(rep_lu)) {
  detail_any  <- num_cols[grepl("detail", num_cols, ignore.case = TRUE)]
  detail_cstd <- detail_any[grepl("cstd|cod.?std|cause.?standard", detail_any, ignore.case = TRUE)]
  detail_cols <- if (length(detail_cstd)) detail_cstd else detail_any
  if (length(detail_cols)) {
    metrics_z2 <- metrics
    for (nm in detail_cols) metrics_z2[[paste0("z_", nm)]] <- as.numeric(scale(metrics[[nm]]))
    detail_z_cols <- paste0("z_", detail_cols)
    detail_labels <- setNames(pretty_label(detail_z_cols), detail_z_cols)

    rp_data <- membership %>%
      inner_join(metrics_z2 %>% select(cluster, period, all_of(detail_z_cols)),
                 by = c("cluster","period")) %>%
      left_join(rep_lu, by = "county_ihme") %>%  # NOTE: membership didn't carry county_ihme here; optional
      distinct(cluster, period, .keep_all = TRUE) %>%   # reduce duplicate joins
      filter(!is.na(period))

    ts_rptype <- rp_data %>%
      pivot_longer(cols = all_of(detail_z_cols), names_to = "div_metric", values_to = "z_value") %>%
      mutate(div_metric = recode(div_metric, !!!detail_labels)) %>%
      group_by(period, div_metric) %>%   # reporting_type is county-level; clusters span mixed counties
      summarise(avg_z = mean(z_value, na.rm = TRUE), .groups = "drop") %>%
      mutate(period = factor(period, levels = period_levels))

    if (nrow(ts_rptype)) {
      g_phillips_type <- ggplot(ts_rptype, aes(period, avg_z, group = div_metric, colour = div_metric)) +
        geom_line(linewidth = 0.9) + geom_point(size = 1.6) +
        labs(title = "Phillips 'detail' diversity (z) — cluster averages",
             x = NULL, y = "Mean z-score", colour = "Metric") +
        theme_bw() + theme(axis.text.x = element_text(angle = 20, hjust = 1))
      ggsave(here(out_dir, "phillips_detail_cstd_by_reporting_type_timeseries.png"),
             g_phillips_type, width = 10, height = 7.5, dpi = 300)
    }
  }
}

message("Done. Figures saved to: ", out_dir)

```
Build public health spending
```{r}
# ──────────────────────────────────────────────────────────────
# Build `fin_all` from fixed-width FinEstDAT (2017/2022) + PID crosswalk
# ──────────────────────────────────────────────────────────────
suppressPackageStartupMessages({
  library(readr); library(dplyr); library(stringr); library(tidyr); library(purrr); library(here)
})

std_fips <- function(x) stringr::str_pad(as.character(x), 5, pad = "0")

# ---- 1) Parser for FinEst Individual Unit fixed-width file ----
# Layout (robustly inferred):
# [govt_id (14 chars)] [item_code (3 chars)] [amount (digits, right-justified)] [year (4)] [flag (1)]
# We parse from the RIGHT to avoid depending on space counts.
parse_finest_fixed <- function(path) {
  stopifnot(file.exists(path))
  lines <- read_lines(path, progress = FALSE)
  # drop blanks
  lines <- lines[nzchar(lines)]
  if (!length(lines)) return(tibble())

  # RIGHT-anchored extraction
  year  <- as.integer(str_sub(lines, -5, -2))
  flag  <- str_sub(lines, -1, -1)
  left1 <- str_sub(lines,  1, -6)                       # everything before year+flag
  # amount is trailing digits on left1
  m <- regexpr("(\\d+)\\s*$", left1, perl = TRUE)
  amt_str <- ifelse(m > 0, regmatches(left1, m), NA_character_)
  amount  <- suppressWarnings(as.numeric(amt_str))
  left2   <- ifelse(m > 0, substr(left1, 1, m - 1L), left1)
  left2   <- rtrim <- sub("\\s+$", "", left2)           # trim right spaces

  # last 3 chars of left2 are the raw item code (alpha+2digits OR 2digits+alpha)
  raw_item <- str_sub(left2, -3, -1)
  govt_id  <- str_sub(left2,  1, -4)

  # Normalize item code to LETTER+2DIGITS (e.g., "E32", "T01")
  item_code <- ifelse(grepl("^[A-Z][0-9]{2}$", raw_item, ignore.case = TRUE),
                      toupper(raw_item),
               ifelse(grepl("^[0-9]{2}[A-Z]$", raw_item, ignore.case = TRUE),
                      paste0(toupper(str_sub(raw_item, -1, -1)), str_sub(raw_item, 1, 2)),
                      toupper(raw_item)))

  tibble(
    govt_id  = govt_id,
    item_code = item_code,
    amount   = amount,
    year     = year,
    flag     = flag
  ) %>%
    filter(!is.na(amount), !is.na(year), nchar(govt_id) >= 10)
}

# ---- 2) PID crosswalk (maps govt_id → county FIPS if available) ----
# PID files vary; try TSV/CSV; look for columns like GOVTID/GOVT_ID and FIPS/GEOID/COUNTYFIPS.
# ──────────────────────────────────────────────────────────────
# Robust PID crosswalk for fixed‑width PID (e.g., Fin_PID_2022.txt)
# Extracts: govt_id (leading digits), county_ihme (stateFIPS + county)
# Lines look like:
# 011003160514BALDWIN COUNTY … 99003   22928722             093022
# ──────────────────────────────────────────────────────────────
read_pid_xwalk <- function(path) {
  if (!file.exists(path)) return(NULL)
  lines <- readr::read_lines(path, progress = FALSE)
  lines <- lines[nzchar(lines)]
  if (!length(lines)) return(NULL)

  # helper: leading digits = GOVTID
  lead_digits <- function(x) sub("^([0-9]+).*$", "\\1", x)

  # find the right-most 5-digit token that starts with "99" (e.g., 99015)
  find_99_code <- function(x) {
    m <- gregexpr("\\b99\\d{3}\\b", x, perl = TRUE)
    if (m[[1]][1] == -1) return(NA_character_)
    # take the last match on the line
    ix <- tail(m[[1]], 1)
    substr(x, ix, ix + attr(m[[1]], "match.length")[length(m[[1]])] - 1)
  }

  tib <- tibble::tibble(raw = lines) %>%
    dplyr::mutate(
      govt_id_raw = lead_digits(raw),
      state_fips  = substr(govt_id_raw, 1, 2),
      code_99     = vapply(raw, find_99_code, character(1)),
      county_ihme = dplyr::if_else(
        !is.na(code_99),
        paste0(state_fips, substr(code_99, 3, 5)),
        NA_character_
      )
    ) %>%
    dplyr::filter(!is.na(county_ihme), nchar(county_ihme) == 5) %>%
    dplyr::transmute(
      govt_id = govt_id_raw,
      county_ihme = stringr::str_pad(county_ihme, 5, pad = "0")
    ) %>%
    dplyr::distinct()

  if (!nrow(tib)) return(NULL)
  tib
}
# ---- 3) Locate files and build fin_all ----
find_first <- function(fname) {
  c(here("data_raw/finance", fname), fname) |> {\(p) p[file.exists(p)][1]}()
}

fin2017_path <- find_first("2017FinEstDAT_09202024modp_pu.txt")
fin2022_path <- find_first("2022FinEstDAT_09202024modp_pu.txt")
pid2017_path <- find_first("Fin_PID_2017.txt")
pid2022_path <- find_first("Fin_PID_2022.txt")

fin_tbls <- list()
if (!is.na(fin2017_path)) fin_tbls <- append(fin_tbls, list(parse_finest_fixed(fin2017_path)))
if (!is.na(fin2022_path)) fin_tbls <- append(fin_tbls, list(parse_finest_fixed(fin2022_path)))
stopifnot(length(fin_tbls) > 0)

fin_raw <- bind_rows(fin_tbls)

# PID crosswalks (optional but recommended for county mapping)
pid_xw <- bind_rows(
  list(read_pid_xwalk(pid2017_path), read_pid_xwalk(pid2022_path)) |> compact()
) %>% distinct()

# ---- 4) Filter to Public Health Expenditures (E32) and summarize by county ----
# item_code "E32" = expenditures, function 32 (Public Health)
fin_e32 <- fin_raw %>% filter(item_code == "E32")

if (nrow(fin_e32) == 0) {
  stop("Parsed finance files but found ZERO rows with item_code == 'E32'. ",
       "Double-check files and item code list; if needed, print a sample of item_code counts.")
}

# Map to counties
if (!nrow(pid_xw)) {
  stop("PID crosswalk not found or had no usable (govt_id, FIPS) columns. ",
       "Please provide Fin_PID_2017.txt / Fin_PID_2022.txt (or their actual paths).")
}

fin_all <- fin_e32 %>%
  left_join(pid_xw, by = "govt_id") %>%
  filter(!is.na(county_ihme)) %>%
  transmute(
    county_ihme = county_ihme,
    fin_year    = as.integer(year),
    ph_exp_total = as.numeric(amount)
  ) %>%
  group_by(county_ihme, fin_year) %>%
  summarise(ph_exp_total = sum(ph_exp_total, na.rm = TRUE), .groups = "drop")

# Sanity check
cat("# fin_all rows:", nrow(fin_all), "\n")
print(fin_all %>% count(fin_year, sort = TRUE))
```
Build pop and other needed things for next chunk
```{r}
# ──────────────────────────────────────────────────────────────
# REPAIR BLOCK — ensure components + build pop_join
# Run this ONCE before the PH-spend plotting code
# ──────────────────────────────────────────────────────────────
suppressPackageStartupMessages({
  library(dplyr); library(tidyr); library(stringr); library(purrr)
})

std_fips <- function(x) stringr::str_pad(as.character(x), 5, pad = "0")
as_county_ihme <- function(df) {
  nm <- names(df)
  id_col <- dplyr::case_when(
    "county_ihme" %in% nm ~ "county_ihme",
    "GEOID"       %in% nm ~ "GEOID",
    "fips"        %in% nm ~ "fips",
    "FIPS"        %in% nm ~ "FIPS",
    "countyrs"    %in% nm ~ "countyrs",
    TRUE ~ NA_character_
  )
  if (is.na(id_col)) stop("No county ID column found. Expect one of county_ihme/GEOID/fips/FIPS/countyrs.")
  df %>% mutate(county_ihme = std_fips(.data[[id_col]]))
}

# 1) Ensure direction_year has component z-cols
ensure_direction_components <- function(direction_year, df) {
  needed_comp <- c("z_DQ_prop_garbage","z_prop_light","z_pct_overd_miss","z_pct_acc_miss")
  if (all(needed_comp %in% names(direction_year))) return(direction_year)

  # We need the raw inputs in `df` to (re)compute z-scores
  req <- c("DQ_prop_garbage","prop_light","pct_overd_miss","pct_acc_miss","year")
  if (!exists("df") || !all(req %in% names(df))) {
    stop("direction_year lacks z-components and `df` is missing required columns: ",
         paste(setdiff(req, names(df)), collapse = ", "))
  }
  df <- as_county_ihme(df)

  # Compute global means/sds once
  gs <- df %>%
    summarise(across(
      c(DQ_prop_garbage, prop_light, pct_overd_miss, pct_acc_miss),
      list(mean = ~mean(.x, na.rm = TRUE), sd = ~sd(.x, na.rm = TRUE)),
      .names = "{.col}_{.fn}"
    ))
  z <- function(x, m, s) (x - m) / s

  # Join raw inputs to direction_year, then add missing z-cols
  direction_year %>%
    left_join(df %>% select(county_ihme, year,
                            DQ_prop_garbage, prop_light, pct_overd_miss, pct_acc_miss),
              by = c("county_ihme","year")) %>%
    mutate(
      z_DQ_prop_garbage = if (!"z_DQ_prop_garbage" %in% names(direction_year))
                            z(DQ_prop_garbage, gs$DQ_prop_garbage_mean, gs$DQ_prop_garbage_sd)
                          else z_DQ_prop_garbage,
      z_prop_light      = if (!"z_prop_light" %in% names(direction_year))
                            z(prop_light, gs$prop_light_mean, gs$prop_light_sd)
                          else z_prop_light,
      z_pct_overd_miss  = if (!"z_pct_overd_miss" %in% names(direction_year))
                            z(pct_overd_miss, gs$pct_overd_miss_mean, gs$pct_overd_miss_sd)
                          else z_pct_overd_miss,
      z_pct_acc_miss    = if (!"z_pct_acc_miss" %in% names(direction_year))
                            z(pct_acc_miss, gs$pct_acc_miss_mean, gs$pct_acc_miss_sd)
                          else z_pct_acc_miss
    ) %>%
    select(-DQ_prop_garbage, -prop_light, -pct_overd_miss, -pct_acc_miss)
}

direction_year <- ensure_direction_components(direction_year, df)

# 2) Build pop_join (ACS preferred; pid_all fallback)
build_pop_join <- function(fin_years) {
  # Reuse if already valid
  if (exists("pop_join", inherits = TRUE)) {
    pj <- get("pop_join", inherits = TRUE)
    if (all(c("county_ihme","fin_year","pop") %in% names(pj))) return(pj)
  }

  # Try ACS via tidycensus
  if (requireNamespace("tidycensus", quietly = TRUE)) {
    message("Building pop_join from ACS (B01001_001, ACS5) …")
    out <- purrr::map_dfr(sort(unique(stats::na.omit(as.integer(fin_years)))), function(y) {
      yy <- max(2009L, min(2023L, as.integer(y)))  # ACS5 window
      tidycensus::get_acs(
        geography = "county", variables = "B01001_001",
        year = yy, survey = "acs5", cache_table = TRUE, show_call = FALSE
      ) %>%
        transmute(county_ihme = GEOID, fin_year = y, pop = estimate)
    })
    if (nrow(out)) return(out)
  }

  # Fallback: pid_all (must exist and have year/pop)
  if (exists("pid_all", inherits = TRUE)) {
    message("Falling back to pid_all for population …")
    pid <- get("pid_all", inherits = TRUE)
    stopifnot(all(c("county_ihme","year","pop") %in% names(pid)))
    return(pid %>% transmute(county_ihme, fin_year = as.integer(year), pop))
  }

  stop("Could not build pop_join: neither ACS nor pid_all available.")
}

# Need fin_years from fin_all
if (!exists("fin_all", inherits = TRUE)) stop("fin_all not found — run the finance builder first.")
fin_all <- as_county_ihme(fin_all)
stopifnot(all(c("county_ihme","fin_year","ph_exp_total") %in% names(fin_all)))

avail_fin_years <- sort(unique(stats::na.omit(as.integer(fin_all$fin_year))))
if (!length(avail_fin_years)) stop("No valid fin_year values in fin_all.")

pop_join <- build_pop_join(avail_fin_years)

# Finally: build ph_pc for downstream chunk
ph_pc <- fin_all %>%
  filter(!is.na(fin_year)) %>%
  left_join(pop_join, by = c("county_ihme","fin_year")) %>%
  mutate(ph_pc = ph_exp_total / pop) %>%
  filter(is.finite(ph_pc))
```
Validation: direction score by public health spending
```{r}
# ──────────────────────────────────────────────────────────────
# 12 · Robust: build direction_year (if missing) + PH spend time series
#     (now also plots each component of the direction score)
# ──────────────────────────────────────────────────────────────
suppressPackageStartupMessages({
  library(dplyr); library(readr); library(stringr); library(tidyr)
  library(ggplot2); library(scales); library(purrr); library(here)
})

options(tigris_use_cache = TRUE)

# -- Helper: standardize to 5-digit county_ihme ----------------
std_fips <- function(x) stringr::str_pad(as.character(x), 5, pad = "0")
as_county_ihme <- function(df) {
  nm <- names(df)
  id_col <- dplyr::case_when(
    "county_ihme" %in% nm ~ "county_ihme",
    "GEOID"       %in% nm ~ "GEOID",
    "fips"        %in% nm ~ "fips",
    "FIPS"        %in% nm ~ "FIPS",
    "countyrs"    %in% nm ~ "countyrs",
    TRUE ~ NA_character_
  )
  if (is.na(id_col)) stop("No county ID column found. Expect one of county_ihme/GEOID/fips/FIPS/countyrs.")
  df %>% mutate(county_ihme = std_fips(.data[[id_col]]))
}

# -- 0) Build direction_year if it doesn't exist ----------------
if (!exists("direction_year")) {
  stopifnot(exists("df"))
  needed <- c("DQ_prop_garbage","prop_light","pct_overd_miss","pct_acc_miss","year")
  if (!all(needed %in% names(df))) stop("df is missing required columns: ", paste(setdiff(needed, names(df)), collapse=", "))
  df <- as_county_ihme(df)

  global_stats <- df %>%
    summarise(across(
      c(DQ_prop_garbage, prop_light, pct_overd_miss, pct_acc_miss),
      list(mean = ~mean(.x, na.rm = TRUE),
           sd   = ~sd(.x, na.rm = TRUE)),
      .names = "{.col}_{.fn}"
    ))
  zscore_global <- function(x, m, s) (x - m) / s

  direction_year <- df %>%
    filter(year >= 1999, year <= 2022) %>%
    mutate(
      z_DQ_prop_garbage  = zscore_global(DQ_prop_garbage, global_stats$DQ_prop_garbage_mean, global_stats$DQ_prop_garbage_sd),
      z_prop_light       = zscore_global(prop_light,      global_stats$prop_light_mean,      global_stats$prop_light_sd),
      z_pct_overd_miss   = zscore_global(pct_overd_miss,  global_stats$pct_overd_miss_mean,  global_stats$pct_overd_miss_sd),
      z_pct_acc_miss     = zscore_global(pct_acc_miss,    global_stats$pct_acc_miss_mean,    global_stats$pct_acc_miss_sd),
      direction_score    = (z_prop_light + z_pct_overd_miss + z_pct_acc_miss + z_DQ_prop_garbage) / 4
    ) %>%
    # KEEP component columns (changed from your original select)
    select(county_ihme, year, direction_score,
           z_DQ_prop_garbage, z_prop_light, z_pct_overd_miss, z_pct_acc_miss) %>%
    filter(!is.na(county_ihme))
}

# -- 1) Finance + population for per-capita ---------------------
fin_all <- as_county_ihme(fin_all)

# Assume you already computed pop_join upstream (ACS or PID fallback)
# (If not, plug in your existing pop_join block here.)

ph_pc <- fin_all %>%
  filter(!is.na(fin_year)) %>%
  left_join(pop_join, by = c("county_ihme","fin_year")) %>%
  mutate(ph_pc = ph_exp_total / pop) %>%
  filter(is.finite(ph_pc))

# --- helpers ---------------------------------------------------
nearest_fin_year <- function(y, avail) {
  if (!length(avail) || is.na(y)) return(NA_integer_)
  avail[ which.min(abs(avail - y)) ]
}
safe_quintile <- function(x) {
  labs <- c("Q1 lowest","Q2","Q3","Q4","Q5 highest")
  v <- x[is.finite(x)]
  if (length(v) < 5L || length(unique(v)) < 5L) {
    return(factor(rep(NA_character_, length(x)), levels = labs))
  }
  qs <- quantile(v, probs = seq(0, 1, 0.2), na.rm = TRUE, names = FALSE, type = 7)
  qs[1] <- min(v) - 1e-9; qs[length(qs)] <- max(v) + 1e-9
  cut(x, breaks = qs, include.lowest = TRUE, right = FALSE, labels = labs)
}

avail_fin_years <- sort(unique(na.omit(fin_all$fin_year)))
if (length(avail_fin_years) == 0L) stop("No valid fin_year values in fin_all.")

# --- build dir_ph with guarded quintiles -----------------------
dir_ph <- direction_year %>%
  mutate(fin_year = vapply(year, nearest_fin_year, integer(1), avail = avail_fin_years)) %>%
  left_join(ph_pc, by = c("county_ihme","fin_year")) %>%
  group_by(fin_year) %>%
  mutate(ph_quintile = safe_quintile(ph_pc)) %>%
  ungroup()

# ---- A) Original overall time series by spend quintile --------
ts_ph <- dir_ph %>%
  filter(!is.na(ph_quintile)) %>%
  group_by(year, ph_quintile) %>%
  summarise(avg_direction = mean(direction_score, na.rm = TRUE), .groups = "drop")

g_ph <- ggplot(ts_ph, aes(year, avg_direction, colour = ph_quintile)) +
  geom_line(linewidth = 1) +
  labs(title = "Direction-score trend by public health spend quintile",
       subtitle = paste0("Per-capita (func 32), snapped to nearest finance year: ",
                         paste(avail_fin_years, collapse = ", ")),
       x = NULL, y = "Mean direction score", colour = "Spend quintile") +
  scale_y_continuous(labels = number_format(accuracy = 0.01)) +
  theme_bw()

ggsave(here("figures", "timeseries_direction_by_ph_spend_quintile.png"),
       g_ph, width = 7, height = 4, dpi = 300)
print(g_ph)

# ---- B) NEW: Component time series by spend quintile ----------
component_lookup <- c(
  z_DQ_prop_garbage = "z(DQ_prop_garbage)",
  z_prop_light      = "z(prop_light)",
  z_pct_overd_miss  = "z(pct_overdose_unspecified)",
  z_pct_acc_miss    = "z(pct_accident_unspecified)"
)

ts_ph_comp <- dir_ph %>%
  filter(!is.na(ph_quintile)) %>%
  pivot_longer(
    cols = c(z_DQ_prop_garbage, z_prop_light, z_pct_overd_miss, z_pct_acc_miss),
    names_to = "component", values_to = "z_value"
  ) %>%
  mutate(component = recode(component, !!!component_lookup)) %>%
  group_by(year, ph_quintile, component) %>%
  summarise(avg_z = mean(z_value, na.rm = TRUE), .groups = "drop")

g_ph_comp <- ggplot(ts_ph_comp, aes(year, avg_z, colour = ph_quintile)) +
  geom_line(linewidth = 0.9) +
  facet_wrap(~ component, ncol = 2, scales = "free_y") +
  labs(title = "Component trends by public health spend quintile",
       subtitle = "Each panel is a z-scored component of the direction score",
       x = NULL, y = "Mean z-score", colour = "Spend quintile") +
  theme_bw()

ggsave(here("figures", "timeseries_components_by_ph_spend_quintile.png"),
       g_ph_comp, width = 8.5, height = 6.5, dpi = 300)
print(g_ph_comp)

# ---- C) Cross-section R² (county averages) vs spend -----------
if (!exists("res")) {
  res <- direction_year %>%
    group_by(county_ihme) %>%
    summarise(direction_score = mean(direction_score, na.rm = TRUE), .groups = "drop")
}
res <- as_county_ihme(res)

latest_fin <- max(avail_fin_years)
var_ph <- res %>%
  select(county_ihme, direction_score) %>%
  left_join(ph_pc %>% filter(fin_year == latest_fin) %>% select(county_ihme, ph_pc),
            by = "county_ihme") %>%
  filter(is.finite(direction_score), is.finite(ph_pc)) %>%
  mutate(log_ph_pc = log10(pmax(ph_pc, 1e-6)))

fit_ph <- lm(direction_score ~ log_ph_pc, data = var_ph)
r2_ph <- summary(fit_ph)$r.squared
cat("Public health spend R² (overall):", sprintf("%.3f", r2_ph),
    "→", round(r2_ph*100,1), "% of variance explained by log10(per-capita PH spend).\n")

# ---- D) NEW: Cross-section R² for each component vs spend ----
# Build county averages for components
comp_avg <- direction_year %>%
  group_by(county_ihme) %>%
  summarise(across(c(z_DQ_prop_garbage, z_prop_light, z_pct_overd_miss, z_pct_acc_miss),
                   ~ mean(.x, na.rm = TRUE), .names = "{.col}"),
            .groups = "drop") %>%
  left_join(ph_pc %>% filter(fin_year == latest_fin) %>% select(county_ihme, ph_pc),
            by = "county_ihme") %>%
  filter(is.finite(ph_pc)) %>%
  mutate(log_ph_pc = log10(pmax(ph_pc, 1e-6)))

for (nm in c("z_DQ_prop_garbage","z_prop_light","z_pct_overd_miss","z_pct_acc_miss")) {
  ff <- as.formula(paste(nm, "~ log_ph_pc"))
  r2 <- summary(lm(ff, data = comp_avg))$r.squared
  cat(sprintf("Public health spend R² (%s): %.3f (%.1f%%)\n",
              component_lookup[[nm]], r2, 100*r2))
}
```
Direction score by reporting type
```{r}
# ──────────────────────────────────────────────────────────────
# 5) Reporting type (ME/Coroner/Mixed/etc.) — time series & R²
#     (now also plots each component of the direction score)
# ──────────────────────────────────────────────────────────────

# 5.1 · Read reporting-type lookup (robust column detection)
reporting_path_opts <- c(
  here("data_raw", "County-Death-Investigation-System-2018-1-9-2024.csv"),
  "/mnt/data/County-Death-Investigation-System-2018-1-9-2024.csv"
)
reporting_path <- reporting_path_opts[file.exists(reporting_path_opts)][1]
if (is.na(reporting_path)) stop("Could not find the reporting-type CSV at either default path.")

rep_raw <- readr::read_csv(reporting_path, show_col_types = FALSE)

get_colname <- function(df, patterns) {
  nm <- names(df)
  hits <- which(Reduce(`|`, lapply(patterns, function(p) grepl(p, nm, ignore.case = TRUE))))
  if (length(hits) == 0) return(NA_character_)
  nm[hits[1]]
}

fips_col <- get_colname(rep_raw, c("^fips$", "^fips_?code$", "geoid", "county_?fips", "countyrs", "fips.*5"))
type_col <- get_colname(rep_raw, c("reporting.*type", "investigation.*type", "death.*investigation.*system", "^type$", "system"))

if (is.na(fips_col) || is.na(type_col)) {
  message("Columns in reporting CSV:\n- ", paste(names(rep_raw), collapse = "\n- "))
  stop("Could not detect FIPS and/or reporting-type column. ",
       "Set them manually, e.g.: fips_col <- 'FIPS'; type_col <- 'Reporting Type'")
}

message("Using columns → FIPS: '", fips_col, "' | reporting type: '", type_col, "'")

rep_lu <- rep_raw %>%
  mutate(
    county_ihme    = std_fips(.data[[fips_col]]),
    reporting_type = trimws(as.character(.data[[type_col]]))
  ) %>%
  filter(!is.na(county_ihme), nchar(county_ihme) == 5, !is.na(reporting_type), reporting_type != "") %>%
  mutate(
    reporting_type = dplyr::recode(tolower(reporting_type),
      "medical examiner" = "Medical Examiner",
      "me"               = "Medical Examiner",
      "coroner"          = "Coroner",
      "mixed"            = "Mixed",
      "hybrid"           = "Mixed",
      .default = stringr::str_to_title(reporting_type)
    )
  ) %>%
  select(county_ihme, reporting_type) %>%
  distinct()

print(rep_lu %>% count(reporting_type, sort = TRUE))

# 5.2 · Time series: mean direction_score by reporting type -----
ts_rep <- direction_year %>%
  left_join(rep_lu, by = "county_ihme") %>%
  filter(!is.na(reporting_type)) %>%
  group_by(year, reporting_type) %>%
  summarise(avg_direction = mean(direction_score, na.rm = TRUE), .groups = "drop")

g_rep <- ggplot(ts_rep, aes(year, avg_direction, colour = reporting_type)) +
  geom_line(linewidth = 1) +
  labs(
    title = "Direction-score trend by death investigation reporting type",
    x = NULL, y = "Mean direction score", colour = "Reporting type"
  ) +
  scale_y_continuous(labels = scales::number_format(accuracy = 0.01)) +
  theme_bw()

ggsave(here("figures", "timeseries_direction_by_reporting_type.png"),
       g_rep, width = 7, height = 4, dpi = 300)
print(g_rep)

# 5.2b · NEW: Components by reporting type (faceted) -----------
component_lookup <- c(
  z_DQ_prop_garbage = "z(DQ_prop_garbage)",
  z_prop_light      = "z(prop_light)",
  z_pct_overd_miss  = "z(pct_overdose_unspecified)",
  z_pct_acc_miss    = "z(pct_accident_unspecified)"
)

ts_rep_comp <- direction_year %>%
  left_join(rep_lu, by = "county_ihme") %>%
  filter(!is.na(reporting_type)) %>%
  pivot_longer(
    cols = c(z_DQ_prop_garbage, z_prop_light, z_pct_overd_miss, z_pct_acc_miss),
    names_to = "component", values_to = "z_value"
  ) %>%
  mutate(component = recode(component, !!!component_lookup)) %>%
  group_by(year, reporting_type, component) %>%
  summarise(avg_z = mean(z_value, na.rm = TRUE), .groups = "drop")

g_rep_comp <- ggplot(ts_rep_comp, aes(year, avg_z, colour = reporting_type)) +
  geom_line(linewidth = 0.9) +
  facet_wrap(~ component, ncol = 2, scales = "free_y") +
  labs(
    title = "Component trends by death investigation reporting type",
    subtitle = "Each panel is a z-scored component of the direction score",
    x = NULL, y = "Mean z-score", colour = "Reporting type"
  ) +
  theme_bw()

ggsave(here("figures", "timeseries_components_by_reporting_type.png"),
       g_rep_comp, width = 8.5, height = 6.5, dpi = 300)
print(g_rep_comp)

# 5.3 · Cross-section R² of county averages vs reporting type --
var_rep <- (if (exists("res")) res else {
  direction_year %>% group_by(county_ihme) %>%
    summarise(direction_score = mean(direction_score, na.rm = TRUE), .groups = "drop")
}) %>%
  left_join(rep_lu, by = "county_ihme") %>%
  filter(!is.na(reporting_type))

if (nrow(var_rep) > 0) {
  fit_rep <- lm(direction_score ~ reporting_type, data = var_rep)
  r2_rep  <- summary(fit_rep)$r.squared
  cat("Reporting-type R² (overall):", sprintf("%.3f", r2_rep),
      "→", round(r2_rep * 100, 1), "% of variance explained.\n")
} else {
  message("No overlap between county averages and reporting-type lookup.")
}

# 5.3b · NEW: Component R² vs reporting type -------------------
comp_rep <- direction_year %>%
  group_by(county_ihme) %>%
  summarise(across(c(z_DQ_prop_garbage, z_prop_light, z_pct_overd_miss, z_pct_acc_miss),
                   ~ mean(.x, na.rm = TRUE), .names = "{.col}"),
            .groups = "drop") %>%
  left_join(rep_lu, by = "county_ihme") %>%
  filter(!is.na(reporting_type))

if (nrow(comp_rep) > 0) {
  for (nm in c("z_DQ_prop_garbage","z_prop_light","z_pct_overd_miss","z_pct_acc_miss")) {
    ff <- as.formula(paste(nm, "~ reporting_type"))
    r2 <- summary(lm(ff, data = comp_rep))$r.squared
    cat(sprintf("Reporting-type R² (%s): %.3f (%.1f%%)\n",
                component_lookup[[nm]], r2, 100*r2))
  }
}
```
Correlation between the metrics
```{r}
# ──────────────────────────────────────────────────────────────
# Correlation between z-score metrics (overall, across all county-years)
# ──────────────────────────────────────────────────────────────
library(dplyr)

# Make sure these columns exist
z_cols <- c("z_DQ_prop_garbage", "z_prop_light", "z_pct_overd_miss", "z_pct_acc_miss")
stopifnot(all(z_cols %in% names(direction_year)))

# Pearson correlation
cor_pearson <- cor(direction_year %>% select(all_of(z_cols)),
                   use = "pairwise.complete.obs", method = "pearson")

# Spearman correlation
cor_spearman <- cor(direction_year %>% select(all_of(z_cols)),
                    use = "pairwise.complete.obs", method = "spearman")

cat("Pearson correlations:\n")
print(round(cor_pearson, 3))

cat("\nSpearman correlations:\n")
print(round(cor_spearman, 3))
```
New diversity metrics
```{r}
# ──────────────────────────────────────────────────────────────
# Map age-bucket detail (UCOD & MCOD) — CLUSTERS ONLY
#   • Handles *_wide.csv (e.g., ucod_age_0-44) and long (age_bucket + value)
#   • Adds optional weighted "ALL" using age_weights_skew4_global.csv
#   • Builds cluster polygons from county_cluster_membership.csv
#   • Saves per-period × age-bucket PNGs for UCOD and MCOD
# ──────────────────────────────────────────────────────────────

suppressPackageStartupMessages({
  library(dplyr); library(readr); library(stringr); library(purrr)
  library(sf); library(tigris); library(ggplot2); library(glue); library(here)
  library(tidyr)
})

options(tigris_use_cache = TRUE, tigris_class = "sf")
crs_proj <- 2163

# ── CONFIG ─────────────────────────────────────────────────────
# Period → shapefile year
shapefile_years <- c(
  "1999_2006" = 2000,
  "2007_2014" = 2010,
  "2015_2022" = 2020
)

# Color scale limits for age-bucket detail maps
lims_age <- c(50, 70)

# Output base directory
out_dir_age <- here("figures","phillips_maps","cluster_age_detail")

# If you want to limit to a subset while testing, set e.g.:
# ONLY_BUCKETS <- c("0-44", "ALL-weighted")
ONLY_BUCKETS <- NULL  # NULL = map all available buckets

# ── Helpers: IO & parsing ─────────────────────────────────────
.read_csv_flex <- function(fname_options) {
  # Try output/, then project root, then /mnt/data
  opts  <- as.character(fname_options)
  paths <- c(file.path(here("output"), opts),
             here(opts),
             file.path("/mnt/data", basename(opts)))
  hit <- paths[file.exists(paths)][1]
  if (is.na(hit)) stop("File not found among: ", paste(paths, collapse=" | "))
  readr::read_csv(hit, show_col_types = FALSE)
}

# Recognize age buckets anywhere in a name, e.g. "ucod_age_0-44", "80+"
# (ASCII only; no lookarounds; robust to "_", space, or "-" as separator)
.is_age_col <- function(nm) {
  grepl("\\d{1,2}([-_ ]\\d{1,2}|\\+)", nm)
}

# Normalize UCOD/MCOD age detail to LONG: cluster, period, age_bucket, detail
.normalize_age_detail <- function(df) {
  nm <- names(df)

  # Harmonize id columns
  if (!"cluster" %in% nm && "clu" %in% nm) df <- dplyr::rename(df, cluster = clu)
  if (!"period"  %in% names(df)) stop("Expected a 'period' column.")
  if (!"cluster" %in% names(df)) stop("Expected a 'cluster' column.")

  if ("age_bucket" %in% names(df)) {
    # LONG input: guess value column if not named 'detail'
    val_cols <- setdiff(names(df), c("cluster","period","age_bucket"))
    if (!"detail" %in% names(df)) {
      numeric_candidates <- val_cols[vapply(df[val_cols], is.numeric, logical(1))]
      chosen <- if (length(numeric_candidates) >= 1) numeric_candidates[1] else val_cols[1]
      df <- dplyr::rename(df, detail = dplyr::all_of(chosen))
    }
    out <- df %>% dplyr::select(cluster, period, age_bucket, detail)
  } else {
    # WIDE input: gather any columns that contain an age bucket
    age_cols <- names(df)[vapply(names(df), .is_age_col, logical(1))]
    if (length(age_cols) == 0) stop("Wide input has no recognizable age-bucket columns.")

    out <- df %>%
      tidyr::pivot_longer(dplyr::all_of(age_cols),
                          names_to = "age_bucket", values_to = "detail") %>%
      dplyr::mutate(
        # Extract just the bucket piece (e.g., "0-44" from "ucod_age_0-44")
        age_bucket = stringr::str_extract(age_bucket, "\\d{1,2}([-_ ]\\d{1,2}|\\+)"),
        age_bucket = gsub("[-_ ]", "-", age_bucket)
      ) %>%
      dplyr::select(cluster, period, age_bucket, detail)
  }

  out %>%
    dplyr::mutate(
      cluster = as.character(cluster),
      period  = as.character(period),
      age_bucket = as.character(age_bucket),
      detail  = as.numeric(detail)
    )
}

# Load optional age weights (expects columns: age_bucket + weight/prop/w)
.load_age_weights <- function() {
  w <- tryCatch(
    .read_csv_flex(c("age_weights_skew4_global.csv", "age_weights.csv")),
    error = function(e) NULL
  )
  if (is.null(w)) return(NULL)

  # Harmonize names
  if (!"age_bucket" %in% names(w)) {
    cand <- names(w)[vapply(names(w), .is_age_col, logical(1))]
    if (length(cand)) {
      w <- w %>% tidyr::pivot_longer(dplyr::all_of(cand), names_to="age_bucket", values_to="weight")
      w <- w %>% dplyr::mutate(age_bucket = gsub("[-_ ]", "-", age_bucket))
    } else {
      stop("Weights file lacks 'age_bucket' column and no age-like columns found.")
    }
  }
  if (!"weight" %in% names(w)) {
    guess <- intersect(names(w), c("w","prop","weight","share","Weight","W"))
    if (length(guess)==0) stop("Weights file found but no weight column (w/prop/weight/share).")
    w <- dplyr::rename(w, weight = dplyr::all_of(guess[1]))
  }

  w %>%
    dplyr::select(age_bucket, weight) %>%
    dplyr::mutate(weight = as.numeric(weight)) %>%
    dplyr::filter(!is.na(weight)) %>%
    dplyr::mutate(weight = weight / sum(weight))
}

# Add weighted "ALL" bucket (if weights supplied)
.prepare_for_mapping <- function(df_age_long, weights = NULL, include_weighted_all = TRUE) {
  out <- df_age_long
  if (include_weighted_all && !is.null(weights)) {
    w_all <- out %>%
      dplyr::inner_join(weights, by = "age_bucket") %>%
      dplyr::group_by(cluster, period) %>%
      dplyr::summarise(detail = stats::weighted.mean(detail, weight, na.rm = TRUE), .groups = "drop") %>%
      dplyr::mutate(age_bucket = "ALL-weighted")
    out <- dplyr::bind_rows(out, w_all)
  }
  out
}

# Filename-safe conversion of labels like "80+"
.sanitize <- function(x) {
  x %>% gsub("\\+", "plus", ., perl = TRUE) %>% gsub("[^0-9A-Za-z]+", "_", ., perl = TRUE)
}

# ── Geospatial helpers ────────────────────────────────────────
.pick_fips <- function(df) {
  hits <- grep("^GEOID", names(df), value = TRUE)
  if (length(hits) >= 1) return(df[[hits[1]]])
  if (all(c("STATEFP","COUNTYFP") %in% names(df))) return(paste0(df$STATEFP, df$COUNTYFP))
  stop("No GEOID or STATEFP/COUNTYFP columns found in tigris::counties() output.")
}

build_cluster_sf <- function(period_name, shp_year, ccm_df) {
  counties_raw <- tigris::counties(year = shp_year, cb = TRUE, class = "sf") %>%
    sf::st_zm(drop = TRUE, what = "ZM") %>%
    sf::st_transform(crs_proj)

  counties_sf <- counties_raw %>%
    mutate(fips = stringr::str_pad(.pick_fips(counties_raw), 5, pad = "0")) %>%
    select(fips, geometry)

  m <- ccm_df %>% filter(period == period_name)
  if (nrow(m) == 0) stop("No county→cluster rows for period ", period_name)

  j <- counties_sf %>% left_join(m, by = "fips") %>% filter(!is.na(cluster))

  clusters_sf <- j %>%
    group_by(cluster) %>%
    summarise(geometry = sf::st_union(geometry), .groups = "drop")
  suppressWarnings(clusters_sf)
}

make_map_age <- function(sf_data, var, title = NULL) {
  states <- tigris::states(cb = TRUE, class = "sf") %>% sf::st_transform(crs_proj)
  plot_title <- if (is.null(title)) var else title

  ggplot() +
    geom_sf(data = sf_data, aes(fill = .data[[var]]), colour = NA) +
    geom_sf(data = states, fill = NA, colour = "white", linewidth = 0.3) +
    scale_fill_distiller(palette = "RdBu", direction = 1,
                         limits = lims_age, oob = scales::squish, na.value = "grey90") +
    coord_sf(xlim = c(-2500000, 2500000),
             ylim = c(-2200000, 730000),
             expand = FALSE) +
    labs(title = plot_title, fill = NULL) +
    theme_void() +
    theme(
      plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
      legend.text = element_text(size = 9)
    )
}

# ── Read inputs ───────────────────────────────────────────────
# County→cluster membership
ccm <- {
  df <- .read_csv_flex(c("county_cluster_membership.csv"))
  if (!all(c("fips","period","cluster") %in% names(df))) {
    stop("county_cluster_membership.csv must have columns: fips, period, cluster")
  }
  df %>%
    mutate(fips = stringr::str_pad(as.character(fips), 5, pad = "0")) %>%
    select(fips, period, cluster)
}

# UCOD / MCOD age detail (wide or long)
ucod_df <- .read_csv_flex(c("ucod_detail_by_age_bucket_wide.csv",
                            "ucod_detail_by_age_bucket.csv"))
mcod_df <- .read_csv_flex(c("mcod_detail_by_age_bucket_wide.csv",
                            "mcod_detail_by_age_bucket.csv"))

ucod_long <- .normalize_age_detail(ucod_df)
mcod_long <- .normalize_age_detail(mcod_df)

# Optional weights
age_w <- .load_age_weights()

ucod_map <- .prepare_for_mapping(ucod_long, age_w, include_weighted_all = TRUE)
mcod_map <- .prepare_for_mapping(mcod_long, age_w, include_weighted_all = TRUE)

stopifnot(all(c("cluster","period","age_bucket","detail") %in% names(ucod_map)))
stopifnot(all(c("cluster","period","age_bucket","detail") %in% names(mcod_map)))

message("UCOD buckets: ", paste(sort(unique(ucod_map$age_bucket)), collapse=", "))
message("MCOD buckets: ", paste(sort(unique(mcod_map$age_bucket)), collapse=", "))

# ── Mapping engine ────────────────────────────────────────────
dir.create(out_dir_age, recursive = TRUE, showWarnings = FALSE)

.do_maps <- function(dat_long, flavor = c("UCOD","MCOD")) {
  flavor <- match.arg(flavor)
  base_dir <- file.path(out_dir_age, tolower(flavor))
  dir.create(base_dir, recursive = TRUE, showWarnings = FALSE)

  # Periods present in data that we have shapefiles for
  periods <- intersect(names(shapefile_years), unique(dat_long$period))

  # Age buckets to draw
  buckets <- sort(unique(dat_long$age_bucket))
  if (!is.null(ONLY_BUCKETS)) {
    buckets <- intersect(buckets, ONLY_BUCKETS)
  }

  walk(buckets, function(ab) {
    for (win in periods) {
      message("Mapping ", flavor, " — ", ab, " — ", win, "…")
      yr <- shapefile_years[[win]]

      cl_sf  <- build_cluster_sf(win, yr, ccm)
      dat_ab <- dat_long %>%
        filter(period == win, age_bucket == ab) %>%
        select(cluster, detail)

      sf_cl  <- cl_sf %>% left_join(dat_ab, by = "cluster")

      ttl <- glue("{flavor} detail — {ab} — {win}")
      p   <- make_map_age(sf_cl, "detail", ttl)

      fout <- file.path(base_dir,
                        glue("{tolower(flavor)}_detail_{.sanitize(ab)}_{win}.png"))
      ggsave(filename = fout, plot = p, width = 8, height = 6, dpi = 320, device = "png")
    }
  })
}

# ── Run ───────────────────────────────────────────────────────
.do_maps(ucod_map, "UCOD")
.do_maps(mcod_map, "MCOD")

message("Done. Maps saved to: ", out_dir_age)
```
Map every metric
```{r}
# ──────────────────────────────────────────────────────────────
# Map ALL cluster-level metrics from cluster_metrics.csv
# ──────────────────────────────────────────────────────────────

# 1) Read metrics
cluster_metrics <- .read_csv_flex("cluster_metrics.csv") %>%
  dplyr::mutate(cluster = as.character(cluster),
                period  = as.character(period))

stopifnot(all(c("cluster","period") %in% names(cluster_metrics)))

# 2) Which columns to map? → all numeric (excluding obvious non-metrics)
metric_cols <- names(cluster_metrics)[vapply(cluster_metrics, is.numeric, logical(1))]
metric_cols <- setdiff(metric_cols, c("fips", "GEOID", "STATEFP", "COUNTYFP"))

if (length(metric_cols) == 0) stop("No numeric metric columns found in cluster_metrics.csv.")

message("Will map metrics: ", paste(metric_cols, collapse = ", "))

# 3) Optional: per-metric overrides for limits/palette
#    Add entries here if you want fixed ranges for specific metrics.
metric_overrides <- list(
  detail_phillips_refsize = list(lims = c(50, 70), diverging = FALSE),
  detail_phillips_raw     = list(lims = c(50, 70), diverging = FALSE)
)

# 4) Helpers for scale selection
.compute_limits <- function(x) {
  x <- x[is.finite(x)]
  if (!length(x)) return(list(lims = c(0, 1), diverging = FALSE))
  q <- stats::quantile(x, c(0.02, 0.98), na.rm = TRUE, names = FALSE)
  if (q[1] < 0 && q[2] > 0) {
    L <- max(abs(q))
    list(lims = c(-L, L), diverging = TRUE)
  } else {
    list(lims = c(q[1], q[2]), diverging = FALSE)
  }
}

make_map_metric <- function(sf_data, var, lims, diverging, title = NULL) {
  states <- tigris::states(cb = TRUE, class = "sf") %>% sf::st_transform(crs_proj)
  plot_title <- if (is.null(title)) var else title

  p <- ggplot() +
    geom_sf(data = sf_data, aes(fill = .data[[var]]), colour = NA) +
    geom_sf(data = states, fill = NA, colour = "white", linewidth = 0.3) +
    coord_sf(xlim = c(-2500000, 2500000),
             ylim = c(-2200000, 730000),
             expand = FALSE) +
    labs(title = plot_title, fill = NULL) +
    theme_void() +
    theme(
      plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
      legend.text = element_text(size = 9)
    )

  if (diverging) {
    p + scale_fill_distiller(palette = "RdBu", direction = 1,
                             limits = lims, oob = scales::squish, na.value = "grey90")
  } else {
    p + scale_fill_viridis_c(limits = lims, oob = scales::squish, na.value = "grey90")
  }
}

# 5) Output dir
out_dir_metrics <- here("figures", "phillips_maps", "cluster_metrics")
dir.create(out_dir_metrics, recursive = TRUE, showWarnings = FALSE)

# 6) Iterate: metric × period
purrr::walk(metric_cols, function(mcol) {
  # Determine limits & palette (override → else robust auto)
  if (!is.null(metric_overrides[[mcol]])) {
    lims_info <- metric_overrides[[mcol]]
  } else {
    lims_info <- .compute_limits(cluster_metrics[[mcol]])
  }

  # Ensure per-metric subdir
  subdir <- file.path(out_dir_metrics, .sanitize(mcol))
  dir.create(subdir, recursive = TRUE, showWarnings = FALSE)

  # Restrict to periods we have shapefiles for
  periods_here <- intersect(names(shapefile_years), unique(cluster_metrics$period))
  purrr::walk(periods_here, function(win) {
    message("Mapping metric ", mcol, " — ", win, "…")
    yr <- shapefile_years[[win]]
    cl_sf <- build_cluster_sf(win, yr, ccm)

    dat <- cluster_metrics %>%
      dplyr::filter(period == win) %>%
      dplyr::select(cluster, !!rlang::sym(mcol)) %>%
      dplyr::rename(value = !!rlang::sym(mcol))

    # Skip if all NA/constant
    vals <- dat$value[is.finite(dat$value)]
    if (!length(vals) || (length(unique(vals)) == 1)) {
      message("  Skipping ", mcol, " (no finite variance in ", win, ").")
      return(invisible(NULL))
    }

    sf_cl <- cl_sf %>% dplyr::left_join(dat, by = "cluster")

    ttl <- glue::glue("{mcol} — {win}")
    p <- make_map_metric(sf_cl, "value", lims = lims_info$lims,
                         diverging = isTRUE(lims_info$diverging), title = ttl)

    fout <- file.path(subdir, glue::glue("{.sanitize(mcol)}_{win}.png"))
    ggsave(filename = fout, plot = p, width = 8, height = 6, dpi = 320, device = "png")
  })
})

message("Done. Metric maps saved to: ", out_dir_metrics)
```
Find drivers of diversity metric
```{r}
# ──────────────────────────────────────────────────────────────
# Drivers of Phillips "detail" (diversity) metric — CLUSTERS
#   • Reads UCOD/MCOD contribution files (gbdl3 or root3; top10/full)
#   • Chooses a target age bucket (prefers ALL-weighted → ALL → first found)
#   • Builds cluster polygons from county_cluster_membership.csv(.gz)
#   • Maps:
#       (A) Top driver category per cluster (limited to top-K nationwide)
#       (B) Dominant driver share (continuous)
#       (C) Small-multiples of shares for top-K categories
# ──────────────────────────────────────────────────────────────

suppressPackageStartupMessages({
  library(dplyr); library(readr); library(stringr); library(purrr)
  library(sf); library(tigris); library(ggplot2); library(glue); library(here)
  library(tidyr); library(forcats); library(tools)
})

options(tigris_use_cache = TRUE, tigris_class = "sf")
crs_proj <- 2163

# ── CONFIG ─────────────────────────────────────────────────────
# Period → shapefile year (match your clustering windows)
shapefile_years <- c(
  "1999_2006" = 2000,
  "2007_2014" = 2010,
  "2015_2022" = 2020
)

# Preferred age bucket(s) to map (first match wins)
PREFERRED_BUCKETS <- c("ALL-weighted","ALL","0-44","0-14","15-24","25-44","45-64","65-74","75-84","85+")

# How many top categories to keep for mapping; others → "Other"
TOP_K <- 8

# Color scale for share maps (in %)
lims_share <- c(0, 50)  # dominant share often in 20–50% range; adjust as needed

# Output base directory
out_dir <- here("figures","phillips_maps","drivers")

# ── Helpers: IO & parsing ─────────────────────────────────────
.read_csv_flex <- function(fname_options) {
  opts  <- as.character(fname_options)
  paths <- c(
    file.path(here("output"), opts),
    here(opts),
    file.path("/mnt/data", basename(opts))
  )
  hit <- paths[file.exists(paths)][1]
  if (is.na(hit)) stop("File not found among: ", paste(paths, collapse=" | "))
  readr::read_csv(hit, show_col_types = FALSE)
}

# Accepts .csv or .csv.gz
.try_read <- function(paths) {
  for (p in paths) {
    if (file.exists(p)) return(readr::read_csv(p, show_col_types = FALSE))
  }
  NULL
}

# Recognize an age-bucket-like string (ASCII)
.is_age_col <- function(nm) grepl("\\d{1,2}([-_ ]\\d{1,2}|\\+)|ALL", nm, ignore.case = TRUE)

# Normalizes contribution file to: cluster, period, age_bucket, cause, contrib
.normalize_contrib <- function(df) {
  nm <- names(df)

  # Harmonize id cols
  if (!"cluster" %in% nm && "clu" %in% nm) df <- dplyr::rename(df, cluster = clu)
  req <- c("cluster","period")
  if (!all(req %in% names(df))) stop("Contribution file must have: cluster, period (found: ", paste(nm, collapse=", "), ")")

  # Age bucket: if absent, assume ALL
  if (!"age_bucket" %in% nm) {
    # Some files might encode bucket inside column names; detect & gather:
    age_cols <- nm[vapply(nm, .is_age_col, logical(1))]
    if (length(age_cols) >= 2 && !"contrib" %in% nm) {
      # Wide w/ buckets as columns → long
      value_cols <- setdiff(age_cols, c("cluster","period"))
      df <- df %>%
        tidyr::pivot_longer(dplyr::all_of(value_cols), names_to = "age_bucket", values_to = "contrib")
      # guess cause col
      nm <- names(df)
    } else {
      df$age_bucket <- "ALL"
    }
  }

  # Contribution column (numeric)
  contrib_col <- intersect(names(df), c("contrib","contribution","value","detail_contrib","weight","share"))
  if (length(contrib_col) == 0) {
    # guess first numeric that's not an id
    cand <- setdiff(names(df), c("cluster","period","age_bucket"))
    numeric_candidates <- cand[vapply(df[cand], is.numeric, logical(1))]
    if (length(numeric_candidates) == 0) stop("No numeric contribution column found.")
    contrib_col <- numeric_candidates[1]
  }
  if (contrib_col[1] != "contrib") df <- dplyr::rename(df, contrib = dplyr::all_of(contrib_col[1]))

  # Cause column
  cause_candidates <- c("cause","gbd_l3","gbdl3","root3","uc3","cause_name","code","category")
  cause_col <- intersect(names(df), cause_candidates)
  if (length(cause_col) == 0) {
    # pick first non-id, non-numeric col
    cand <- setdiff(names(df), c("cluster","period","age_bucket","contrib"))
    chr_cand <- cand[vapply(df[cand], is.character, logical(1))]
    if (length(chr_cand) == 0) stop("No cause/category column found.")
    cause_col <- chr_cand[1]
  }
  if (cause_col[1] != "cause") df <- dplyr::rename(df, cause = dplyr::all_of(cause_col[1]))

  out <- df %>%
    dplyr::transmute(
      cluster = as.character(cluster),
      period  = as.character(period),
      age_bucket = as.character(age_bucket),
      cause   = as.character(cause),
      contrib = as.numeric(contrib)
    ) %>%
    dplyr::filter(!is.na(cluster), !is.na(period), !is.na(cause), !is.na(contrib))

  # Clean bucket label
  out$age_bucket <- gsub("[-_ ]", "-", out$age_bucket)
  out
}

# Choose the best available age bucket to map
.choose_bucket <- function(buckets) {
  buckets <- unique(as.character(buckets))
  hit <- intersect(PREFERRED_BUCKETS, buckets)
  if (length(hit)) return(hit[1])
  buckets[1]
}

# Filename-safe label
.sanitize <- function(x) x %>% gsub("\\+", "plus", ., perl = TRUE) %>% gsub("[^0-9A-Za-z]+", "_", ., perl = TRUE)

# ── Geospatial helpers ────────────────────────────────────────
.pick_fips <- function(df) {
  hits <- grep("^GEOID", names(df), value = TRUE)
  if (length(hits) >= 1) return(df[[hits[1]]])
  if (all(c("STATEFP","COUNTYFP") %in% names(df))) return(paste0(df$STATEFP, df$COUNTYFP))
  stop("No GEOID or STATEFP/COUNTYFP columns found in tigris::counties() output.")
}

build_cluster_sf <- function(period_name, shp_year, ccm_df) {
  counties_raw <- tigris::counties(year = shp_year, cb = TRUE, class = "sf") %>%
    sf::st_zm(drop = TRUE, what = "ZM") %>%
    sf::st_transform(crs_proj)

  counties_sf <- counties_raw %>%
    mutate(fips = stringr::str_pad(.pick_fips(counties_raw), 5, pad = "0")) %>%
    select(fips, geometry)

  m <- ccm_df %>% filter(period == period_name)
  if (nrow(m) == 0) stop("No county→cluster rows for period ", period_name)

  j <- counties_sf %>% left_join(m, by = "fips") %>% filter(!is.na(cluster))

  clusters_sf <- j %>%
    group_by(cluster) %>%
    summarise(geometry = sf::st_union(geometry), .groups = "drop")
  suppressWarnings(clusters_sf)
}

# Base map (states for outlines)
.states_sf <- function() tigris::states(cb = TRUE, class = "sf") %>% sf::st_transform(crs_proj)

# ── Plotters ──────────────────────────────────────────────────
make_category_map <- function(sf_data, var, title = NULL, palette = NULL) {
  states <- .states_sf()
  plot_title <- if (is.null(title)) var else title
  if (is.null(palette)) {
    palette <- scales::hue_pal()(length(unique(na.omit(sf_data[[var]]))))
  }
  ggplot() +
    geom_sf(data = sf_data, aes(fill = .data[[var]]), colour = NA) +
    geom_sf(data = states, fill = NA, colour = "white", linewidth = 0.3) +
    coord_sf(xlim = c(-2500000, 2500000), ylim = c(-2200000, 730000), expand = FALSE) +
    scale_fill_manual(values = palette, na.value = "grey90") +
    labs(title = plot_title, fill = NULL) +
    theme_void() +
    theme(
      plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
      legend.text = element_text(size = 9)
    )
}

make_share_map <- function(sf_data, var, title = NULL, limits = lims_share) {
  states <- .states_sf()
  plot_title <- if (is.null(title)) var else title
  ggplot() +
    geom_sf(data = sf_data, aes(fill = .data[[var]]), colour = NA) +
    geom_sf(data = states, fill = NA, colour = "white", linewidth = 0.3) +
    scale_fill_distiller(palette = "RdBu", direction = 1,
                         limits = limits, oob = scales::squish, na.value = "grey90") +
    coord_sf(xlim = c(-2500000, 2500000), ylim = c(-2200000, 730000), expand = FALSE) +
    labs(title = plot_title, fill = "%") +
    theme_void() +
    theme(
      plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
      legend.text = element_text(size = 9)
    )
}

# ── Read inputs ───────────────────────────────────────────────
# County→cluster membership (csv or csv.gz)
ccm <- {
  paths <- c(
    here("output","county_cluster_membership.csv.gz"),
    here("output","county_cluster_membership.csv"),
    here("county_cluster_membership.csv.gz"),
    here("county_cluster_membership.csv"),
    "/mnt/data/county_cluster_membership.csv.gz"
  )
  df <- .try_read(paths)
  if (is.null(df)) stop("county_cluster_membership file not found (csv or csv.gz).")
  if (!all(c("fips","period","cluster") %in% names(df))) {
    stop("county_cluster_membership must have columns: fips, period, cluster")
  }
  df %>%
    mutate(fips = stringr::str_pad(as.character(fips), 5, pad = "0")) %>%
    select(fips, period, cluster) %>%
    mutate(period = as.character(period),
           cluster = as.character(cluster))
}

# Convenience loader for contrib files
read_contrib <- function(flavor = c("UCOD","MCOD"), domain = c("gbdl3","root3")) {
  flavor <- tolower(match.arg(flavor))
  domain <- tolower(match.arg(domain))
  base <- glue("detail_contrib_{flavor}_{domain}")
  candidates <- c(
    here("output", glue("{base}_top10.csv.gz")),
    here("output", glue("{base}_full.csv.gz")),
    here(glue("{base}_top10.csv.gz")),
    here(glue("{base}_full.csv.gz")),
    file.path("/mnt/data", glue("{base}_top10.csv.gz")),
    file.path("/mnt/data", glue("{base}_full.csv.gz")),
    here("output", glue("{base}_top10.csv")),
    here("output", glue("{base}_full.csv")),
    here(glue("{base}_top10.csv")),
    here(glue("{base}_full.csv")),
    file.path("/mnt/data", glue("{base}_top10.csv")),
    file.path("/mnt/data", glue("{base}_full.csv"))
  )
  df <- .try_read(candidates)
  if (is.null(df)) stop("No contribution file found for ", base)
  .normalize_contrib(df)
}

# ── Engine: build summaries and maps ──────────────────────────
make_driver_maps <- function(flavor = c("UCOD","MCOD"),
                             domain = c("gbdl3","root3"),
                             target_bucket = NULL,
                             top_k = TOP_K) {
  flavor <- toupper(match.arg(flavor))
  domain <- tolower(match.arg(domain))
  message("→ Loading contributions: ", flavor, " / ", toupper(domain))
  contrib <- read_contrib(flavor, domain)

  # Select target age bucket
  if (is.null(target_bucket)) {
    target_bucket <- .choose_bucket(contrib$age_bucket)
  }
  message("→ Using age bucket: ", target_bucket)

  contrib_tb <- contrib %>% filter(age_bucket == target_bucket)

  # Periods present that we have shapefiles for
  periods <- intersect(names(shapefile_years), unique(contrib_tb$period))

  # Create output dir
  base_dir <- file.path(out_dir, glue("{tolower(flavor)}-{domain}"))
  dir.create(base_dir, recursive = TRUE, showWarnings = FALSE)

  # Top-K categories nationwide (by total contrib) — separate by period to reflect shifts
  topk_by_period <- contrib_tb %>%
    group_by(period, cause) %>%
    summarise(total_contrib = sum(contrib, na.rm = TRUE), .groups = "drop_last") %>%
    arrange(period, desc(total_contrib)) %>%
    group_modify(~ head(.x, top_k)) %>%
    ungroup() %>%
    group_by(period) %>%
    summarise(top_k_causes = list(.data$cause), .groups = "drop")

  # For facet maps we’ll use a stable ordering of causes by national total
  topk_global <- contrib_tb %>%
    group_by(cause) %>%
    summarise(total_contrib = sum(contrib, na.rm = TRUE), .groups = "drop") %>%
    arrange(desc(total_contrib)) %>%
    slice_head(n = top_k) %>%
    pull(cause)

  # Build per-period maps
  walk(periods, function(win) {
    message("→ Mapping period: ", win)
    yr <- shapefile_years[[win]]
    cl_sf <- build_cluster_sf(win, yr, ccm)

    # Compute shares within cluster
    shares <- contrib_tb %>%
      filter(period == win) %>%
      group_by(cluster) %>%
      mutate(total = sum(contrib, na.rm = TRUE)) %>%
      filter(total > 0) %>%
      mutate(share = 100 * contrib / total) %>%
      ungroup()

    # Dominant cause and its share
    dom <- shares %>%
      arrange(cluster, desc(share)) %>%
      group_by(cluster) %>%
      slice_head(n = 1) %>%
      ungroup() %>%
      transmute(cluster, dom_cause = cause, dom_share = share)

    # Limit category map to period-specific top-K (others → "Other")
    period_top <- topk_by_period %>% filter(period == win) %>% pull(top_k_causes) %>% .[[1]]
    dom <- dom %>%
      mutate(dom_cause_limited = ifelse(dom_cause %in% period_top, dom_cause, "Other"),
             dom_cause_limited = forcats::fct_inorder(dom_cause_limited))

    # Join to geometry
    sf_dom <- cl_sf %>% left_join(dom, by = "cluster")

    # (A) Category map — dominant driver
    cats <- levels(sf_dom$dom_cause_limited)
    pal <- setNames(scales::hue_pal()(length(cats)), cats)
    pal["Other"] <- "grey70"

    pA <- make_category_map(
      sf_dom, "dom_cause_limited",
      title = glue("{flavor} {toupper(domain)} — Dominant driver — {win} (bucket: {target_bucket})"),
      palette = pal
    )
    foutA <- file.path(base_dir, glue("driver_domcat_{tolower(flavor)}_{domain}_{.sanitize(target_bucket)}_{win}.png"))
    ggsave(foutA, pA, width = 9, height = 6.2, dpi = 320)

    # (B) Share map — % share of dominant cause
    pB <- make_share_map(
      sf_dom, "dom_share",
      title = glue("{flavor} {toupper(domain)} — Dominant driver share (%) — {win} (bucket: {target_bucket})"),
      limits = lims_share
    )
    foutB <- file.path(base_dir, glue("driver_domshare_{tolower(flavor)}_{domain}_{.sanitize(target_bucket)}_{win}.png"))
    ggsave(foutB, pB, width = 9, height = 6.2, dpi = 320)

    # (C) Small multiples for global top-K causes — share by cluster
    shares_top <- shares %>%
      mutate(cause_f = ifelse(cause %in% topk_global, cause, NA_character_)) %>%
      filter(!is.na(cause_f)) %>%
      select(cluster, cause_f, share) %>%
      distinct()

    sf_facets <- cl_sf %>%
      left_join(shares_top, by = "cluster")

pC <- ggplot() +
  geom_sf(data = sf_facets, aes(fill = share, geometry = geometry), colour = NA) +
  geom_sf(data = .states_sf(), fill = NA, colour = "white", linewidth = 0.25) +
  scale_fill_distiller(
    palette = "RdBu", direction = 1,
    limits = c(0, max(25, ceiling(max(sf_facets$share, na.rm = TRUE) / 5) * 5))) + 
  coord_sf(
    xlim = c(-2500000, 2500000),
    ylim = c(-2200000, 730000),
    expand = FALSE
  ) +
  facet_wrap(~ cause_f, ncol = 3) +
  labs(
    title = glue("{flavor} {toupper(domain)} — Share of detail by cause — {win} (bucket: {target_bucket})"),
    fill = "%"
  ) +
  theme_void() +
  theme(
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    legend.text = element_text(size = 9),
    strip.text = element_text(size = 10, face = "bold")
  )


    foutC <- file.path(base_dir, glue("driver_facets_{tolower(flavor)}_{domain}_{.sanitize(target_bucket)}_{win}.png"))
    ggsave(foutC, pC, width = 10, height = 9.5, dpi = 300)

    message("   Saved: ", foutA)
    message("   Saved: ", foutB)
    message("   Saved: ", foutC)
  })

  message("Done. Output in: ", base_dir)
}

# ── Run (edit as needed) ──────────────────────────────────────
dir.create(out_dir, recursive = TRUE, showWarnings = FALSE)

# Example runs (uncomment what you want)
make_driver_maps("UCOD", "gbdl3")  # preferred if you have GBD L3 files

# If you want ICD-10 root-3 drivers:
make_driver_maps("UCOD", "root3")
make_driver_maps("MCOD", "root3")

```
COD standardized diversity maps
```{r}
# ──────────────────────────────────────────────────────────────
# Cluster metric maps with AK/HI resized like your working code
# ──────────────────────────────────────────────────────────────

suppressPackageStartupMessages({
  library(readr);  library(dplyr);  library(stringr); library(tidyr);  library(tibble)
  library(ggplot2); library(sf);     library(tigris);  library(scales); library(here)
})

options(tigris_use_cache = TRUE, tigris_class = "sf", sf_use_s2 = TRUE)

# ------------------------ paths -------------------------------
metrics_file    <- here("output", "cluster_metrics_ucr39_cstd.csv.gz")
membership_file <- here("output", "county_cluster_membership.csv.gz")
out_dir         <- here("figures", "phillips_maps", "cluster_metrics_resized")
dir.create(out_dir, recursive = TRUE, showWarnings = FALSE)

# -------------------- projection ------------------------------
crs_proj <- 2163  # US National Atlas Equal Area (matches your notebook)

# ------------------------ helpers -----------------------------
compute_limits <- function(x) {
  x <- x[is.finite(x)]
  if (!length(x)) return(list(lims = c(0, 1), diverging = FALSE))
  q <- stats::quantile(x, c(0.02, 0.98), na.rm = TRUE, names = FALSE)
  if (q[1] < 0 && q[2] > 0) {
    L <- max(abs(q))
    list(lims = c(-L, L), diverging = TRUE)
  } else {
    list(lims = c(q[1], q[2]), diverging = FALSE)
  }
}

sanitize <- function(x) {
  x %>% str_replace_all("[^A-Za-z0-9]+", "_") %>% str_replace_all("^_+|_+$", "") %>% tolower()
}

# Same geometry utilities you used
st_shift <- function(sf_obj, shift = c(0, 0)) {
  st_geometry(sf_obj) <- st_geometry(sf_obj) + shift
  sf_obj
}
st_scale <- function(sf_obj, scale = 1) {
  centroid <- st_centroid(st_union(sf_obj))
  st_geometry(sf_obj) <- (st_geometry(sf_obj) - centroid) * scale + centroid
  sf_obj
}

# Build 2020 counties in 2163 and apply the SAME AK/HI transforms you liked
build_counties_2163_resized <- function() {
  us_counties <- tigris::counties(year = 2020, cb = TRUE, class = "sf") |>
    sf::st_transform(crs_proj) |>
    dplyr::select(GEOID, STATEFP, geometry)

  mainland <- us_counties |> dplyr::filter(!STATEFP %in% c("02","15"))  # exclude AK, HI (PR not included)
  alaska   <- us_counties |> dplyr::filter(STATEFP == "02") |>
                st_scale(0.33) |>
                st_shift(c(1100000, -4700000)) |>
                sf::st_set_crs(crs_proj)
  hawaii   <- us_counties |> dplyr::filter(STATEFP == "15") |>
                st_scale(1)  |>
                st_shift(c(5000000, -1100000)) |>
                sf::st_set_crs(crs_proj)

  dplyr::bind_rows(mainland, alaska, hawaii) |>
    dplyr::rename(fips = GEOID, statefp = STATEFP) |>
    sf::st_make_valid()
}

# Do the same to STATES so outlines align perfectly
build_states_2163_resized <- function() {
  us_states <- tigris::states(year = 2020, cb = TRUE, class = "sf") |>
    sf::st_transform(crs_proj) |>
    dplyr::select(STATEFP, geometry)

  mainland <- us_states |> dplyr::filter(!STATEFP %in% c("02","15"))
  alaska   <- us_states |> dplyr::filter(STATEFP == "02") |>
                st_scale(0.33) |>
                st_shift(c(1100000, -4700000)) |>
                sf::st_set_crs(crs_proj)
  hawaii   <- us_states |> dplyr::filter(STATEFP == "15") |>
                st_scale(1)  |>
                st_shift(c(5000000, -1100000)) |>
                sf::st_set_crs(crs_proj)

  dplyr::bind_rows(mainland, alaska, hawaii) |>
    sf::st_make_valid()
}

# Build cluster polygons from the transformed counties (join → union)
build_cluster_sf <- function(period_key, membership_df, counties_sf) {
  mm <- membership_df %>%
    dplyr::filter(.data$period == period_key) %>%
    dplyr::transmute(fips = stringr::str_pad(as.character(fips), 5, pad = "0"),
                     cluster = as.character(cluster)) %>%
    dplyr::distinct()
  if (!nrow(mm)) stop("No membership rows for period: ", period_key)

  counties_sf %>%
    dplyr::inner_join(mm, by = "fips") %>%
    dplyr::group_by(cluster) %>%
    dplyr::summarise(geometry = sf::st_union(geometry), .groups = "drop") %>%
    sf::st_make_valid()
}

# Mapping helper (accepts lims/diverging; crops to mainland window you used)
make_map <- function(sf_data, fill_col, lims = NULL, diverging = NULL,
                     title = NULL, palette = "RdBu", states_sf) {
  ggplot() +
    geom_sf(data = sf_data, aes(fill = .data[[fill_col]]), colour = NA) +
    geom_sf(data = states_sf, fill = NA, colour = "white", linewidth = 0.3) +
    scale_fill_distiller(
      palette = "RdBu",     # red ↔ blue diverging
      direction = 1,       # flip if you want blue = low, red = high
      na.value = "grey90"
    ) +
    coord_sf(
      xlim = c(-2500000, 2500000),
      ylim = c(-2200000,  730000),
      expand = FALSE
    ) +
    labs(title = title %||% fill_col, fill = NULL) +
    theme_void() +
    theme(
      plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
      legend.text = element_text(size = 9)
    )
}

# ------------------------- load data --------------------------
message("Reading metrics: ", metrics_file)
metrics <- readr::read_csv(metrics_file, show_col_types = FALSE) %>%
  dplyr::mutate(cluster = as.character(cluster), period = as.character(period))

message("Reading membership: ", membership_file)
membership <- readr::read_csv(membership_file, show_col_types = FALSE) %>%
  dplyr::mutate(cluster = as.character(cluster),
                period  = as.character(period),
                fips    = stringr::str_pad(as.character(fips), 5, pad = "0"))

# Prepare transformed geographies ONCE
counties_tf <- build_counties_2163_resized()
states_tf   <- build_states_2163_resized()

# Metric columns & periods
metric_cols <- names(metrics)[vapply(metrics, is.numeric, logical(1))]
metric_cols <- setdiff(metric_cols, c("fips", "GEOID", "STATEFP", "COUNTYFP"))
periods <- unique(metrics$period)

# ----------------------- map everything -----------------------
for (mcol in metric_cols) {
  subdir <- here(out_dir, sanitize(mcol))
  dir.create(subdir, recursive = TRUE, showWarnings = FALSE)

  lims_info <- compute_limits(metrics[[mcol]])

  for (win in periods) {
    message(sprintf("Mapping %s — %s", mcol, win))

    cl_sf <- build_cluster_sf(win, membership, counties_tf)

    dat <- metrics %>%
      dplyr::filter(period == win) %>%
      dplyr::select(cluster, !!rlang::sym(mcol)) %>%
      dplyr::rename(value = !!rlang::sym(mcol))

    vals <- dat$value[is.finite(dat$value)]
    if (!length(vals) || length(unique(vals)) == 1) {
      message(sprintf("  Skipping %s (%s): no finite variance.", mcol, win))
      next
    }

    sf_cl <- cl_sf %>% dplyr::left_join(dat, by = "cluster")
    title <- sprintf("%s — %s", mcol, win)

    p <- make_map(sf_cl, "value",
                  lims = lims_info$lims,
                  diverging = isTRUE(lims_info$diverging),
                  title = title,
                  states_sf = states_tf)

    fout <- here(subdir, sprintf("%s_%s.png", sanitize(mcol), win))
    ggsave(fout, p, width = 8, height = 6, dpi = 320)
  }
}

message("Done. Maps in: ", out_dir)
```
Scores by reporting type and public health spending for diversity metrics
```{r}
# ──────────────────────────────────────────────────────────────
# Phillips detail metrics vs reporting type & PH spend — FINAL+
# (period normalization, de-dup membership, robust fin-year snapping)
# + One time series per metric with 5 lines for PH-spend quintiles
# ──────────────────────────────────────────────────────────────
suppressPackageStartupMessages({
  library(readr); library(dplyr); library(tidyr); library(stringr)
  library(purrr); library(ggplot2); library(scales); library(here)
})

dir.create(here("figures","phillips_detail"), recursive = TRUE, showWarnings = FALSE)
dir.create(here("output"), recursive = TRUE, showWarnings = FALSE)

# ---- helpers --------------------------------------------------
std_fips <- function(x) stringr::str_pad(as.character(x), 5, pad = "0")

norm_period <- function(x) {
  # normalize "1999–2005", "1999-2005", "1999 — 2005", etc. → "1999_2005"
  x <- gsub("\u2013|\u2014|—|-", "_", x)  # en/em dash/hyphen → underscore
  x <- gsub("\\s+", "", x)                # drop spaces
  re <- regmatches(x, gregexpr("\\d{4}", x))
  out <- mapply(function(lbl, yrs) {
    if (length(yrs) >= 2) {
      y1 <- min(as.integer(yrs[1]), as.integer(yrs[2]))
      y2 <- max(as.integer(yrs[1]), as.integer(yrs[2]))
      paste0(y1, "_", y2)
    } else lbl
  }, x, re, USE.NAMES = FALSE)
  out
}

get_years <- function(s) as.integer(stringr::str_extract_all(s, "\\d{4}")[[1]])
parse_period <- function(lbl) {
  yrs <- get_years(lbl)
  if (length(yrs) >= 2) {
    y1 <- min(yrs[1], yrs[2]); y2 <- max(yrs[1], yrs[2]); mid <- floor((y1 + y2)/2)
    tibble(period = lbl, start = y1, end = y2, mid = mid)
  } else tibble(period = lbl, start = NA_integer_, end = NA_integer_, mid = NA_integer_)
}
nearest_fin_year <- function(y, avail) if (!length(avail) || is.na(y)) NA_integer_ else avail[which.min(abs(avail - y))]

# ---- load cluster metrics + membership -----------------------
metrics_file_candidates <- c(
  here("output","cluster_metrics_ucr39_cstd.csv.gz"),
  here("output","cluster_metrics.csv.gz"),
  "output/cluster_metrics_ucr39_cstd.csv.gz",
  "output/cluster_metrics.csv.gz",
  "cluster_metrics_ucr39_cstd.csv.gz",
  "cluster_metrics.csv.gz"
)
membership_file_candidates <- c(
  here("output","county_cluster_membership.csv.gz"),
  "output/county_cluster_membership.csv.gz",
  "county_cluster_membership.csv.gz"
)
metrics_file    <- metrics_file_candidates[file.exists(metrics_file_candidates)][1]
membership_file <- membership_file_candidates[file.exists(membership_file_candidates)][1]
stopifnot(!is.na(metrics_file), !is.na(membership_file))

metrics <- readr::read_csv(metrics_file, show_col_types = FALSE) %>%
  mutate(cluster = as.character(cluster),
         period  = norm_period(as.character(period)))

membership <- readr::read_csv(membership_file, show_col_types = FALSE) %>%
  mutate(cluster     = as.character(cluster),
         period      = norm_period(as.character(period)),
         county_ihme = std_fips(if ("fips" %in% names(.)) fips else if ("GEOID" %in% names(.)) GEOID else fips)) %>%
  filter(grepl("^[0-9]{5}$", county_ihme)) %>%
  distinct(cluster, period, county_ihme, .keep_all = FALSE)    # one row per key

# ---- pick 4 Phillips metrics ---------------------------------
num_cols <- names(metrics)[vapply(metrics, is.numeric, logical(1))]
preferred <- c(
  "detail_mcod_root3_cstd","detail_mcod_icd4_cstd","detail_ucod_root3_cstd","detail_ucod_icd4_cstd",
  "detail_d95","detail_d2000","detail_2000","detail_ref2000",
  "cod_diversity_cstd","cod_diversity_std","cod_diversity",
  "neff_cause","richness_cause","richness_cstd","neff_cstd"
)
present_pref <- intersect(preferred, num_cols)
if (length(present_pref) < 4) {
  extra <- setdiff(num_cols, present_pref)
  cand  <- extra[grepl("detail|divers|rich|neff|d95", extra, ignore.case = TRUE)]
  present_pref <- unique(c(present_pref, cand))
}
detail_cols <- unique(present_pref)[1:min(4, length(unique(present_pref)))]
stopifnot(length(detail_cols) > 0)
message("Using Phillips detail metrics: ", paste(detail_cols, collapse = " | "))

# ---- diagnostics: period coverage BEFORE expansion -----------
cat("\n# Period counts (metrics):\n")
print(metrics %>% count(period, name = "clusters_in_metrics") %>% arrange(period))
cat("\n# Period counts (membership):\n")
print(membership %>% count(period, name = "counties_in_membership") %>% arrange(period))

# ---- expand cluster→county -----------------------------------
detail_long <- metrics %>%
  select(cluster, period, all_of(detail_cols)) %>%
  pivot_longer(cols = all_of(detail_cols), names_to = "metric", values_to = "value") %>%
  inner_join(membership %>% select(cluster, period, county_ihme),
             by = c("cluster","period"),
             relationship = "many-to-many") %>%
  filter(is.finite(value))

cat("\n# After expansion: rows by period & metric\n")
print(detail_long %>% count(period, metric, name = "rows") %>% arrange(metric, period))

# ---- reporting-type lookup (or build) ------------------------
if (!exists("rep_lu")) {
  reporting_path_opts <- c(
    here("data_raw","County-Death-Investigation-System-2018-1-9-2024.csv"),
    "/mnt/data/County-Death-Investigation-System-2018-1-9-2024.csv"
  )
  reporting_path <- reporting_path_opts[file.exists(reporting_path_opts)][1]
  stopifnot(!is.na(reporting_path))
  rep_raw <- readr::read_csv(reporting_path, show_col_types = FALSE)
  get_colname <- function(df, patterns) {
    nm <- names(df)
    hits <- which(Reduce(`|`, lapply(patterns, function(p) grepl(p, nm, ignore.case = TRUE))))
    if (!length(hits)) return(NA_character_) else nm[hits[1]]
  }
  fips_col <- get_colname(rep_raw, c("^fips$","^fips_?code$","geoid","county_?fips","countyrs","fips.*5"))
  type_col <- get_colname(rep_raw, c("reporting.*type","investigation.*type","death.*investigation.*system","^type$","system"))
  stopifnot(!is.na(fips_col), !is.na(type_col))
  rep_lu <- rep_raw %>%
    mutate(county_ihme = std_fips(.data[[fips_col]]),
           reporting_type = trimws(as.character(.data[[type_col]]))) %>%
    filter(grepl("^[0-9]{5}$", county_ihme), !is.na(reporting_type), reporting_type != "") %>%
    mutate(reporting_type = dplyr::recode(tolower(reporting_type),
      "medical examiner"="Medical Examiner","me"="Medical Examiner",
      "coroner"="Coroner","mixed"="Mixed","hybrid"="Mixed",
      .default = stringr::str_to_title(reporting_type))) %>%
    select(county_ihme, reporting_type) %>% distinct()
}
cat("\n# Reporting-type coverage (distinct counties):\n")
print(rep_lu %>% count(reporting_type) %>% mutate(total = sum(n)))

# ---- finance-year snapping (+ fallback) — ROBUST --------------
stopifnot(exists("ph_pc"), exists("direction_year"))
# Make sure fin_year is integer for joins; be tolerant if fin_all missing
ph_pc   <- ph_pc   %>% mutate(fin_year = suppressWarnings(as.integer(fin_year)))
fin_all <- if (exists("fin_all")) fin_all else tibble()
if (nrow(fin_all)) {
  year_col <- dplyr::case_when(
    "fin_year" %in% names(fin_all) ~ "fin_year",
    "year_fin" %in% names(fin_all) ~ "year_fin",
    "year"     %in% names(fin_all) ~ "year",
    TRUE ~ NA_character_
  )
  if (!is.na(year_col)) {
    fin_all <- fin_all %>% mutate(fin_year = suppressWarnings(as.integer(.data[[year_col]])))
  }
}

# Derive actual finance years to snap to (prefer fin_all; fallback to ph_pc)
fin_years_actual <- c(
  if (nrow(fin_all) && "fin_year" %in% names(fin_all)) fin_all$fin_year,
  if ("fin_year" %in% names(ph_pc)) ph_pc$fin_year
) %>%
  suppressWarnings(as.integer(.)) %>%
  { .[is.finite(.)] } %>%
  unique() %>%
  sort()

if (!length(fin_years_actual)) {
  stop("No finance years available. Re-run the finance builder to create fin_all/ph_pc with fin_years (e.g., 2017, 2022).")
}
fin_years_actual <- unique(fin_years_actual)
cat("\n# Using finance years for snapping:", paste(fin_years_actual, collapse = ", "), "\n")

# Build period_info from periods present in detail_long
period_info <- unique(detail_long$period) %>% sort() %>%
  purrr::map_dfr(parse_period) %>%
  dplyr::filter(!is.na(start), !is.na(end), !is.na(mid)) %>%
  dplyr::arrange(start) %>%
  dplyr::mutate(fin_year = vapply(mid, nearest_fin_year, integer(1), avail = fin_years_actual))

# Main PH-per-period using nearest AVAILABLE finance year
ph_period_main <- period_info %>%
  dplyr::select(period, fin_year) %>%
  dplyr::inner_join(
    ph_pc %>% dplyr::select(county_ihme, fin_year, ph_pc),
    by = "fin_year", relationship = "many-to-many"
  ) %>%
  dplyr::mutate(log_ph_pc = log10(pmax(ph_pc, 1e-6))) %>%
  dplyr::select(county_ihme, period, ph_pc, log_ph_pc)

# Fallback (periods without a mapped fin_year row will use county median spend)
ph_any <- ph_pc %>%
  dplyr::group_by(county_ihme) %>%
  dplyr::summarise(ph_pc = median(ph_pc, na.rm = TRUE), .groups = "drop") %>%
  dplyr::mutate(log_ph_pc = log10(pmax(ph_pc, 1e-6)))

need_fallback <- setdiff(period_info$period, unique(ph_period_main$period))
ph_period_fb <- if (length(need_fallback)) {
  tidyr::crossing(county_ihme = unique(membership$county_ihme), period = need_fallback) %>%
    dplyr::left_join(ph_any, by = "county_ihme")
} else tibble(county_ihme = character(), period = character(), ph_pc = numeric(), log_ph_pc = numeric())

ph_period <- dplyr::bind_rows(ph_period_main, ph_period_fb)

cat("\n# Period→finance-year map (forced to available years)\n")
print(period_info %>% dplyr::mutate(fin_year = as.character(fin_year)))
cat("\n# PH spend rows by period (should be large)\n")
print(ph_period %>% dplyr::count(period, name = "rows") %>% dplyr::arrange(period))
cat("\n# ph_pc rows by fin_year\n")
print(ph_pc %>% dplyr::count(fin_year, name = "rows") %>% dplyr::arrange(fin_year))

# ---- A) Means by reporting type across periods ----------------
detail_rep <- detail_long %>%
  left_join(rep_lu, by = "county_ihme") %>%
  filter(!is.na(reporting_type)) %>%
  group_by(period, reporting_type, metric) %>%
  summarise(avg_value = mean(value, na.rm = TRUE),
            n = dplyr::n(), .groups = "drop") %>%
  left_join(period_info %>% select(period, start), by = "period") %>%
  mutate(period_ord = factor(period, levels = period_info$period[order(period_info$start)]))

if (nrow(detail_rep) > 0) {
  g_detail_rep <- ggplot(detail_rep, aes(period_ord, avg_value, group = reporting_type, colour = reporting_type)) +
    geom_line(linewidth = 1) +
    geom_point(size = 2) +
    facet_wrap(~ metric, scales = "free_y", ncol = 2) +
    labs(title = "Phillips detail metrics by reporting type (cluster→county, period means)",
         x = NULL, y = "Mean value", colour = "Reporting type") +
    theme_bw()
  ggsave(here("figures","phillips_detail","detail_by_reporting_type_timeseries.png"),
         g_detail_rep, width = 9, height = 6.5, dpi = 300)
  print(g_detail_rep)
} else {
  message("No rows for reporting-type timeseries — skipping the plot.")
}

# ---- B) R²: metric ~ reporting_type (per period) --------------
r2_rep <- detail_long %>%
  left_join(rep_lu, by = "county_ihme") %>%
  filter(!is.na(reporting_type)) %>%
  group_by(period, metric) %>%
  group_modify(\(d, key) {
    nn <- nrow(d)
    if (nn < 50 || n_distinct(d$reporting_type) < 2) return(tibble(r2 = NA_real_, n = nn))
    tibble(r2 = summary(lm(value ~ reporting_type, data = d))$r.squared, n = nn)
  }) %>% ungroup()

cat("\nR²: Phillips metric ~ reporting_type (by period)\n")
print(r2_rep %>% arrange(metric, period))

# ---- C) R²: metric ~ log10(PH spend) (per period) -------------
r2_spend <- detail_long %>%
  inner_join(ph_period, by = c("county_ihme","period")) %>%
  group_by(period, metric) %>%
  group_modify(\(d, key) {
    nn <- nrow(d)
    if (nn < 50 || !any(is.finite(d$log_ph_pc))) return(tibble(r2 = NA_real_, n = nn))
    tibble(r2 = summary(lm(value ~ log_ph_pc, data = d))$r.squared, n = nn)
  }) %>% ungroup()

cat("\nR²: Phillips metric ~ log10(PH spend per capita) (by period)\n")
print(r2_spend %>% arrange(metric, period))

# ---- D) Scatter (guarded) ------------------------------------
scatter_sample <- detail_long %>%
  inner_join(ph_period, by = c("county_ihme","period")) %>%
  left_join(period_info %>% select(period, start), by = "period") %>%
  mutate(period_ord = factor(period, levels = period_info$period[order(period_info$start)])) %>%
  filter(is.finite(value), is.finite(log_ph_pc))

if (nrow(scatter_sample) > 0) {
  g_detail_spend <- ggplot(scatter_sample, aes(log_ph_pc, value)) +
    geom_point(alpha = 0.35, size = 1) +
    geom_smooth(method = "lm", se = FALSE, linewidth = 0.8) +
    facet_grid(metric ~ period_ord, scales = "free_y") +
    labs(title = "Phillips detail metrics vs log10(PH spend per capita)",
         x = "log10(PH spend per capita)", y = "Metric value") +
    theme_bw()
  ggsave(here("figures","phillips_detail","detail_vs_ph_spend_scatter.png"),
         g_detail_spend, width = 12, height = 7.5, dpi = 300)
  print(g_detail_spend)
} else {
  message("No rows for spend vs metrics scatter — skipping the plot.")
}

# ---- E) NEW: Time series with multiple lines for PH-spend quintiles ----
# Quintiles computed GLOBALLY from each county's median PH spend (ph_any),
# so periods with sparse PH coverage still get full quintile lines.
labs_quint <- c("Q1 lowest","Q2","Q3","Q4","Q5 highest")

# Build monotone breaks even if there are ties
make_quintile_breaks <- function(v) {
  v <- v[is.finite(v)]
  stopifnot(length(v) > 0)
  qs <- quantile(v, probs = seq(0, 1, 0.2), na.rm = TRUE, names = FALSE, type = 7)
  # enforce strictly increasing breaks
  for (i in 2:length(qs)) if (qs[i] <= qs[i-1]) qs[i] <- qs[i-1] + .Machine$double.eps
  qs[1] <- min(v) - 1e-9
  qs[length(qs)] <- max(v) + 1e-9
  qs
}

quint_breaks <- make_quintile_breaks(ph_any$ph_pc)

county_quintile <- ph_any %>%
  mutate(spend_quintile = cut(ph_pc, breaks = quint_breaks, include.lowest = TRUE,
                              right = FALSE, labels = labs_quint)) %>%
  select(county_ihme, spend_quintile)

period_levels <- period_info$period[order(period_info$start)]

detail_with_quint <- detail_long %>%
  left_join(county_quintile, by = "county_ihme") %>%
  filter(!is.na(spend_quintile)) %>%
  mutate(period_ord = factor(period, levels = period_levels))

# Summarize mean metric per period × quintile
ts_quint <- detail_with_quint %>%
  group_by(metric, period_ord, spend_quintile) %>%
  summarise(avg_value = mean(value, na.rm = TRUE), n = dplyr::n(), .groups = "drop")

cat("\n# Rows per period × quintile (any metric):\n")
print(
  detail_with_quint %>%
    count(period_ord, spend_quintile, name = "rows") %>%
    tidyr::complete(period_ord = period_levels, spend_quintile = labs_quint, fill = list(rows = 0)) %>%
    arrange(period_ord, spend_quintile)
)

# Plot (guarded)
if (nrow(ts_quint) > 0) {
  g_quint <- ggplot(ts_quint,
                    aes(x = period_ord, y = avg_value,
                        group = spend_quintile, colour = spend_quintile)) +
    geom_line(linewidth = 1) +
    geom_point(size = 2) +
    facet_wrap(~ metric, ncol = 2, scales = "free_y") +
    labs(
      title = "Phillips detail metrics over time by public-health spend quintile",
      x = NULL, y = "Mean metric value", colour = "PH spend (per-capita) quintile"
    ) +
    theme_bw()
  ggsave(here("figures","phillips_detail","detail_timeseries_by_spend_quintile.png"),
         g_quint, width = 10, height = 6.5, dpi = 300)
  print(g_quint)
} else {
  message("No rows for time-series-by-quintile — skipping the plot.")
}

# ---- F) Correlations with direction score (by period) ---------
assign_period <- function(y, info) {
  with(info, {
    p <- period[y >= start & y <= end]
    ifelse(length(p) >= 1, p[1], NA_character_)
  })
}
stopifnot(exists("direction_year"))
direction_period <- direction_year %>%
  mutate(period = vapply(year, assign_period, character(1), info = period_info)) %>%
  filter(!is.na(period)) %>%
  group_by(county_ihme, period) %>%
  summarise(direction_score = mean(direction_score, na.rm = TRUE), .groups = "drop")

detail_wide <- detail_long %>%
  select(county_ihme, period, metric, value) %>%
  distinct() %>%
  pivot_wider(names_from = metric, values_from = value)

corr_tbl <- direction_period %>%
  inner_join(detail_wide, by = c("county_ihme","period"))

cols_metrics <- setdiff(names(detail_wide), c("county_ihme","period"))
corr_out <- map_dfr(split(corr_tbl, corr_tbl$period), function(dd) {
  tibble(
    period = unique(dd$period),
    metric = cols_metrics,
    pearson_r = map_dbl(cols_metrics, ~ suppressWarnings(
      cor(dd$direction_score, dd[[.x]], use = "pairwise.complete.obs")
    )),
    n = nrow(dd)
  )
})
readr::write_csv(corr_out, here("output","correlation_direction_vs_phillips_by_period.csv"))
cat("\nSaved correlations to: ", here("output","correlation_direction_vs_phillips_by_period.csv"), "\n")
print(corr_out %>% arrange(metric, period))
```
Check by income
```{r}
# ---- G) Time series by INCOME quintiles ------------------------
safe_quintile <- function(x) {
  labs <- c("Q1 lowest","Q2","Q3","Q4","Q5 highest")
  v <- x[is.finite(x)]
  if (length(v) < 5L || length(unique(v)) < 5L) {
    return(factor(rep(NA_character_, length(x)), levels = labs))
  }
  qs <- quantile(v, probs = seq(0, 1, 0.2), na.rm = TRUE, names = FALSE, type = 7)
  qs[1] <- min(v) - 1e-9; qs[length(qs)] <- max(v) + 1e-9
  cut(x, breaks = qs, include.lowest = TRUE, right = FALSE, labels = labs)
}

period_levels <- period_info$period[order(period_info$start)]

# Join detail with ACS income
detail_with_income <- detail_long %>%
  inner_join(income_all %>% rename(fin_year = acs_year), by = "county_ihme") %>%
  inner_join(period_info %>% select(period, fin_year), by = "period") %>%
  filter(is.finite(value), is.finite(avg_income)) %>%
  group_by(period) %>%
  mutate(income_quintile = safe_quintile(avg_income)) %>%
  ungroup() %>%
  filter(!is.na(income_quintile)) %>%
  mutate(period_ord = factor(period, levels = period_levels))

ts_income <- detail_with_income %>%
  group_by(metric, period_ord, income_quintile) %>%
  summarise(avg_value = mean(value, na.rm = TRUE), n = n(), .groups = "drop")

if (nrow(ts_income) > 0) {
  g_income <- ggplot(ts_income,
                     aes(x = period_ord, y = avg_value,
                         group = income_quintile, colour = income_quintile)) +
    geom_line(linewidth = 1) +
    geom_point(size = 2) +
    facet_wrap(~ metric, ncol = 2, scales = "free_y") +
    labs(title = "Phillips detail metrics over time by INCOME quintile",
         x = NULL, y = "Mean metric value", colour = "Income quintile") +
    theme_bw()
  ggsave(here("figures","phillips_detail","detail_timeseries_by_income_quintile.png"),
         g_income, width = 10, height = 6.5, dpi = 300)
  print(g_income)
} else {
  message("Income quintile time-series empty after guards — skipping.")
}


# ---- H) Time series by BA+ quintiles ---------------------------
detail_with_ba <- detail_long %>%
  inner_join(ba_all %>% rename(fin_year = acs_year), by = "county_ihme") %>%
  inner_join(period_info %>% select(period, fin_year), by = "period") %>%
  filter(is.finite(value), is.finite(ba_share)) %>%
  group_by(period) %>%
  mutate(ba_quintile = safe_quintile(ba_share)) %>%
  ungroup() %>%
  filter(!is.na(ba_quintile)) %>%
  mutate(period_ord = factor(period, levels = period_levels))

ts_ba <- detail_with_ba %>%
  group_by(metric, period_ord, ba_quintile) %>%
  summarise(avg_value = mean(value, na.rm = TRUE), n = n(), .groups = "drop")

if (nrow(ts_ba) > 0) {
  g_ba <- ggplot(ts_ba,
                 aes(x = period_ord, y = avg_value,
                     group = ba_quintile, colour = ba_quintile)) +
    geom_line(linewidth = 1) +
    geom_point(size = 2) +
    facet_wrap(~ metric, ncol = 2, scales = "free_y") +
    labs(title = "Phillips detail metrics over time by BA+ share quintile",
         x = NULL, y = "Mean metric value", colour = "BA+ quintile") +
    theme_bw()
  ggsave(here("figures","phillips_detail","detail_timeseries_by_ba_quintile.png"),
         g_ba, width = 10, height = 6.5, dpi = 300)
  print(g_ba)
} else {
  message("BA+ quintile time-series empty after guards — skipping.")
}

```






