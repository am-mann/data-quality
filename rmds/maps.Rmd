---
title: "Mapping Notebook"
output: html_notebook
---

Temporality stable groupings
```{r}
# ──────────────────────────────────────────────────────────────
# 0 · Setup – packages, paths, helpers
# ──────────────────────────────────────────────────────────────
required <- c("sf", "tigris", "readr", "dplyr", "stringr", "tidyr",
              "purrr", "ggplot2", "viridis", "patchwork")
missing <- setdiff(required, rownames(installed.packages()))
if (length(missing)) install.packages(missing)
invisible(lapply(required, library, character.only = TRUE))
options(tigris_use_cache = TRUE, tigris_class = "sf")

comp_vars_pct <- c("pct_marstat_comp_k", "pct_placdth_comp_k", "pct_educ_comp_k", "pct_mandeath_comp_k","pct_age_comp_k", "pct_sex_comp_k", "pct_race_comp_k", "DQ_overall")

vars_to_map <- c(
  "prop_garbage", "prop_light",  "pct_acc_miss", "pct_overd_miss",  "pct_gc_I64", "pct_gc_C_misc", "pct_gc_I10", "pct_gc_R_misc", "pct_gc_N19", "pct_gc_J80", comp_vars_pct
)

# ──────────────────────────────────────────────────────────────
# 2 · Load & preprocess county-year data
# ──────────────────────────────────────────────────────────────
dq <- read_csv(here::here("data", "county_year_quality_metrics.csv.gz"), show_col_types = FALSE)

dq <- dq |>
  mutate(
    year        = as.integer(year),
    time_window = case_when(
      year >= 1999 & year <= 2004 ~ "1999_2004",
      year >= 2005 & year <= 2010 ~ "2005_2010",
      year >= 2011 & year <= 2017 ~ "2011_2017",
      year >= 2018 & year <= 2022 ~ "2018_2022",
      TRUE ~ NA_character_
    )
  )

dq <- dq |>
  mutate(across(
    ends_with("_comp_k"),
    ~ 100 * (.x / n_cert),
    .names = "pct_{.col}"
  ))

# ──────────────────────────────────────────────────────────────
# 1 · Load IHME grouping dictionary  (orig_fips → ihme_fips)  ──
# ──────────────────────────────────────────────────────────────
load(here::here("data_raw", "ihme_fips.rda"))     # gives object 'ihme_fips'

ihme_map <- ihme_fips %>%
  dplyr::transmute(
    fips_raw    = stringr::str_pad(orig_fips, 5, pad = "0"),
    county_ihme = stringr::str_pad(ihme_fips, 5, pad = "0")
  ) %>%
  dplyr::distinct()                                # one row per county

# add Broomfield (created 2001) if missing ------------
if (!"08014" %in% ihme_map$fips_raw) {
  ihme_map <- dplyr::add_row(
    ihme_map,
    fips_raw    = "08014",
    county_ihme = "08014"
  )
  message("• Added Broomfield (08014) to IHME map")
}

# ──────────────────────────────────────────────────────────────
# 2 · Attach IHME group IDs to your dq table
#    (rename raw-FIPS column here if it differs)
# ──────────────────────────────────────────────────────────────
raw_fips_col <- "fips"            # ← change if your column name differs
stopifnot(raw_fips_col %in% names(dq))

dq <- dq %>%
  dplyr::left_join(
    ihme_map,
    by = setNames("fips_raw", raw_fips_col)
  )

stopifnot(!anyNA(dq$county_ihme))   # every row now has a group ID

# ──────────────────────────────────────────────────────────────
# 3 · 5-year averages *by IHME group* (not individual counties)
# ──────────────────────────────────────────────────────────────
dq_avg <- dq %>%
  dplyr::group_by(county_ihme, time_window) %>%
  dplyr::summarise(
    across(all_of(vars_to_map), ~ mean(.x, na.rm = TRUE)),
    n_years = dplyr::n(),
    .groups = "drop"
  )

# ──────────────────────────────────────────────────────────────
# 4 · Build one polygon per IHME group for each shapefile year
# ──────────────────────────────────────────────────────────────
crs_proj <- 2163
shapefile_years <- c("1999_2004" = 2000,
                     "2005_2010" = 2010,
                     "2011_2017" = 2015,
                     "2018_2022" = 2020)

st_shift <- function(x, offset) { st_geometry(x) <- st_geometry(x) + offset; x }
st_scale <- function(x, factor) {
  ctr <- sf::st_centroid(sf::st_union(x))
  sf::st_geometry(x) <- (sf::st_geometry(x) - ctr) * factor + ctr
  x
}

library(purrr)   # for imap()

joined_by_period <- purrr::imap(
  shapefile_years,
  function(year, window) {

    counties_sf <- tigris::counties(year = year, cb = TRUE, class = "sf") %>%
      sf::st_zm(drop = TRUE, what = "ZM") %>%
      sf::st_transform(crs_proj) %>%
      dplyr::mutate(fips_raw = GEOID) %>%               # raw 5-digit FIPS
      dplyr::left_join(ihme_map, by = "fips_raw")

    # dissolve to one polygon per IHME group ------------
    groups_sf <- counties_sf %>%
      dplyr::group_by(county_ihme) %>%
      dplyr::summarise(.groups = "drop")     # st_union() inside summarise

    # shift & scale AK / HI ------------------------------
    alaska  <- groups_sf %>% dplyr::filter(substr(county_ihme, 1, 2) == "02") %>%
      st_scale(0.40) %>% st_shift(c(1300000, -4900000)) %>% sf::st_set_crs(crs_proj)
    hawaii  <- groups_sf %>% dplyr::filter(substr(county_ihme, 1, 2) == "15") %>%
      st_scale(1.50) %>% st_shift(c(5200000, -1400000)) %>% sf::st_set_crs(crs_proj)
    mainland <- groups_sf %>% dplyr::filter(!substr(county_ihme, 1, 2) %in% c("02", "15"))

    shape_all <- dplyr::bind_rows(mainland, alaska, hawaii)

    # attach the 5-year averages -------------------------
    dplyr::left_join(shape_all,
                     dplyr::filter(dq_avg, time_window == window),
                     by = "county_ihme")
  }
)

# make the name that your walk() expects
joined_by_period <- county_shapes_by_window

# ──────────────────────────────────────────────────────────────
# 7 · Mapping helper (identical to your original)
# ──────────────────────────────────────────────────────────────
make_map <- function(sf_data, var, title = NULL, palette = "RdBu") {
  states <- tigris::states(cb = TRUE, class = "sf") |> st_transform(2163)

  fill_scale <- if (var == "prop_garbage") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0.15, 0.4),
    oob = scales::squish,
    direction = -1,
    na.value = "grey90"
  )
} else if (var == "prop_light") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0.20, 0.40),
    oob = scales::squish,
    direction = -1,
    na.value = "grey90"
  )
} else if (var == "pct_overd_miss") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0, 0.75),
    oob = scales::squish,
    direction = -1,
    na.value = "grey90"
  )
} else if (var == "direction_score") {
  scale_fill_distiller(
    palette = palette,
    limits = c(-1,1.5),
    oob = scales::squish,
    direction = -1,
    na.value = "grey90"
  )
} else if (var == "DQ_overall") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0,0.40),
    oob = scales::squish,
    direction = 1,
    na.value = "grey90"
  )
} else if (var == "pct_acc_miss") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0, 0.75),
    oob = scales::squish,
    direction = -1,
    na.value = "grey90"
  )
} else if (var == "pct_mandeath_comp_k") {
  scale_fill_distiller(
    palette = palette,
    limits = c(5, 20),
    oob = scales::squish,
    direction = 1,
    na.value = "grey90"
  )
} else if (var == "pct_placdth_comp_k") {
  scale_fill_distiller(
    palette = palette,
    limits = c(98, 100),
    oob = scales::squish,
    direction = 1,
    na.value = "grey90"
  )
} else if (var == "pct_educ_comp_k") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0, 100),
    oob = scales::squish,
    direction = 1,
    na.value = "grey90"
  )
} else if (var == "pct_marstat_comp_k") {
  scale_fill_distiller(
    palette = palette,
    limits = c(97, 100),
    oob = scales::squish,
    direction = 1,
    na.value = "grey90"
  )
} else if (var %in% c("pct_age_comp_k", "pct_sex_comp_k", "pct_race_comp_k")) {
  scale_fill_distiller(
    palette = palette,
    limits = c(99.5, 100),
    oob = scales::squish,
    direction = 1,
    na.value = "grey90"
  )
} else if (var == "pct_gc_I64") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0.01, 0.05),
    oob = scales::squish,
    direction = -1,
    na.value = "grey90"
  )
} else if (var == "pct_gc_C_misc") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0.005, 0.03),
    oob = scales::squish,
    direction = -1,
    na.value = "grey90"
  )
} else if (var == "pct_gc_I10") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0, 0.02),
    oob = scales::squish,
    direction = -1,
    na.value = "grey90"
  )
} else if (var == "pct_gc_R_misc") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0, 0.03),
    oob = scales::squish,
    direction = -1,
    na.value = "grey90"
  )
} else if (var == "pct_gc_N19") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0, 0.025),
    oob = scales::squish,
    direction = -1,
    na.value = "grey90"
  )
} else if (var == "pct_gc_J80") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0, 0.005),
    oob = scales::squish,
    direction = -1,
    na.value = "grey90"
  )
} else if (startsWith(var, "pct_") || var == "overall_completeness_pct") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0, 100),
    oob = scales::squish,
    direction = 1,
    na.value = "grey90"
  )
} else {
  scale_fill_distiller(
    palette = palette,
    oob = scales::squish,
    na.value = "grey90",
    direction = -1
  )
}

  ggplot() +
    geom_sf(data = sf_data, aes(fill = .data[[var]]), colour = NA) +
    geom_sf(data = states, fill = NA, colour = "white", linewidth = 0.3) +
    fill_scale +
    coord_sf(xlim = c(-2500000, 2500000),
             ylim = c(-2200000, 730000),
             expand = FALSE) +
    labs(title = title %||% var, fill = NULL) +
    theme_void() +
    theme(
      plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
      legend.text = element_text(size = 9)
    )
}

# ──────────────────────────────────────────────────────────────
# 8 · Save maps
# ──────────────────────────────────────────────────────────────
output_dir <- "/Users/amymann/Documents/Data Quality Project/data_quality/figures/5yr_avg_stable"
dir.create(output_dir, showWarnings = FALSE, recursive = TRUE)

walk(vars_to_map, function(var) {
  walk(names(joined_by_period), function(win) {
    map_plot <- make_map(joined_by_period[[win]], var, paste(var, win))
    ggsave(
      filename = file.path(output_dir, paste0(var, "_", win, ".png")),
      plot     = map_plot,
      width    = 8, height = 6, dpi = 320
    )
  })
})

```
Relationship between rural and garbage
```{r}
# 0 · Load packages -------------------------------------------------------
library(dplyr)
library(stringr)
library(ggplot2)
library(scales)
library(tigris)
library(tidycensus)
library(readr)
library(here)

# 1 · Load your data ------------------------------------------------------
dq <- read_csv(here("data", "county_year_quality_metrics.csv.gz"),
               show_col_types = FALSE)

# 3 · Convert county codes to GEOID ---------------------------------------
dq_all <- dq %>%
  mutate(
    GEOID = county_ihme
  )

cat("Counties without valid GEOID:", sum(is.na(dq_all$GEOID)), "\n")

# 4 · Get 2022 population estimates ---------------------------------------
census_api_key("671b64055fc192103cbc199d2dff91b46d9cc781", overwrite = FALSE)

pop22 <- get_acs(
  geography = "county",
  variables = "B01003_001",
  year = 2022,
  survey = "acs5"
) %>%
  select(GEOID, pop_2022 = estimate)

# 5 · Define county size buckets ------------------------------------------
cuts   <- c(0, 50000, 100000, 250000, 1000000, Inf)
labels <- c("<50k", "50–100k", "100–250k", "250k–1M", "≥1M")

county_sizes <- pop22 %>%
  mutate(size_bucket = cut(pop_2022, breaks = cuts, labels = labels, right = FALSE)) %>%
  select(GEOID, pop_2022, size_bucket)

# 6 · Join population and size to main data -------------------------------
dq_all <- dq_all %>%
  left_join(pop22, by = "GEOID") %>%
  left_join(county_sizes %>% select(GEOID, size_bucket), by = "GEOID")

# 7 · Indicators to plot --------------------------------------------------
indicators <- c(
  "prop_garbage",
  "prop_light",
  "pct_overd_miss",
  "pct_acc_miss")

# 8 · Time trends by size bucket ------------------------------------------
for (var in indicators) {
  ts_df <- dq_all %>%
    filter(!is.na(size_bucket)) %>%
    group_by(year, size_bucket) %>%
    summarise(avg_prop = mean(.data[[var]], na.rm = TRUE), .groups = "drop")
  
  g <- ggplot(ts_df, aes(year, avg_prop, colour = size_bucket)) +
    geom_line(linewidth = 1) +
    scale_y_continuous(labels = percent_format(accuracy = 0.1)) +
    labs(
      title = paste("Trend of", gsub("_", " ", var), "by county size"),
      x = NULL,
      y = "Mean percentage",
      colour = "2022 size bucket"
    ) +
    theme_bw()
  
  print(g)
  
  ggsave(
    filename = here("figures", paste0("trend_", var, "_by_size_bucket.png")),
    plot = g,
    width = 7, height = 4, dpi = 300
  )
}

# 9 · Scatterplots: pop vs indicator (2022) -------------------------------
scatter_data <- dq_all %>%
  filter(year == 2022, !is.na(pop_2022))

for (var in indicators) {
  scatter_df <- scatter_data %>%
    filter(!is.na(.data[[var]]))
  
  g <- ggplot(scatter_df, aes(x = pop_2022, y = .data[[var]])) +
    geom_point(alpha = 0.6) +
    scale_x_log10(labels = comma) +
    scale_y_continuous(labels = percent_format(accuracy = 0.1)) +
    labs(
      title = paste("County population vs", gsub("_", " ", var), "(2022)"),
      x = "County population (log10 scale)",
      y = paste("Proportion:", gsub("_", " ", var))
    ) +
    theme_bw()
  
  print(g)
  
  ggsave(
    filename = here("figures", paste0("scatter_", var, "_vs_population_2022.png")),
    plot = g,
    width = 7, height = 4, dpi = 300
  )
}
```
Clustering or anomaly detection: Uses unsupervised ML (clustering, autoencoders) to identify unusual records or patterns. If a county’s records are more often flagged as outliers, that suggests quality issues.
```{r}
# 0.  PACKAGES ------------------------------------------------------------------
library(tidyverse)
library(solitude)
library(dbscan)
library(factoextra)
library(patchwork)
library(here)

# 1.  LOAD & CLEAN DATA --------------------------------------------------------
df <- read_csv(
  here("data", "county_year_quality_metrics.csv.gz"),
  show_col_types = FALSE
) %>%
  mutate(
    year = as.integer(year)
  )

# 2.  COMPUTE prop_all_comp & DEFINE VARIABLES --------------------------------
df <- df %>%
  mutate(
    prop_all_comp = (
      marstat_comp_k + placdth_comp_k + educ_comp_k +
      age_comp_k     + sex_comp_k      + mandeath_comp_k
    ) / (6 * n_cert)
  )

quality_vars <- c(
  "DQ_prop_garbage", "prop_light", "pct_overd_miss", "pct_acc_miss"
)

# 3.  AVERAGE OVER YEARS BY COUNTY_ihme ----------------------------------------
county_avg <- df %>%
  filter(year >= 1999, year <= 2022) %>%
  group_by(county_ihme) %>%
  summarise(
    across(all_of(quality_vars), ~ mean(.x, na.rm = TRUE)),
    n_cert = mean(n_cert, na.rm = TRUE),
    .groups = "drop"
  )

# ------------------------------------------------------------------
# 4 · SCALE, FIT MODELS & ADD SCORES   (corrected)
# ------------------------------------------------------------------

## 4.1  Impute medians for NAs -------------------------------------
mat <- county_avg %>%
  mutate(across(all_of(quality_vars), 
                ~ ifelse(is.na(.x), median(.x, na.rm = TRUE), .x)))

## 4.2  Drop any rows missing county_ihme --------------------------
if (any(is.na(mat$county_ihme))) {
  dropped <- mat %>% filter(is.na(county_ihme)) %>% nrow()
  message("Dropping ", dropped, " rows with missing county_ihme.")
  mat <- mat %>% filter(!is.na(county_ihme))
}

## 4.3  Build scaled predictor matrix ------------------------------
X_df <- mat %>%
  select(all_of(quality_vars)) %>%
  scale() %>%
  as.data.frame()
row.names(X_df) <- mat$county_ihme

# 4.4  Isolation Forest (solitude) ------------------------------
iso_mod <- solitude::isolationForest$new(num_trees = 100)
iso_mod$fit(X_df)
iso_scores <- iso_mod$predict(X_df)$anomaly_score

# 4.5  Local Outlier Factor (dbscan) -----------------------------
lof_scores <- dbscan::lof(as.matrix(X_df),
                          minPts = round(sqrt(nrow(X_df))))

# 4.6  k-means clustering (k = 3) --------------------------------
set.seed(123)
km <- kmeans(X_df, centers = 3, nstart = 25)

# 4.7  Bind results back to mat ----------------------------------
mat <- mat %>%
  mutate(
    iso_score  = iso_scores,
    lof_score  = lof_scores,
    km_cluster = km$cluster,
    iso_rank   = rank(-iso_score, ties.method = "first"),
    lof_rank   = rank(-lof_score,  ties.method = "first"),
    anomaly_flag = iso_rank <= ceiling(0.05 * n()) |
                   lof_rank <= ceiling(0.05 * n())
  )

# store final results
res <- mat

# 5.  PCA FOR PLOTTING --------------------------------------------------------
pca <- prcomp(X_df, center = TRUE, scale. = FALSE)
pca2 <- as_tibble(pca$x[,1:2]) %>%
  set_names(c("PC1", "PC2")) %>%
  mutate(
    county_ihme  = row.names(pca$x),
    iso_score    = res$iso_score,
    km_cluster   = factor(res$km_cluster),
    anomaly_flag = res$anomaly_flag
  )

# 6.  SAVE OUTPUTS ------------------------------------------------------------
write_csv(res,
          here("output", "county_outlier_scores_1999_2022_avg.csv"))

p1 <- ggplot(pca2, aes(PC1, PC2, colour = km_cluster)) +
  geom_point(alpha = 0.7, size = 3) +
  labs(title = "K-means clusters (k = 3)") +
  theme_minimal()

p2 <- ggplot(pca2, aes(PC1, PC2, size = iso_score)) +
  geom_point(colour = "red", alpha = 0.5) +
  labs(title = "Isolation Forest anomaly score") +
  theme_minimal()

(p1 + p2) %>%
  ggsave(
    filename = here("figures", "outlier_plot_1999_2022_avg.png"),
    width    = 10, height = 5, dpi = 300
  )

# 7.  PRINT TOP OUTLIERS & COMPUTE DIRECTION_SCORE ---------------------------
message("Top 15 counties by Isolation Forest score:\n")
print(res %>% arrange(desc(iso_score)) %>% slice_head(n = 15))

message("Top 15 counties by LOF score:\n")
print(res %>% arrange(desc(lof_score)) %>% slice_head(n = 15))

res <- res %>%
  mutate(
    across(all_of(quality_vars),
           ~ as.numeric(scale(.x)),
           .names = "z_{.col}"),
    direction_score = (
      z_prop_light     +
      z_pct_overd_miss +
      z_pct_acc_miss   +
      z_DQ_prop_garbage
    ) / 4
  )

message("Script complete.")
```
map direction scores
```{r}

# map direction scores
# ──────────────────────────────────────────────────────────────
# 0 · Packages (load after your existing libs)
# ──────────────────────────────────────────────────────────────
library(sf)
library(tigris)      # for county & state shapes
library(ggplot2)
library(dplyr)
library(stringr)

options(tigris_use_cache = TRUE, tigris_class = "sf")
crs_proj <- 2163     # CONUS Albers (EPSG:2163)

# ──────────────────────────────────────────────────────────────
# 1 · Pull direction_score & county_ihme from your results
# ──────────────────────────────────────────────────────────────
dir_scores <- res %>%       
  select(county_ihme, direction_score)

# ──────────────────────────────────────────────────────────────
# 2 · Build county geometry collapsed to county_ihme
# ──────────────────────────────────────────────────────────────
# 2015 TIGER/CB shapefile is a good compromise between detail & performance
county_sf <- counties(cb = TRUE, year = 2015, class = "sf") %>%
  st_transform(crs_proj) %>%
  mutate(fips = GEOID,
         fips = str_pad(fips, 5, pad = "0")) %>%
  # Attach IHME mapping and collapse to temporally-stable polygons
  left_join(ihme_xwalk, by = c("fips" = "fips")) %>%
  mutate(county_ihme = coalesce(county_ihme, fips)) %>%
  select(county_ihme, geometry) %>%
  group_by(county_ihme) %>%
  summarise(geometry = st_union(geometry), .groups = "drop")  # dissolve splits


# ──────────────────────────────────────────────────────────────
# 3 · Join scores ➜ geometry
# ──────────────────────────────────────────────────────────────
map_sf <- left_join(county_sf, dir_scores, by = "county_ihme")

# ──────────────────────────────────────────────────────────────
# 4 · Build colour scale limits (symmetric diverging)
# ──────────────────────────────────────────────────────────────
max_abs <- max(abs(map_sf$direction_score), na.rm = TRUE)

# ─────────────────────────────────────────────────────────
# 0 · Prepare geometry for every county_ihme
# ─────────────────────────────────────────────────────────
library(sf)
library(tigris)
options(tigris_use_cache = TRUE, tigris_class = "sf")

crs_proj <- 2163

# county_sf: one polygon per IHME county_ihme
county_sf <- counties(year = 2015, cb = TRUE, class = "sf") |>
  st_transform(crs_proj) |>
  mutate(fips = GEOID) |>
  left_join(ihme_xwalk, by = c("fips" = "fips")) |>
  mutate(county_ihme = dplyr::coalesce(county_ihme, fips)) |>
  select(county_ihme, geometry) |>
  group_by(county_ihme) |>
  summarise(geometry = st_union(geometry), .groups = "drop")

# ─────────────────────────────────────────────────────────
# 1 · Join your results (res) to geometry
#    res must contain 'county_ihme' and the variable you want to map
# ─────────────────────────────────────────────────────────
map_sf <- county_sf |>
  left_join(res, by = "county_ihme")      # res is your data-frame

# ─────────────────────────────────────────────────────────
# 2 · Call make_map()
#    Example: visualise the direction_score column
# ─────────────────────────────────────────────────────────
map_plot <- make_map(
  sf_data = map_sf,
  var     = "direction_score",            # any column present in map_sf
  title   = "Average Z-score with direction (1999–2022)"
)

# Show in RStudio viewer
print(map_plot)

# Save to file (optional)
ggsave(here("figures", "direction_score_map.png"), map_plot,
       width = 9, height = 6, dpi = 320)
```
Direction scores by 5 year period with global mean and standard deviation
```{r}
# ──────────────────────────────────────────────────────────────
# 0 · Globals  (μ and σ from the overall 1999–2022 county means)
# ──────────────────────────────────────────────────────────────
quality_vars <- c("DQ_prop_garbage", "prop_light", "pct_overd_miss", "pct_acc_miss")

# If `mat` from the earlier chunk is in memory, use that; otherwise rebuild quickly
if (!exists("mat")) {
  mat <- df %>%                            # `df` was read in the anomaly-detection chunk
    filter(year >= 1999, year <= 2022) %>%
    mutate(prop_all_comp = (marstat_comp_k + placdth_comp_k +
                             age_comp_k + sex_comp_k ) /
                           (4 * n_cert)) %>%
    group_by(county_ihme) %>%
    summarise(across(all_of(quality_vars), ~ mean(.x, na.rm = TRUE)),
              .groups = "drop")
}

global_means <- sapply(mat[quality_vars], mean, na.rm = TRUE)
global_sds   <- sapply(mat[quality_vars], sd,   na.rm = TRUE)

# ──────────────────────────────────────────────────────────────
# 1 · Period definitions
# ──────────────────────────────────────────────────────────────
periods <- list(
  `1999_2004` = 1999:2004,
  `2005_2010` = 2005:2010,
  `2011_2017` = 2011:2017,
  `2018_2022` = 2018:2022,
  `2020_2022` = 2020:2022
)

# ──────────────────────────────────────────────────────────────
# 2 · Geometry (one polygon per IHME county_ihme)
# ──────────────────────────────────────────────────────────────
if (!exists("county_sf")) {
  county_sf <- counties(cb = TRUE, year = 2015, class = "sf") %>%
    st_transform(2163) %>%
    mutate(fips = GEOID) %>%
    left_join(ihme_xwalk, by = "fips") %>%
    mutate(county_ihme = coalesce(county_ihme, fips)) %>%
    select(county_ihme, geometry) %>%
    group_by(county_ihme) %>%
    summarise(geometry = st_union(geometry), .groups = "drop")
}

# ──────────────────────────────────────────────────────────────
# 3 · Loop over periods: compute & map direction_score
# ──────────────────────────────────────────────────────────────
out_dir <- here::here("figures", "direction_score_periods")
dir.create(out_dir, showWarnings = FALSE, recursive = TRUE)

for (pname in names(periods)) {
  yrs <- periods[[pname]]

  # county means within the period
  peri_df <- df %>%
    filter(year %in% yrs) %>%
    mutate(prop_all_comp = (marstat_comp_k + placdth_comp_k + educ_comp_k +
                             age_comp_k + sex_comp_k + mandeath_comp_k) /
                           (6 * n_cert)) %>%
    group_by(county_ihme) %>%
    summarise(across(all_of(quality_vars), ~ mean(.x, na.rm = TRUE)),
              .groups = "drop")

  # fixed-reference Z-scores
  peri_df <- peri_df %>%
    mutate(across(all_of(quality_vars),
                  \(x, nm=cur_column()) (x - global_means[nm]) / global_sds[nm],
                  .names = "z_{.col}")) %>%
    mutate(
      direction_score = (z_prop_light + z_pct_overd_miss +
                         z_pct_acc_miss + z_DQ_prop_garbage) / 4
    )
  
    # county means within the period  ──────────────────────────────
  peri_df <- df %>%
    filter(year %in% yrs) %>%
    mutate(prop_all_comp = (marstat_comp_k + placdth_comp_k + educ_comp_k +
                            age_comp_k + sex_comp_k + mandeath_comp_k) /
                           (6 * n_cert)) %>%
    group_by(county_ihme) %>%
    summarise(across(all_of(quality_vars), ~ mean(.x, na.rm = TRUE)),
              .groups = "drop")

  ## NEW 1: guarantee every county_ihme appears (even if all NA)
  peri_df <- county_sf %>%                        # drop geometry, keep IDs
    st_drop_geometry() %>%
    select(county_ihme) %>%
    left_join(peri_df, by = "county_ihme")

  ## NEW 2: impute missing period means with the *same* values
  ##        we used for the overall map (global_means)
  peri_df <- peri_df %>%
    mutate(across(all_of(quality_vars),
                  \(x, nm = cur_column()) replace_na(x, global_means[nm])))

  # fixed-reference Z-scores  ───────────────────────────────────
  peri_df <- peri_df %>%
    mutate(across(all_of(quality_vars),
                  \(x, nm = cur_column()) (x - global_means[nm]) / global_sds[nm],
                  .names = "z_{.col}")) %>%
    ## NEW 3: compute score with all four components now present
    mutate(direction_score = (z_prop_light + z_pct_overd_miss +
                              z_pct_acc_miss  + z_DQ_prop_garbage) / 4)


  # join geometry ➜ map
  map_sf <- county_sf %>% left_join(peri_df, by = "county_ihme")

  p <- make_map(map_sf,
                var   = "direction_score",
                title = glue::glue("Direction score {pname} (scaled to 1999–2022 μ/σ)"))

  ggsave(file.path(out_dir, glue::glue("direction_score_{pname}.png")),
         plot = p, width = 9, height = 6, dpi = 320)
}
```

```{r}
# Assign each county its 2022 size bucket (once per county)
size_r2 <- direction_year %>%
  filter(year >= 1999, year <= 2022) %>%
  group_by(county_ihme) %>%
  summarise(
    mean_direction = mean(direction_score, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  left_join(pop22, by = "county_ihme") %>%
  mutate(size_bucket = cut(pop_2022, breaks = size_breaks, labels = size_labels, right = FALSE))

# Fit one-way ANOVA
fit_size <- lm(mean_direction ~ size_bucket, data = size_r2)
r2_size <- summary(fit_size)$r.squared
r2p_size <- round(r2_size * 100, 1)

cat("County size R²:", sprintf("%.3f", r2_size),
    "→", r2p_size, "% of variance in average direction_scores explained by 2022 county size bucket.\n")

income_r2 <- direction_year %>%
  filter(year >= 1999, year <= 2022) %>%
  group_by(county_ihme) %>%
  summarise(
    mean_direction = mean(direction_score, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  left_join(income22, by = "county_ihme") %>%
  mutate(income_bucket = cut(medhhinc_2022, breaks = income_breaks, labels = income_labels, right = FALSE))

fit_income <- lm(mean_direction ~ income_bucket, data = income_r2)
r2_income <- summary(fit_income)$r.squared
r2p_income <- round(r2_income * 100, 1)

cat("Income R²:", sprintf("%.3f", r2_income),
    "→", r2p_income, "% of variance in average direction_scores explained by 2022 income bucket.\n")

```


```{r}
# ──────────────────────────────────────────────────────────────
# 11 · Education: time-series + variance explained
# ──────────────────────────────────────────────────────────────
library(tidycensus)
library(dplyr)
library(ggplot2)
library(glue)
library(scales)

# 1. Get global means and sds for each variable across all years
global_stats <- df %>%
  summarise(across(
    c(DQ_prop_garbage, prop_light, pct_overd_miss, pct_acc_miss),
    list(mean = ~mean(.x, na.rm = TRUE),
         sd   = ~sd(.x, na.rm = TRUE)),
    .names = "{.col}_{.fn}"
  ))

# 2. Define a function to compute global z-score
zscore_global <- function(x, m, s) (x - m) / s

# 3. Calculate direction_score for every county-year using global stats
direction_year <- df %>%
  filter(year >= 1999, year <= 2022) %>%
  mutate(
    z_DQ_prop_garbage  = zscore_global(DQ_prop_garbage, global_stats$DQ_prop_garbage_mean, global_stats$DQ_prop_garbage_sd),
    z_prop_light       = zscore_global(prop_light,      global_stats$prop_light_mean,      global_stats$prop_light_sd),
    z_pct_overd_miss   = zscore_global(pct_overd_miss,  global_stats$pct_overd_miss_mean,  global_stats$pct_overd_miss_sd),
    z_pct_acc_miss     = zscore_global(pct_acc_miss,    global_stats$pct_acc_miss_mean,    global_stats$pct_acc_miss_sd),
    direction_score    = (z_prop_light + z_pct_overd_miss + z_pct_acc_miss + z_DQ_prop_garbage) / 4
  ) %>%
  select(county_ihme, year, direction_score) %>%
  filter(!is.na(county_ihme), !is.na(direction_score))


# 11-a · 2022 % bachelor’s+ from ACS table B15003
edu22_raw <- get_acs(
  geography = "county",
  table     = "B15003",
  year      = 2022,
  survey    = "acs5",
  cache_table = TRUE
)

# Keep total (001) and bachelor+ (022-025) → compute share
edu22 <- edu22_raw %>%
  select(GEOID, variable, estimate) %>%
  pivot_wider(names_from = variable, values_from = estimate) %>%
  transmute(
    county_ihme        = GEOID,
    total_25_plus      = B15003_001,
    bach_plus          = B15003_022 + B15003_023 + B15003_024 + B15003_025,
    pct_bachplus_2022  = bach_plus / total_25_plus
  )

# 11-b · Build education buckets (share bachelor’s+)
edu_breaks  <- c(-Inf, .20, .30, .40, .50, Inf)
edu_labels  <- c("<20%", "20–30%", "30–40%", "40–50%", "≥50%")

direction_year <- direction_year %>%
  left_join(edu22, by = "county_ihme") %>%
  mutate(edu_bucket = cut(pct_bachplus_2022,
                          breaks = edu_breaks,
                          labels = edu_labels,
                          right  = FALSE))

# Join education data to annual direction scores
direction_year <- direction_year %>%
  left_join(edu22, by = "county_ihme")


# Proceed with analysis and plotting
ts_edu <- direction_year %>%
  filter(!is.na(edu_bucket)) %>%
  group_by(year, edu_bucket) %>%
  summarise(avg_direction = mean(direction_score, na.rm = TRUE),
            .groups = "drop")

g_edu <- ggplot(ts_edu,
                aes(year, avg_direction, colour = edu_bucket)) +
  geom_line(linewidth = 1) +
  labs(title = "Direction-score trend by 2022 education bucket",
       x = NULL, y = "Mean direction score",
       colour = "% bachelor’s or higher") +
  theme_bw()

ggsave(here("figures", "timeseries_direction_by_edu_bucket.png"),
       g_edu, width = 7, height = 4, dpi = 300)
print(g_edu)

res <- res %>%
  mutate(
    z_DQ_prop_garbage  = as.numeric(scale(DQ_prop_garbage)),
    z_prop_light       = as.numeric(scale(prop_light)),
    z_pct_overd_miss   = as.numeric(scale(pct_overd_miss)),
    z_pct_acc_miss     = as.numeric(scale(pct_acc_miss)),
    direction_score    = (z_prop_light + z_pct_overd_miss + z_pct_acc_miss + z_DQ_prop_garbage) / 4
  )

# 11-d · R²: variance explained by education
var_edu <- res %>%                       # 1999-2022 averages
  left_join(edu22, by = "county_ihme") %>%
  filter(!is.na(direction_score), !is.na(pct_bachplus_2022))

fit_edu <- lm(direction_score ~ pct_bachplus_2022, data = var_edu)
r2e  <- summary(fit_edu)$r.squared
r2ep <- round(r2e * 100, 1)

cat("Education R²:", sprintf("%.3f", r2e),
    "→", r2ep, "% of variance in average direction_scores explained by pct bachelor’s+.\n")

message("✓ Time-series plot saved: timeseries_direction_by_edu_bucket.png")
```
```{r}
# ---- County population size bucket (using 2022) ----
library(tidycensus)

# Get 2022 population for each county
pop22_raw <- get_acs(
  geography = "county",
  variables = "B01003_001",
  year = 2022,
  survey = "acs5",
  cache_table = TRUE
)

pop22 <- pop22_raw %>%
  transmute(
    county_ihme = GEOID,
    pop_2022 = estimate
  )

# Create population size buckets
size_breaks <- c(-Inf, 50000, 100000, 250000, 1e6, Inf)
size_labels <- c("<50k", "50–100k", "100–250k", "250k–1M", "≥1M")

direction_year_size <- direction_year %>%
  left_join(pop22, by = "county_ihme") %>%
  mutate(
    size_bucket = cut(pop_2022, breaks = size_breaks, labels = size_labels, right = FALSE)
  )

ts_size <- direction_year_size %>%
  filter(!is.na(size_bucket)) %>%
  group_by(year, size_bucket) %>%
  summarise(avg_direction = mean(direction_score, na.rm = TRUE), .groups = "drop")

g_size <- ggplot(ts_size, aes(year, avg_direction, colour = size_bucket)) +
  geom_line(linewidth = 1) +
  labs(
    title = "Direction-score trend by 2022 county size bucket",
    x = NULL, y = "Mean direction score",
    colour = "2022 size bucket"
  ) +
  theme_bw()

ggsave(here("figures", "timeseries_direction_by_size_bucket.png"),
       g_size, width = 7, height = 4, dpi = 300)
print(g_size)

# ---- County household income bucket (using 2022) ----

# Get 2022 median household income (ACS table B19013)
income22_raw <- get_acs(
  geography = "county",
  variables = "B19013_001",
  year = 2022,
  survey = "acs5",
  cache_table = TRUE
)

income22 <- income22_raw %>%
  transmute(
    county_ihme = GEOID,
    medhhinc_2022 = estimate
  )

# Create household income buckets (edit cutpoints as desired)
income_breaks <- c(-Inf, 45000, 55000, 65000, 75000, Inf)
income_labels <- c("<$45k", "$45–55k", "$55–65k", "$65–75k", "$75k+")

direction_year_income <- direction_year %>%
  left_join(income22, by = "county_ihme") %>%
  mutate(
    income_bucket = cut(medhhinc_2022, breaks = income_breaks, labels = income_labels, right = FALSE)
  )

ts_income <- direction_year_income %>%
  filter(!is.na(income_bucket)) %>%
  group_by(year, income_bucket) %>%
  summarise(avg_direction = mean(direction_score, na.rm = TRUE), .groups = "drop")

g_income <- ggplot(ts_income, aes(year, avg_direction, colour = income_bucket)) +
  geom_line(linewidth = 1) +
  labs(
    title = "Direction-score trend by 2022 income bucket",
    x = NULL, y = "Mean direction score",
    colour = "Household income"
  ) +
  theme_bw()

ggsave(here("figures", "timeseries_direction_by_income_bucket.png"),
       g_income, width = 7, height = 4, dpi = 300)
print(g_income)

```

```{r}
# ──────────────────────────────────────────────────────────────
# 12 · Racial composition: time-series + variance explained
#     Metric: % non-Hispanic White alone (ACS table B03002)
# ──────────────────────────────────────────────────────────────
library(tidycensus)
library(dplyr)
library(tidyr)
library(ggplot2)
library(glue)
library(scales)

# 12-a · Pull 2022 race data (ACS 5-year, table B03002)
race22_raw <- get_acs(
  geography = "county",
  table     = "B03002",
  year      = 2022,
  survey    = "acs5",
  cache_table = TRUE
)

race22 <- race22_raw %>%
  select(GEOID, variable, estimate) %>%
  pivot_wider(names_from = variable, values_from = estimate) %>%
  transmute(
    county_ihme      = GEOID,
    total_pop        = B03002_001,
    nh_white_alone   = B03002_003,
    pct_nh_white_2022 = nh_white_alone / total_pop
  )

# 12-b · Define race buckets (share non-Hispanic White)
race_breaks <- c(-Inf, .20, .40, .60, .80, Inf)
race_labels <- c("<20%", "20–40%", "40–60%", "60–80%", "≥80%")

direction_year <- direction_year %>%   # from earlier steps
  left_join(race22, by = "county_ihme") %>%
  mutate(race_bucket = cut(pct_nh_white_2022,
                           breaks = race_breaks,
                           labels = race_labels,
                           right  = FALSE))

# 12-c · TIME-SERIES by race bucket
ts_race <- direction_year %>%
  filter(!is.na(race_bucket)) %>%
  group_by(year, race_bucket) %>%
  summarise(avg_direction = mean(direction_score, na.rm = TRUE),
            .groups = "drop")

g_race <- ggplot(ts_race,
                 aes(year, avg_direction, colour = race_bucket)) +
  geom_line(linewidth = 1) +
  labs(title = "Direction-score trend by 2022 racial composition",
       x = NULL, y = "Mean direction score",
       colour = "% non-Hispanic White") +
  theme_bw()

ggsave(here("figures", "timeseries_direction_by_race_bucket.png"),
       g_race, width = 7, height = 4, dpi = 300)
print(g_race)

# 12-d · R²: variance explained by racial composition
var_race <- res %>%                       # 1999-2022 averages
  left_join(race22, by = "county_ihme") %>%
  filter(!is.na(direction_score), !is.na(pct_nh_white_2022))

fit_race <- lm(direction_score ~ pct_nh_white_2022, data = var_race)
r2r  <- summary(fit_race)$r.squared
r2rp <- round(r2r * 100, 1)

cat("Race R²:", sprintf("%.3f", r2r),
    "→", r2rp, "% of variance in average direction_scores explained by % non-Hispanic White.\n")

message("✓ Time-series plot saved: timeseries_direction_by_race_bucket.png")
```
```{r}
# ──────────────────────────────────────────────────────────────
# 13 · % Black & % Hispanic: time-series + variance explained
# ──────────────────────────────────────────────────────────────
library(dplyr)
library(tidyr)
library(ggplot2)
library(glue)
library(scales)

# 13-a · Build race22 with additional % Black & % Hispanic
race22 <- race22_raw %>%                # pulled in step 12-a
  select(GEOID, variable, estimate) %>%
  pivot_wider(names_from = variable, values_from = estimate) %>%
  transmute(
    county_ihme        = GEOID,
    total_pop          = B03002_001,
    pct_nh_white_2022  = B03002_003 / total_pop,
    pct_nh_black_2022  = B03002_004 / total_pop,
    pct_hisp_2022      = B03002_012 / total_pop        # Hispanic of any race
  )

# -------------------------------------------------------------
# helper to build plots & R² for an arbitrary race metric
build_race_outputs <- function(var_col,
                               breaks, labels,
                               plot_fname, title_lab, legend_lab) {

  # join & bucket
  tmp_year <- direction_year %>%        # from step 9
    left_join(race22, by = "county_ihme") %>%
    mutate(bucket = cut(.data[[var_col]],
                        breaks = breaks,
                        labels = labels,
                        right  = FALSE))

  # time-series
  ts_df <- tmp_year %>%
    filter(!is.na(bucket)) %>%
    group_by(year, bucket) %>%
    summarise(avg_direction = mean(direction_score, na.rm = TRUE),
              .groups = "drop")

  g <- ggplot(ts_df,
              aes(year, avg_direction, colour = bucket)) +
    geom_line(linewidth = 1) +
    labs(title = title_lab,
         x = NULL, y = "Mean direction score",
         colour = legend_lab) +
    theme_bw()

  ggsave(here("figures", plot_fname),
         g, width = 7, height = 4, dpi = 300)
  print(g)

  # R²
  var_df <- res %>%                       # 1999-2022 averages
    left_join(race22, by = "county_ihme") %>%
    filter(!is.na(direction_score), !is.na(.data[[var_col]]))

  # build formula like "direction_score ~ pct_hisp_2022"
  fit <- lm(reformulate(var_col, response = "direction_score"), data = var_df)

  r2  <- summary(fit)$r.squared
  r2p <- round(r2 * 100, 1)

  cat(title_lab, "– R²:", sprintf("%.3f", r2),
      "→", r2p, "% variance explained.\n")

}

# 13-b · % non-Hispanic Black alone ------------------------------------------
build_race_outputs(
  var_col   = "pct_nh_black_2022",
  breaks    = c(-Inf, .10, .20, .40, .6, Inf),
  labels    = c("<10%", "10–20%", "20–40%", "40–60%", "≥60%"),
  plot_fname = "timeseries_direction_by_black_bucket.png",
  title_lab  = "Direction-score trend by 2022 % non-Hispanic Black",
  legend_lab = "% non-Hispanic Black"
)

# 13-c · % Hispanic/Latino (any race) ----------------------------------------
build_race_outputs(
  var_col   = "pct_hisp_2022",
  breaks    = c(-Inf, .10, .20, .40, .6, Inf),
  labels    = c("<10%", "10–20%", "20–40%", "40–60%", "≥60%"),
  plot_fname = "timeseries_direction_by_hisp_bucket.png",
  title_lab  = "Direction-score trend by 2022 % Hispanic",
  legend_lab = "% Hispanic"
)

message("✓ Time-series plots saved:",
        "\n  • timeseries_direction_by_black_bucket.png",
        "\n  • timeseries_direction_by_hisp_bucket.png")
```
```{r}
# ──────────────────────────────────────────────────────────────
# 0 · Load packages
# ──────────────────────────────────────────────────────────────
library(dplyr)
library(stringr)
library(ggplot2)
library(glue)
library(purrr)
library(scales)
library(here)
library(sf)
library(tigris)
library(tidyr)
library(tidycensus)

options(tigris_use_cache = TRUE, tigris_class = "sf")

# ──────────────────────────────────────────────────────────────
# 1 · Load data & construct county_ihme
# ──────────────────────────────────────────────────────────────
df <- read_csv(here("data", "county_year_quality_metrics.csv"),
               show_col_types = FALSE) %>%
  mutate(county_ihme = str_sub(county_ihme, 1, 5))

# List of columns used in direction_score calculation
quality_vars <- c("prop_light", "pct_overd_miss", "pct_acc_miss", "DQ_prop_garbage")

# ──────────────────────────────────────────────────────────────
# 2 · Compute GLOBAL mean and SD for each quality indicator
# ──────────────────────────────────────────────────────────────
stats_tbl <- df %>%
  summarise(across(all_of(quality_vars),
                   list(mu = ~ mean(.x, na.rm = TRUE),
                        sd = ~  sd(.x, na.rm = TRUE))))

mu_vec <- stats_tbl %>%
  select(ends_with("_mu")) %>%
  setNames(str_remove(names(.), "_mu")) %>%
  unlist()

sd_vec <- stats_tbl %>%
  select(ends_with("_sd")) %>%
  setNames(str_remove(names(.), "_sd")) %>%
  unlist()

# ──────────────────────────────────────────────────────────────
# 3 · Load county shapefile
# ──────────────────────────────────────────────────────────────
crs_proj <- 2163
county_sf <- counties(year = 2015, cb = TRUE, class = "sf") %>%
  st_transform(crs_proj) %>%
  mutate(county_ihme = GEOID) %>%
  select(county_ihme, geometry)

# ──────────────────────────────────────────────────────────────
# 4 · Create county-level averages across all years (res)
# ──────────────────────────────────────────────────────────────
res <- df %>%
  group_by(county_ihme) %>%
  summarise(across(all_of(quality_vars), ~ mean(.x, na.rm = TRUE)), .groups = "drop") %>%
  mutate(across(all_of(quality_vars),
                ~ (.x - mu_vec[cur_column()]) / sd_vec[cur_column()],
                .names = "z_{.col}")) %>%
  mutate(direction_score =
           (z_prop_light + z_pct_overd_miss + z_pct_acc_miss + z_DQ_prop_garbage) /4)

# ──────────────────────────────────────────────────────────────
# 5 · Define periods and mapping function using global z-scores
# ──────────────────────────────────────────────────────────────
periods <- list(
  `1999_2004` = 1999:2004,
  `2005_2010` = 2005:2010,
  `2011_2017` = 2011:2017,
  `2018_2022` = 2018:2022,
  `2020_2022` = 2020:2022
)

calc_period_scores <- function(year_vec) {
  df %>%
    filter(year %in% year_vec) %>%
    group_by(county_ihme) %>%
    summarise(across(all_of(quality_vars), ~ mean(.x, na.rm = TRUE)), .groups = "drop") %>%
    mutate(across(all_of(quality_vars),
                  ~ (.x - mu_vec[cur_column()]) / sd_vec[cur_column()],
                  .names = "z_{.col}")) %>%
    mutate(direction_score =
             (z_prop_light + z_pct_overd_miss + z_pct_acc_miss + z_DQ_prop_garbage) / 4)
}

maps <- imap(periods, function(yrs, nm) {
  per_res <- calc_period_scores(yrs)

  per_map_sf <- county_sf %>%
    left_join(per_res %>% select(county_ihme, direction_score),
              by = "county_ihme")

  p_map <- make_map(
    sf_data = per_map_sf,
    var     = "direction_score",
    title   = glue("Direction score ({nm})")
  )

  ggsave(here("figures", glue("direction_score_map_{nm}.png")),
         p_map, width = 9, height = 6, dpi = 320)

  p_map
})

# ──────────────────────────────────────────────────────────────
# 6 · Time-series data frame (county-year global z-scores)
# ──────────────────────────────────────────────────────────────
direction_year <- df %>%
  mutate(across(all_of(quality_vars),
                ~ (.x - mu_vec[cur_column()]) / sd_vec[cur_column()],
                .names = "z_{.col}")) %>%
  mutate(direction_score =
           (z_prop_light + z_pct_overd_miss + z_pct_acc_miss + z_DQ_prop_garbage) / 4) %>%
  select(county_ihme, year, direction_score)

# ──────────────────────────────────────────────────────────────
# 7 · Get 2022 ACS race shares (used later in race bucket plots)
# ──────────────────────────────────────────────────────────────
census_api_key("671b64055fc192103cbc199d2dff91b46d9cc781", overwrite = FALSE)


race22 <- get_acs(geography = "county",
                  table = "B03002",
                  year = 2022,
                  survey = "acs5",
                  cache_table = TRUE) %>%
  select(GEOID, variable, estimate) %>%
  pivot_wider(names_from = variable, values_from = estimate) %>%
  transmute(
    county_ihme       = GEOID,
    total_pop         = B03002_001,
    pct_nh_white_2022 = B03002_003 / total_pop,
    pct_nh_black_2022 = B03002_004 / total_pop,
    pct_hisp_2022     = B03002_012 / total_pop
  )

# ──────────────────────────────────────────────────────────────
# 8 · Optional: Load 2020 election results for vote-share plot
# ──────────────────────────────────────────────────────────────
e20_raw <- read_csv(here("data", "county_election_2020.csv"),
                    show_col_types = FALSE)
```
map diversity
```{r}
# ──────────────────────────────────────────────────────────────
# NEW · Map S (diversity) and I (inequality) by COUNTY CLUSTER
# ──────────────────────────────────────────────────────────────
suppressPackageStartupMessages({
  library(dplyr); library(readr); library(stringr); library(purrr)
  library(sf); library(tigris); library(ggplot2); library(glue)
})

options(tigris_use_cache = TRUE, tigris_class = "sf")
crs_proj <- 2163

# 1) Load cluster-level diversity results (S, I) ----------------
div_tbl <- read_csv(here::here("cluster_cod_diversity.csv"), show_col_types = FALSE)
stopifnot(all(c("cluster","period","deaths","S","I") %in% names(div_tbl)))

# 2) Load county→cluster membership (handles 1 file or 4 files) -
load_ccm <- function() {
  # Preferred single long file
  one <- here::here("county_cluster_membership.csv")
  if (file.exists(one)) {
    return(read_csv(one, show_col_types = FALSE) %>%
             select(fips, period, cluster))
  }
  # Otherwise look for per-period files
  wins <- c("1999_2004","2005_2010","2011_2017","2018_2022")
  paths <- here::here(paste0("county_cluster_membership_", wins, ".csv"))
  has   <- file.exists(paths)
  if (!any(has)) {
    stop("No county→cluster membership file(s) found. ",
         "Expected either county_cluster_membership.csv ",
         "or county_cluster_membership_<period>.csv")
  }
  bind_rows(map2(paths[has], wins[has], ~{
    read_csv(.x, show_col_types = FALSE) %>%
      mutate(period = .y)
  })) %>% select(fips, period, cluster)
}

ccm <- load_ccm() %>%
  mutate(fips = stringr::str_pad(as.character(fips), 5, pad = "0"))

# 3) County geometry (we’ll reuse your period→year mapping) -----
shapefile_years <- c("1999_2004" = 2000,
                     "2005_2010" = 2010,
                     "2011_2017" = 2015,
                     "2018_2022" = 2020)

st_shift <- function(x, offset) { st_geometry(x) <- st_geometry(x) + offset; x }
st_scale <- function(x, factor) {
  ctr <- sf::st_centroid(sf::st_union(x))
  sf::st_geometry(x) <- (sf::st_geometry(x) - ctr) * factor + ctr
  x
}

.pick_fips <- function(df) {
  # Look for GEOID* first
  hits <- grep("^GEOID", names(df), value = TRUE)
  if (length(hits) >= 1) {
    return(df[[hits[1]]])
  }
  # Fall back to STATEFP + COUNTYFP if present
  if (all(c("STATEFP", "COUNTYFP") %in% names(df))) {
    return(paste0(df$STATEFP, df$COUNTYFP))
  }
  stop("No GEOID or STATEFP/COUNTYFP columns found in counties() output. Names: ",
       paste(names(df), collapse = ", "))
}

build_cluster_sf <- function(period_name, shp_year) {
  counties_raw <- tigris::counties(year = shp_year, cb = TRUE, class = "sf") %>%
    sf::st_zm(drop = TRUE, what = "ZM") %>%
    sf::st_transform(crs_proj)

  counties_sf <- counties_raw %>%
    dplyr::mutate(fips = stringr::str_pad(.pick_fips(counties_raw), 5, pad = "0")) %>%
    dplyr::select(fips, geometry)

  # join membership for this period
  m <- ccm %>% dplyr::filter(period == period_name)
  if (nrow(m) == 0) stop("No county→cluster rows for period ", period_name)

  j <- counties_sf %>%
    dplyr::left_join(m, by = "fips") %>%
    dplyr::filter(!is.na(cluster))

  clusters_sf <- j %>%
    dplyr::group_by(cluster) %>%
    dplyr::summarise(geometry = sf::st_union(geometry), .groups = "drop")

  alaska  <- j %>% dplyr::filter(stringr::str_starts(fips, "02"))
  hawaii  <- j %>% dplyr::filter(stringr::str_starts(fips, "15"))

  if (nrow(alaska)) {
    ak <- sf::st_as_sf(sf::st_union(alaska), crs = crs_proj) %>%
      st_scale(0.40) %>% st_shift(c(1300000, -4900000))
    clusters_sf <- suppressWarnings(sf::st_make_valid(clusters_sf))
  }
  if (nrow(hawaii)) {
    hi <- sf::st_as_sf(sf::st_union(hawaii), crs = crs_proj) %>%
      st_scale(1.50) %>% st_shift(c(5200000, -1400000))
    clusters_sf <- suppressWarnings(sf::st_make_valid(clusters_sf))
  }

  clusters_sf
}

# if you don't want to attach rlang, avoid %||%
make_cluster_map <- function(sf_data, var, title = NULL) {
  states <- tigris::states(cb = TRUE, class = "sf") %>% sf::st_transform(crs_proj)
  vr <- range(sf_data[[var]], na.rm = TRUE)
  lims <- c(floor(min(vr)*1000)/1000, ceiling(max(vr)*1000)/1000)
  plot_title <- if (is.null(title)) var else title

  ggplot() +
    geom_sf(data = sf_data, aes(fill = .data[[var]]), colour = NA) +
    geom_sf(data = states, fill = NA, colour = "white", linewidth = 0.3) +
    scale_fill_distiller(palette = "RdBu", direction = -1,
                         limits = lims, oob = scales::squish, na.value = "grey90") +
    coord_sf(xlim = c(-2500000, 2500000),
             ylim = c(-2200000, 730000),
             expand = FALSE) +
    labs(title = plot_title, fill = NULL) +
    theme_void() +
    theme(
      plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
      legend.text = element_text(size = 9)
    )
}

# 6) Build + plot for each period (S and I) ---------------------
out_dir <- here::here("figures", "cluster_diversity_maps")
dir.create(out_dir, showWarnings = FALSE, recursive = TRUE)

imap(shapefile_years, function(yr, win) {
  message("Mapping ", win, "…")

  # cluster polygons
  cl_sf <- build_cluster_sf(win, yr)

  # attach S & I for this period
  dat <- div_tbl %>% filter(period == win) %>% select(cluster, S, I, deaths)
  sf_dat <- cl_sf %>% left_join(dat, by = "cluster")

  # S map
  pS <- make_cluster_map(sf_dat, "S", glue("Cause-of-death diversity S — {win}"))
  ggsave(filename = file.path(out_dir, glue("cluster_S_{win}.png")),
         plot = pS, width = 8, height = 6, dpi = 320)

  # I map
  pI <- make_cluster_map(sf_dat, "I", glue("CoD inequality (Rao’s Q) — {win}"))
  ggsave(filename = file.path(out_dir, glue("cluster_I_{win}.png")),
         plot = pI, width = 8, height = 6, dpi = 320)

  # also print to the notebook
  print(pS); print(pI)
})
```
maps phillips detail metric
```{r}
# ──────────────────────────────────────────────────────────────
# Map Phillips detail (cluster + county) by period — robust
# ──────────────────────────────────────────────────────────────
suppressPackageStartupMessages({
  library(dplyr); library(readr); library(stringr); library(purrr)
  library(sf); library(tigris); library(ggplot2); library(glue); library(here)
})

options(tigris_use_cache = TRUE, tigris_class = "sf")
crs_proj <- 2163

# ‣ Inputs -----------------------------------------------------
clu_det <- read_csv(here("output", "cluster_phillips_detail.csv"), show_col_types = FALSE)
stopifnot(all(c("cluster","period","detail_phillips") %in% names(clu_det)))

cty_det <- read_csv(here("output", "county_phillips_detail.csv"), show_col_types = FALSE) %>%
  mutate(fips = str_pad(as.character(fips), 5, pad = "0"))
stopifnot(all(c("fips","period","detail_phillips") %in% names(cty_det)))

ccm <- read_csv(here("output", "county_cluster_membership.csv"), show_col_types = FALSE) %>%
  mutate(fips = str_pad(as.character(fips), 5, pad = "0")) %>%
  select(fips, period, cluster)

# ‣ Period → shapefile year -----------------------------------
shapefile_years <- c("1999_2004" = 2000,
                     "2005_2010" = 2010,
                     "2011_2017" = 2015,
                     "2018_2022" = 2020)

# ‣ Helpers ----------------------------------------------------
.pick_fips <- function(df) {
  hits <- grep("^GEOID", names(df), value = TRUE)
  if (length(hits) >= 1) return(df[[hits[1]]])
  if (all(c("STATEFP","COUNTYFP") %in% names(df))) return(paste0(df$STATEFP, df$COUNTYFP))
  stop("No GEOID or STATEFP/COUNTYFP columns found in counties() output. Names: ",
       paste(names(df), collapse = ", "))
}

build_cluster_sf <- function(period_name, shp_year, ccm_df) {
  counties_raw <- tigris::counties(year = shp_year, cb = TRUE, class = "sf") %>%
    sf::st_zm(drop = TRUE, what = "ZM") %>%
    sf::st_transform(crs_proj)

  counties_sf <- counties_raw %>%
    mutate(fips = str_pad(.pick_fips(counties_raw), 5, pad = "0")) %>%
    select(fips, geometry)

  m <- ccm_df %>% filter(period == period_name)
  if (nrow(m) == 0) stop("No county→cluster rows for period ", period_name)

  j <- counties_sf %>% left_join(m, by = "fips") %>% filter(!is.na(cluster))

  clusters_sf <- j %>%
    group_by(cluster) %>%
    summarise(geometry = sf::st_union(geometry), .groups = "drop") %>%
    suppressWarnings()
  clusters_sf
}

build_county_sf <- function(period_name, shp_year, county_values_df) {
  counties_raw <- tigris::counties(year = shp_year, cb = TRUE, class = "sf") %>%
    sf::st_zm(drop = TRUE, what = "ZM") %>%
    sf::st_transform(crs_proj)

  counties_sf <- counties_raw %>%
    mutate(fips = str_pad(.pick_fips(counties_raw), 5, pad = "0")) %>%
    select(fips, geometry)

  vals <- county_values_df %>%
    filter(period == period_name) %>%
    select(fips, detail_phillips)

  counties_sf %>% left_join(vals, by = "fips")
}

make_map <- function(sf_data, var, title = NULL) {
  states <- tigris::states(cb = TRUE, class = "sf") %>% sf::st_transform(crs_proj)
  vr <- range(sf_data[[var]], na.rm = TRUE)
  # If everything is NA (rare), avoid errors:
  if (!is.finite(vr[1]) || !is.finite(vr[2])) vr <- c(0, 1)
  lims <- c(floor(min(vr)*1000)/1000, ceiling(max(vr)*1000)/1000)
  plot_title <- if (is.null(title)) var else title

  ggplot() +
    geom_sf(data = sf_data, aes(fill = .data[[var]]), colour = NA) +
    geom_sf(data = states, fill = NA, colour = "white", linewidth = 0.3) +
    scale_fill_distiller(palette = "RdBu", direction = -1,
                         limits = lims, oob = scales::squish, na.value = "grey90") +
    coord_sf(xlim = c(-2500000, 2500000),
             ylim = c(-2200000, 730000),
             expand = FALSE) +
    labs(title = plot_title, fill = NULL) +
    theme_void() +
    theme(
      plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
      legend.text = element_text(size = 9)
    )
}

# ‣ Output dirs & device --------------------------------------
dir.create(here("figures","phillips_maps","cluster"), recursive = TRUE, showWarnings = FALSE)
dir.create(here("figures","phillips_maps","county"),  recursive = TRUE, showWarnings = FALSE)

# If you have {ragg}, uncomment this for bullet‑proof PNGs while knitting:
# if (requireNamespace("ragg", quietly = TRUE)) {
#   dev_png <- ragg::agg_png
# } else {
#   dev_png <- "png"
# }
dev_png <- "png"  # keep base PNG if you don't want ragg

# ‣ Build + save (no inline print to avoid notebook device errors) -----------
imap(shapefile_years, function(yr, win) {
  message("Mapping ", win, "…")

  # CLUSTER
  cl_sf  <- build_cluster_sf(win, yr, ccm)
  dat_cl <- clu_det %>% filter(period == win) %>% select(cluster, detail_phillips)
  sf_cl  <- cl_sf %>% left_join(dat_cl, by = "cluster")

  pC <- make_map(sf_cl, "detail_phillips", glue("Phillips detail (cluster) — {win}"))
  ggsave(filename = here("figures","phillips_maps","cluster", glue("cluster_phillips_{win}.png")),
         plot = pC, width = 8, height = 6, dpi = 320, device = dev_png)

  # COUNTY (pre‑cluster)
  cty_sf <- build_county_sf(win, yr, cty_det)
  pK <- make_map(cty_sf, "detail_phillips", glue("Phillips detail (county, pre‑cluster) — {win}"))
  ggsave(filename = here("figures","phillips_maps","county", glue("county_phillips_{win}.png")),
         plot = pK, width = 8, height = 6, dpi = 320, device = dev_png)

  # If you're running interactively in RStudio and want to see them, uncomment:
  # if (interactive()) { print(pC); print(pK) }
})
```









