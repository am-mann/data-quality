---
title: "Mapping Notebook"
output: html_notebook
---

Temporality stable groupings
```{r}
# ──────────────────────────────────────────────────────────────
# 5-year average maps by temporally-stable IHME groups — NO EXTRA PROCESSING
# ──────────────────────────────────────────────────────────────

# 0) Packages & options
required <- c("sf","tigris","readr","dplyr","stringr","purrr","ggplot2","here","scales")
missing <- setdiff(required, rownames(installed.packages()))
if (length(missing)) install.packages(missing)
invisible(lapply(required, library, character.only = TRUE))
options(tigris_use_cache = TRUE, tigris_class = "sf")

`%||%` <- function(a, b) if (!is.null(a)) a else b

# 1) Variables to map (no transformations will be applied)
vars_to_map <- c(
  "prop_garbage","prop_light","pct_acc_miss","pct_overd_miss","DQ_overall",
  "bd_underreport_noacc_nofent","bd_underreport_withacc_nofent"
)

# 2) Load your county-year data (as-is); add time windows only
dq <- readr::read_csv(here::here("data","county_year_quality_metrics.csv.gz"), show_col_types = FALSE) %>%
  dplyr::mutate(
    year        = as.integer(year),
    county_ihme = stringr::str_pad(as.character(county_ihme), 5, pad = "0"),
    time_window = dplyr::case_when(
      year >= 1999 & year <= 2004 ~ "1999_2004",
      year >= 2005 & year <= 2010 ~ "2005_2010",
      year >= 2011 & year <= 2017 ~ "2011_2017",
      year >= 2018 & year <= 2022 ~ "2018_2022",
      TRUE ~ NA_character_
    )
  )

# Keep only variables that actually exist; no conversions
vars_present <- intersect(vars_to_map, names(dq))
if (!length(vars_present)) stop("None of vars_to_map exist in dq. Check column names.")
vars_missing <- setdiff(vars_to_map, vars_present)
if (length(vars_missing)) message("Skipping missing vars: ", paste(vars_missing, collapse = ", "))

# 3) 5-year averages by temporally-stable group (means of existing cols only)
dq_avg <- dq %>%
  dplyr::group_by(county_ihme, time_window) %>%
  dplyr::summarise(
    dplyr::across(dplyr::all_of(vars_present), ~ mean(.x, na.rm = TRUE)),
    n_years = dplyr::n(),
    .groups = "drop"
  )

# 4) Crosswalk for temporality-stable shapes (dissolve multi-GEOID groups)
load(here::here("data_raw","ihme_fips.rda"))  # provides ihme_fips
ihme_map <- ihme_fips %>%
  dplyr::transmute(
    GEOID       = stringr::str_pad(orig_fips, 5, pad = "0"),
    county_ihme = stringr::str_pad(ihme_fips, 5, pad = "0")
  ) %>%
  dplyr::distinct()

# 5) Build shapes per period & join averages
crs_proj <- 2163
shapefile_years <- c("1999_2004"=2000, "2005_2010"=2010, "2011_2017"=2015, "2018_2022"=2020)

st_shift <- function(x, offset) { sf::st_geometry(x) <- sf::st_geometry(x) + offset; x }
st_scale <- function(x, factor) { ctr <- sf::st_centroid(sf::st_union(x)); sf::st_geometry(x) <- (sf::st_geometry(x) - ctr) * factor + ctr; x }

# ——— Normalizer: ensure a 5‑digit GEOID exists, with robust fallbacks ———
normalize_geoid <- function(sf, year) {
  nms <- names(sf)

  # A) If any column starts with GEOID (GEOID, GEOID10, GEOID20…), use it.
  geoid_col <- grep("^GEOID", nms, value = TRUE)[1]
  if (!is.na(geoid_col)) {
    message("[", year, "] Using GEOID column: ", geoid_col)
    sf <- dplyr::rename(sf, GEOID = !!rlang::sym(geoid_col))
    sf$GEOID <- stringr::str_pad(as.character(sf$GEOID), 5, pad = "0")
    return(sf)
  }

  # B) Else build from STATEFP + COUNTYFP (present in 2000 & 2010)
  state_col  <- grep("^STATE.*FP$", nms, value = TRUE)[1]
  county_col <- grep("^COUNTY.*FP$", nms, value = TRUE)[1]
  if (!is.na(state_col) && !is.na(county_col)) {
    message("[", year, "] Constructing GEOID from ", state_col, " + ", county_col)
    sf$GEOID <- paste0(
      stringr::str_pad(as.character(sf[[state_col]]),  2, pad = "0"),
      stringr::str_pad(as.character(sf[[county_col]]), 3, pad = "0")
    )
    return(sf)
  }

  # C) Last resort: legacy combined id
  combo_col <- grep("^CNTYIDFP$", nms, value = TRUE)[1]
  if (!is.na(combo_col)) {
    message("[", year, "] Using combined ID column: ", combo_col, " as GEOID")
    sf <- dplyr::rename(sf, GEOID = !!rlang::sym(combo_col))
    sf$GEOID <- stringr::str_pad(as.character(sf$GEOID), 5, pad = "0")
    return(sf)
  }

  stop("No GEOID-compatible columns found in shapefile for year ", year,
       ". Columns were: ", paste(nms, collapse = ", "))
}

# ——— Build groups: normalize → transform → dissolve by county_ihme ———
build_groups_sf <- function(year) {
  counties_raw <- tigris::counties(year = year, cb = TRUE, class = "sf") %>%
    sf::st_zm(drop = TRUE, what = "ZM")

  counties_norm <- normalize_geoid(counties_raw, year) %>%  # ← normalize FIRST
    sf::st_transform(crs_proj)

  groups_sf <- counties_norm %>%
    dplyr::left_join(ihme_map, by = "GEOID") %>%
    dplyr::mutate(county_ihme = dplyr::coalesce(county_ihme, GEOID)) %>%
    dplyr::group_by(county_ihme) %>%
    dplyr::summarise(.groups = "drop")  # st_union here

  # AK / HI layout (unchanged)
  alaska   <- groups_sf %>% dplyr::filter(substr(county_ihme,1,2)=="02") %>%
    { ctr <- sf::st_centroid(sf::st_union(.)); sf::st_geometry(.) <- (sf::st_geometry(.) - ctr)*0.40 + ctr; . } %>%
    { sf::st_geometry(.) <- sf::st_geometry(.) + c(1300000, -4900000); . } %>%
    sf::st_set_crs(crs_proj)
  hawaii   <- groups_sf %>% dplyr::filter(substr(county_ihme,1,2)=="15") %>%
    { ctr <- sf::st_centroid(sf::st_union(.)); sf::st_geometry(.) <- (sf::st_geometry(.) - ctr)*1.50 + ctr; . } %>%
    { sf::st_geometry(.) <- sf::st_geometry(.) + c(5200000, -1400000); . } %>%
    sf::st_set_crs(crs_proj)
  mainland <- groups_sf %>% dplyr::filter(!substr(county_ihme,1,2) %in% c("02","15"))

  dplyr::bind_rows(mainland, alaska, hawaii)
}


joined_by_period <- purrr::imap(
  shapefile_years,
  function(year, window) {
    shape_all <- build_groups_sf(year)
    dplyr::left_join(shape_all, dplyr::filter(dq_avg, time_window == window), by = "county_ihme")
  }
)

# 6) Your original make_map (unchanged)
make_map <- function(sf_data, var, title = NULL, palette = "RdBu") {
  states <- tigris::states(cb = TRUE, class = "sf") |> st_transform(2163)

  fill_scale <- if (var == "prop_garbage") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0.15, 0.4),
    oob = scales::squish,
    direction = -1,
    na.value = "grey90"
  )
} else if (var == "prop_light") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0.20, 0.40),
    oob = scales::squish,
    direction = -1,
    na.value = "grey90"
  )
} else if (var == "pct_overd_miss") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0, 0.75),
    oob = scales::squish,
    direction = -1,
    na.value = "grey90"
  )
} else if (var == "direction_score") {
  scale_fill_distiller(
    palette = palette,
    limits = c(-1,1.5),
    oob = scales::squish,
    direction = -1,
    na.value = "grey90"
  )
} else if (var == "DQ_overall") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0,0.40),
    oob = scales::squish,
    direction = 1,
    na.value = "grey90"
  )
} else if (var == "pct_acc_miss") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0, 0.75),
    oob = scales::squish,
    direction = -1,
    na.value = "grey90"
  )
} else if (var == "pct_mandeath_comp_k") {
  scale_fill_distiller(
    palette = palette,
    limits = c(5, 20),
    oob = scales::squish,
    direction = 1,
    na.value = "grey90"
  )
} else if (var == "pct_placdth_comp_k") {
  scale_fill_distiller(
    palette = palette,
    limits = c(98, 100),
    oob = scales::squish,
    direction = 1,
    na.value = "grey90"
  )
} else if (var == "pct_educ_comp_k") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0, 100),
    oob = scales::squish,
    direction = 1,
    na.value = "grey90"
  )
} else if (var == "pct_marstat_comp_k") {
  scale_fill_distiller(
    palette = palette,
    limits = c(97, 100),
    oob = scales::squish,
    direction = 1,
    na.value = "grey90"
  )
} else if (var %in% c("pct_age_comp_k", "pct_sex_comp_k", "pct_race_comp_k")) {
  scale_fill_distiller(
    palette = palette,
    limits = c(99.5, 100),
    oob = scales::squish,
    direction = 1,
    na.value = "grey90"
  )
} else if (var == "pct_gc_I64") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0.01, 0.05),
    oob = scales::squish,
    direction = -1,
    na.value = "grey90"
  )
} else if (var == "pct_gc_C_misc") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0.005, 0.03),
    oob = scales::squish,
    direction = -1,
    na.value = "grey90"
  )
} else if (var == "pct_gc_I10") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0, 0.02),
    oob = scales::squish,
    direction = -1,
    na.value = "grey90"
  )
} else if (var == "pct_gc_R_misc") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0, 0.03),
    oob = scales::squish,
    direction = -1,
    na.value = "grey90"
  )
} else if (var == "pct_gc_N19") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0, 0.025),
    oob = scales::squish,
    direction = -1,
    na.value = "grey90"
  )
} else if (var == "pct_gc_J80") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0, 0.005),
    oob = scales::squish,
    direction = -1,
    na.value = "grey90"
  )
} else if (startsWith(var, "pct_") || var == "overall_completeness_pct") {
  scale_fill_distiller(
    palette = palette,
    limits = c(0, 100),
    oob = scales::squish,
    direction = 1,
    na.value = "grey90"
  )
} else {
  scale_fill_distiller(
    palette = palette,
    oob = scales::squish,
    na.value = "grey90",
    direction = -1
  )
}

  ggplot() +
    geom_sf(data = sf_data, aes(fill = .data[[var]]), colour = NA) +
    geom_sf(data = states, fill = NA, colour = "white", linewidth = 0.3) +
    fill_scale +
    coord_sf(xlim = c(-2500000, 2500000),
             ylim = c(-2200000, 730000),
             expand = FALSE) +
    labs(title = title %||% var, fill = NULL) +
    theme_void() +
    theme(
      plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
      legend.text = element_text(size = 9)
    )
}

# 7) Save maps
output_dir <- here::here("figures","5yr_avg_stable")
dir.create(output_dir, showWarnings = FALSE, recursive = TRUE)

purrr::walk(vars_present, function(var) {
  purrr::walk(names(shapefile_years), function(win) {
    sf_dat  <- joined_by_period[[win]]
    if (!var %in% names(sf_dat)) return(invisible(NULL))
    map_plot <- make_map(sf_dat, var, paste(var, win))
    ggsave(filename = file.path(output_dir, paste0(var, "_", win, ".png")),
           plot = map_plot, width = 8, height = 6, dpi = 320)
  })
})

message("✓ Done. Maps saved to: ", output_dir)
```
Relationship between rural and garbage
```{r}
# 0 · Load packages -------------------------------------------------------
library(dplyr)
library(stringr)
library(ggplot2)
library(scales)
library(tigris)
library(tidycensus)
library(readr)
library(here)

# 1 · Load your data ------------------------------------------------------
dq <- read_csv(here("data", "county_year_quality_metrics.csv.gz"),
               show_col_types = FALSE)

# 3 · Convert county codes to GEOID ---------------------------------------
dq_all <- dq %>%
  mutate(
    GEOID = county_ihme
  )

cat("Counties without valid GEOID:", sum(is.na(dq_all$GEOID)), "\n")

# 4 · Get 2022 population estimates ---------------------------------------
census_api_key("671b64055fc192103cbc199d2dff91b46d9cc781", overwrite = FALSE)

pop22 <- get_acs(
  geography = "county",
  variables = "B01003_001",
  year = 2022,
  survey = "acs5"
) %>%
  select(GEOID, pop_2022 = estimate)

# 5 · Define county size buckets ------------------------------------------
cuts   <- c(0, 50000, 100000, 250000, 1000000, Inf)
labels <- c("<50k", "50–100k", "100–250k", "250k–1M", "≥1M")

county_sizes <- pop22 %>%
  mutate(size_bucket = cut(pop_2022, breaks = cuts, labels = labels, right = FALSE)) %>%
  select(GEOID, pop_2022, size_bucket)

# 6 · Join population and size to main data -------------------------------
dq_all <- dq_all %>%
  left_join(pop22, by = "GEOID") %>%
  left_join(county_sizes %>% select(GEOID, size_bucket), by = "GEOID")

# 7 · Indicators to plot --------------------------------------------------
indicators <- c(
  "prop_garbage",
  "prop_light",
  "pct_overd_miss",
  "pct_acc_miss")

# 8 · Time trends by size bucket ------------------------------------------
for (var in indicators) {
  ts_df <- dq_all %>%
    filter(!is.na(size_bucket)) %>%
    group_by(year, size_bucket) %>%
    summarise(avg_prop = mean(.data[[var]], na.rm = TRUE), .groups = "drop")
  
  g <- ggplot(ts_df, aes(year, avg_prop, colour = size_bucket)) +
    geom_line(linewidth = 1) +
    scale_y_continuous(labels = percent_format(accuracy = 0.1)) +
    labs(
      title = paste("Trend of", gsub("_", " ", var), "by county size"),
      x = NULL,
      y = "Mean percentage",
      colour = "2022 size bucket"
    ) +
    theme_bw()
  
  print(g)
  
  ggsave(
    filename = here("figures", paste0("trend_", var, "_by_size_bucket.png")),
    plot = g,
    width = 7, height = 4, dpi = 300
  )
}

# 9 · Scatterplots: pop vs indicator (2022) -------------------------------
scatter_data <- dq_all %>%
  filter(year == 2022, !is.na(pop_2022))

for (var in indicators) {
  scatter_df <- scatter_data %>%
    filter(!is.na(.data[[var]]))
  
  g <- ggplot(scatter_df, aes(x = pop_2022, y = .data[[var]])) +
    geom_point(alpha = 0.6) +
    scale_x_log10(labels = comma) +
    scale_y_continuous(labels = percent_format(accuracy = 0.1)) +
    labs(
      title = paste("County population vs", gsub("_", " ", var), "(2022)"),
      x = "County population (log10 scale)",
      y = paste("Proportion:", gsub("_", " ", var))
    ) +
    theme_bw()
  
  print(g)
  
  ggsave(
    filename = here("figures", paste0("scatter_", var, "_vs_population_2022.png")),
    plot = g,
    width = 7, height = 4, dpi = 300
  )
}
```
Clustering or anomaly detection: Uses unsupervised ML (clustering, autoencoders) to identify unusual records or patterns. If a county’s records are more often flagged as outliers, that suggests quality issues.
```{r}
# 0.  PACKAGES ------------------------------------------------------------------
library(tidyverse)
library(solitude)
library(dbscan)
library(factoextra)
library(patchwork)
library(here)

# 1.  LOAD & CLEAN DATA --------------------------------------------------------
df <- read_csv(
  here("data", "county_year_quality_metrics.csv.gz"),
  show_col_types = FALSE
) %>%
  mutate(
    year = as.integer(year)
  )

# 2.  COMPUTE prop_all_comp & DEFINE VARIABLES --------------------------------
df <- df %>%
  mutate(
    prop_all_comp = (
      marstat_comp_k + placdth_comp_k + educ_comp_k +
      age_comp_k     + sex_comp_k      + mandeath_comp_k
    ) / (6 * n_cert)
  )

quality_vars <- c(
  "DQ_prop_garbage", "prop_light", "pct_overd_miss", "pct_acc_miss"
)

# 3.  AVERAGE OVER YEARS BY COUNTY_ihme ----------------------------------------
county_avg <- df %>%
  filter(year >= 1999, year <= 2022) %>%
  group_by(county_ihme) %>%
  summarise(
    across(all_of(quality_vars), ~ mean(.x, na.rm = TRUE)),
    n_cert = mean(n_cert, na.rm = TRUE),
    .groups = "drop"
  )

# ------------------------------------------------------------------
# 4 · SCALE, FIT MODELS & ADD SCORES   (corrected)
# ------------------------------------------------------------------

## 4.1  Impute medians for NAs -------------------------------------
mat <- county_avg %>%
  mutate(across(all_of(quality_vars), 
                ~ ifelse(is.na(.x), median(.x, na.rm = TRUE), .x)))

## 4.2  Drop any rows missing county_ihme --------------------------
if (any(is.na(mat$county_ihme))) {
  dropped <- mat %>% filter(is.na(county_ihme)) %>% nrow()
  message("Dropping ", dropped, " rows with missing county_ihme.")
  mat <- mat %>% filter(!is.na(county_ihme))
}

## 4.3  Build scaled predictor matrix ------------------------------
X_df <- mat %>%
  select(all_of(quality_vars)) %>%
  scale() %>%
  as.data.frame()
row.names(X_df) <- mat$county_ihme

# 4.4  Isolation Forest (solitude) ------------------------------
iso_mod <- solitude::isolationForest$new(num_trees = 100)
iso_mod$fit(X_df)
iso_scores <- iso_mod$predict(X_df)$anomaly_score

# 4.5  Local Outlier Factor (dbscan) -----------------------------
lof_scores <- dbscan::lof(as.matrix(X_df),
                          minPts = round(sqrt(nrow(X_df))))

# 4.6  k-means clustering (k = 3) --------------------------------
set.seed(123)
km <- kmeans(X_df, centers = 3, nstart = 25)

# 4.7  Bind results back to mat ----------------------------------
mat <- mat %>%
  mutate(
    iso_score  = iso_scores,
    lof_score  = lof_scores,
    km_cluster = km$cluster,
    iso_rank   = rank(-iso_score, ties.method = "first"),
    lof_rank   = rank(-lof_score,  ties.method = "first"),
    anomaly_flag = iso_rank <= ceiling(0.05 * n()) |
                   lof_rank <= ceiling(0.05 * n())
  )

# store final results
res <- mat

# 5.  PCA FOR PLOTTING --------------------------------------------------------
pca <- prcomp(X_df, center = TRUE, scale. = FALSE)
pca2 <- as_tibble(pca$x[,1:2]) %>%
  set_names(c("PC1", "PC2")) %>%
  mutate(
    county_ihme  = row.names(pca$x),
    iso_score    = res$iso_score,
    km_cluster   = factor(res$km_cluster),
    anomaly_flag = res$anomaly_flag
  )

# 6.  SAVE OUTPUTS ------------------------------------------------------------
write_csv(res,
          here("output", "county_outlier_scores_1999_2022_avg.csv"))

p1 <- ggplot(pca2, aes(PC1, PC2, colour = km_cluster)) +
  geom_point(alpha = 0.7, size = 3) +
  labs(title = "K-means clusters (k = 3)") +
  theme_minimal()

p2 <- ggplot(pca2, aes(PC1, PC2, size = iso_score)) +
  geom_point(colour = "red", alpha = 0.5) +
  labs(title = "Isolation Forest anomaly score") +
  theme_minimal()

(p1 + p2) %>%
  ggsave(
    filename = here("figures", "outlier_plot_1999_2022_avg.png"),
    width    = 10, height = 5, dpi = 300
  )

# 7.  PRINT TOP OUTLIERS & COMPUTE DIRECTION_SCORE ---------------------------
message("Top 15 counties by Isolation Forest score:\n")
print(res %>% arrange(desc(iso_score)) %>% slice_head(n = 15))

message("Top 15 counties by LOF score:\n")
print(res %>% arrange(desc(lof_score)) %>% slice_head(n = 15))

res <- res %>%
  mutate(
    across(all_of(quality_vars),
           ~ as.numeric(scale(.x)),
           .names = "z_{.col}"),
    direction_score = (
      z_prop_light     +
      z_pct_overd_miss +
      z_pct_acc_miss   +
      z_DQ_prop_garbage
    ) / 4
  )

message("Script complete.")
```
map direction scores
```{r}

# map direction scores
# ──────────────────────────────────────────────────────────────
# 0 · Packages (load after your existing libs)
# ──────────────────────────────────────────────────────────────
library(sf)
library(tigris)      # for county & state shapes
library(ggplot2)
library(dplyr)
library(stringr)

options(tigris_use_cache = TRUE, tigris_class = "sf")
crs_proj <- 2163     # CONUS Albers (EPSG:2163)

# ──────────────────────────────────────────────────────────────
# 1 · Pull direction_score & county_ihme from your results
# ──────────────────────────────────────────────────────────────
dir_scores <- res %>%       
  select(county_ihme, direction_score)

# ──────────────────────────────────────────────────────────────
# 2 · Build county geometry collapsed to county_ihme
# ──────────────────────────────────────────────────────────────
# 2015 TIGER/CB shapefile is a good compromise between detail & performance
county_sf <- counties(cb = TRUE, year = 2015, class = "sf") %>%
  st_transform(crs_proj) %>%
  mutate(fips = GEOID,
         fips = str_pad(fips, 5, pad = "0")) %>%
  # Attach IHME mapping and collapse to temporally-stable polygons
  left_join(ihme_xwalk, by = c("fips" = "fips")) %>%
  mutate(county_ihme = coalesce(county_ihme, fips)) %>%
  select(county_ihme, geometry) %>%
  group_by(county_ihme) %>%
  summarise(geometry = st_union(geometry), .groups = "drop")  # dissolve splits


# ──────────────────────────────────────────────────────────────
# 3 · Join scores ➜ geometry
# ──────────────────────────────────────────────────────────────
map_sf <- left_join(county_sf, dir_scores, by = "county_ihme")

# ──────────────────────────────────────────────────────────────
# 4 · Build colour scale limits (symmetric diverging)
# ──────────────────────────────────────────────────────────────
max_abs <- max(abs(map_sf$direction_score), na.rm = TRUE)

# ─────────────────────────────────────────────────────────
# 0 · Prepare geometry for every county_ihme
# ─────────────────────────────────────────────────────────
library(sf)
library(tigris)
options(tigris_use_cache = TRUE, tigris_class = "sf")

crs_proj <- 2163

# county_sf: one polygon per IHME county_ihme
county_sf <- counties(year = 2015, cb = TRUE, class = "sf") |>
  st_transform(crs_proj) |>
  mutate(fips = GEOID) |>
  left_join(ihme_xwalk, by = c("fips" = "fips")) |>
  mutate(county_ihme = dplyr::coalesce(county_ihme, fips)) |>
  select(county_ihme, geometry) |>
  group_by(county_ihme) |>
  summarise(geometry = st_union(geometry), .groups = "drop")

# ─────────────────────────────────────────────────────────
# 1 · Join your results (res) to geometry
#    res must contain 'county_ihme' and the variable you want to map
# ─────────────────────────────────────────────────────────
map_sf <- county_sf |>
  left_join(res, by = "county_ihme")      # res is your data-frame

# ─────────────────────────────────────────────────────────
# 2 · Call make_map()
#    Example: visualise the direction_score column
# ─────────────────────────────────────────────────────────
map_plot <- make_map(
  sf_data = map_sf,
  var     = "direction_score",            # any column present in map_sf
  title   = "Average Z-score with direction (1999–2022)"
)

# Show in RStudio viewer
print(map_plot)

# Save to file (optional)
ggsave(here("figures", "direction_score_map.png"), map_plot,
       width = 9, height = 6, dpi = 320)
```
Direction scores by 5 year period with global mean and standard deviation
```{r}
# ──────────────────────────────────────────────────────────────
# 0 · Globals  (μ and σ from the overall 1999–2022 county means)
# ──────────────────────────────────────────────────────────────
quality_vars <- c("DQ_prop_garbage", "prop_light", "pct_overd_miss", "pct_acc_miss")

# If `mat` from the earlier chunk is in memory, use that; otherwise rebuild quickly
if (!exists("mat")) {
  mat <- df %>%                            # `df` was read in the anomaly-detection chunk
    filter(year >= 1999, year <= 2022) %>%
    mutate(prop_all_comp = (marstat_comp_k + placdth_comp_k +
                             age_comp_k + sex_comp_k ) /
                           (4 * n_cert)) %>%
    group_by(county_ihme) %>%
    summarise(across(all_of(quality_vars), ~ mean(.x, na.rm = TRUE)),
              .groups = "drop")
}

global_means <- sapply(mat[quality_vars], mean, na.rm = TRUE)
global_sds   <- sapply(mat[quality_vars], sd,   na.rm = TRUE)

# ──────────────────────────────────────────────────────────────
# 1 · Period definitions
# ──────────────────────────────────────────────────────────────
periods <- list(
  `1999_2004` = 1999:2004,
  `2005_2010` = 2005:2010,
  `2011_2017` = 2011:2017,
  `2018_2022` = 2018:2022,
  `2020_2022` = 2020:2022
)

# ──────────────────────────────────────────────────────────────
# 2 · Geometry (one polygon per IHME county_ihme)
# ──────────────────────────────────────────────────────────────
if (!exists("county_sf")) {
  county_sf <- counties(cb = TRUE, year = 2015, class = "sf") %>%
    st_transform(2163) %>%
    mutate(fips = GEOID) %>%
    left_join(ihme_xwalk, by = "fips") %>%
    mutate(county_ihme = coalesce(county_ihme, fips)) %>%
    select(county_ihme, geometry) %>%
    group_by(county_ihme) %>%
    summarise(geometry = st_union(geometry), .groups = "drop")
}

# ──────────────────────────────────────────────────────────────
# 3 · Loop over periods: compute & map direction_score
# ──────────────────────────────────────────────────────────────
out_dir <- here::here("figures", "direction_score_periods")
dir.create(out_dir, showWarnings = FALSE, recursive = TRUE)

for (pname in names(periods)) {
  yrs <- periods[[pname]]

  # county means within the period
  peri_df <- df %>%
    filter(year %in% yrs) %>%
    mutate(prop_all_comp = (marstat_comp_k + placdth_comp_k + educ_comp_k +
                             age_comp_k + sex_comp_k + mandeath_comp_k) /
                           (6 * n_cert)) %>%
    group_by(county_ihme) %>%
    summarise(across(all_of(quality_vars), ~ mean(.x, na.rm = TRUE)),
              .groups = "drop")

  # fixed-reference Z-scores
  peri_df <- peri_df %>%
    mutate(across(all_of(quality_vars),
                  \(x, nm=cur_column()) (x - global_means[nm]) / global_sds[nm],
                  .names = "z_{.col}")) %>%
    mutate(
      direction_score = (z_prop_light + z_pct_overd_miss +
                         z_pct_acc_miss + z_DQ_prop_garbage) / 4
    )
  
    # county means within the period  ──────────────────────────────
  peri_df <- df %>%
    filter(year %in% yrs) %>%
    mutate(prop_all_comp = (marstat_comp_k + placdth_comp_k + educ_comp_k +
                            age_comp_k + sex_comp_k + mandeath_comp_k) /
                           (6 * n_cert)) %>%
    group_by(county_ihme) %>%
    summarise(across(all_of(quality_vars), ~ mean(.x, na.rm = TRUE)),
              .groups = "drop")

  ## NEW 1: guarantee every county_ihme appears (even if all NA)
  peri_df <- county_sf %>%                        # drop geometry, keep IDs
    st_drop_geometry() %>%
    select(county_ihme) %>%
    left_join(peri_df, by = "county_ihme")

  ## NEW 2: impute missing period means with the *same* values
  ##        we used for the overall map (global_means)
  peri_df <- peri_df %>%
    mutate(across(all_of(quality_vars),
                  \(x, nm = cur_column()) replace_na(x, global_means[nm])))

  # fixed-reference Z-scores  ───────────────────────────────────
  peri_df <- peri_df %>%
    mutate(across(all_of(quality_vars),
                  \(x, nm = cur_column()) (x - global_means[nm]) / global_sds[nm],
                  .names = "z_{.col}")) %>%
    ## NEW 3: compute score with all four components now present
    mutate(direction_score = (z_prop_light + z_pct_overd_miss +
                              z_pct_acc_miss  + z_DQ_prop_garbage) / 4)


  # join geometry ➜ map
  map_sf <- county_sf %>% left_join(peri_df, by = "county_ihme")

  p <- make_map(map_sf,
                var   = "direction_score",
                title = glue::glue("Direction score {pname} (scaled to 1999–2022 μ/σ)"))

  ggsave(file.path(out_dir, glue::glue("direction_score_{pname}.png")),
         plot = p, width = 9, height = 6, dpi = 320)
}
```

```{r}
# Assign each county its 2022 size bucket (once per county)
size_r2 <- direction_year %>%
  filter(year >= 1999, year <= 2022) %>%
  group_by(county_ihme) %>%
  summarise(
    mean_direction = mean(direction_score, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  left_join(pop22, by = "county_ihme") %>%
  mutate(size_bucket = cut(pop_2022, breaks = size_breaks, labels = size_labels, right = FALSE))

# Fit one-way ANOVA
fit_size <- lm(mean_direction ~ size_bucket, data = size_r2)
r2_size <- summary(fit_size)$r.squared
r2p_size <- round(r2_size * 100, 1)

cat("County size R²:", sprintf("%.3f", r2_size),
    "→", r2p_size, "% of variance in average direction_scores explained by 2022 county size bucket.\n")

income_r2 <- direction_year %>%
  filter(year >= 1999, year <= 2022) %>%
  group_by(county_ihme) %>%
  summarise(
    mean_direction = mean(direction_score, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  left_join(income22, by = "county_ihme") %>%
  mutate(income_bucket = cut(medhhinc_2022, breaks = income_breaks, labels = income_labels, right = FALSE))

fit_income <- lm(mean_direction ~ income_bucket, data = income_r2)
r2_income <- summary(fit_income)$r.squared
r2p_income <- round(r2_income * 100, 1)

cat("Income R²:", sprintf("%.3f", r2_income),
    "→", r2p_income, "% of variance in average direction_scores explained by 2022 income bucket.\n")

```


```{r}
# ──────────────────────────────────────────────────────────────
# 11 · Education: time-series + variance explained
# ──────────────────────────────────────────────────────────────
library(tidycensus)
library(dplyr)
library(ggplot2)
library(glue)
library(scales)

# 1. Get global means and sds for each variable across all years
global_stats <- df %>%
  summarise(across(
    c(DQ_prop_garbage, prop_light, pct_overd_miss, pct_acc_miss),
    list(mean = ~mean(.x, na.rm = TRUE),
         sd   = ~sd(.x, na.rm = TRUE)),
    .names = "{.col}_{.fn}"
  ))

# 2. Define a function to compute global z-score
zscore_global <- function(x, m, s) (x - m) / s

# 3. Calculate direction_score for every county-year using global stats
direction_year <- df %>%
  filter(year >= 1999, year <= 2022) %>%
  mutate(
    z_DQ_prop_garbage  = zscore_global(DQ_prop_garbage, global_stats$DQ_prop_garbage_mean, global_stats$DQ_prop_garbage_sd),
    z_prop_light       = zscore_global(prop_light,      global_stats$prop_light_mean,      global_stats$prop_light_sd),
    z_pct_overd_miss   = zscore_global(pct_overd_miss,  global_stats$pct_overd_miss_mean,  global_stats$pct_overd_miss_sd),
    z_pct_acc_miss     = zscore_global(pct_acc_miss,    global_stats$pct_acc_miss_mean,    global_stats$pct_acc_miss_sd),
    direction_score    = (z_prop_light + z_pct_overd_miss + z_pct_acc_miss + z_DQ_prop_garbage) / 4
  ) %>%
  select(county_ihme, year, direction_score) %>%
  filter(!is.na(county_ihme), !is.na(direction_score))


# 11-a · 2022 % bachelor’s+ from ACS table B15003
edu22_raw <- get_acs(
  geography = "county",
  table     = "B15003",
  year      = 2022,
  survey    = "acs5",
  cache_table = TRUE
)

# Keep total (001) and bachelor+ (022-025) → compute share
edu22 <- edu22_raw %>%
  select(GEOID, variable, estimate) %>%
  pivot_wider(names_from = variable, values_from = estimate) %>%
  transmute(
    county_ihme        = GEOID,
    total_25_plus      = B15003_001,
    bach_plus          = B15003_022 + B15003_023 + B15003_024 + B15003_025,
    pct_bachplus_2022  = bach_plus / total_25_plus
  )

# 11-b · Build education buckets (share bachelor’s+)
edu_breaks  <- c(-Inf, .20, .30, .40, .50, Inf)
edu_labels  <- c("<20%", "20–30%", "30–40%", "40–50%", "≥50%")

direction_year <- direction_year %>%
  left_join(edu22, by = "county_ihme") %>%
  mutate(edu_bucket = cut(pct_bachplus_2022,
                          breaks = edu_breaks,
                          labels = edu_labels,
                          right  = FALSE))

# Join education data to annual direction scores
direction_year <- direction_year %>%
  left_join(edu22, by = "county_ihme")


# Proceed with analysis and plotting
ts_edu <- direction_year %>%
  filter(!is.na(edu_bucket)) %>%
  group_by(year, edu_bucket) %>%
  summarise(avg_direction = mean(direction_score, na.rm = TRUE),
            .groups = "drop")

g_edu <- ggplot(ts_edu,
                aes(year, avg_direction, colour = edu_bucket)) +
  geom_line(linewidth = 1) +
  labs(title = "Direction-score trend by 2022 education bucket",
       x = NULL, y = "Mean direction score",
       colour = "% bachelor’s or higher") +
  theme_bw()

ggsave(here("figures", "timeseries_direction_by_edu_bucket.png"),
       g_edu, width = 7, height = 4, dpi = 300)
print(g_edu)

res <- res %>%
  mutate(
    z_DQ_prop_garbage  = as.numeric(scale(DQ_prop_garbage)),
    z_prop_light       = as.numeric(scale(prop_light)),
    z_pct_overd_miss   = as.numeric(scale(pct_overd_miss)),
    z_pct_acc_miss     = as.numeric(scale(pct_acc_miss)),
    direction_score    = (z_prop_light + z_pct_overd_miss + z_pct_acc_miss + z_DQ_prop_garbage) / 4
  )

# 11-d · R²: variance explained by education
var_edu <- res %>%                       # 1999-2022 averages
  left_join(edu22, by = "county_ihme") %>%
  filter(!is.na(direction_score), !is.na(pct_bachplus_2022))

fit_edu <- lm(direction_score ~ pct_bachplus_2022, data = var_edu)
r2e  <- summary(fit_edu)$r.squared
r2ep <- round(r2e * 100, 1)

cat("Education R²:", sprintf("%.3f", r2e),
    "→", r2ep, "% of variance in average direction_scores explained by pct bachelor’s+.\n")

message("✓ Time-series plot saved: timeseries_direction_by_edu_bucket.png")
```

```{r}
# ---- County population size bucket (using 2022) ----
library(tidycensus)

# Get 2022 population for each county
pop22_raw <- get_acs(
  geography = "county",
  variables = "B01003_001",
  year = 2022,
  survey = "acs5",
  cache_table = TRUE
)

pop22 <- pop22_raw %>%
  transmute(
    county_ihme = GEOID,
    pop_2022 = estimate
  )

# Create population size buckets
size_breaks <- c(-Inf, 50000, 100000, 250000, 1e6, Inf)
size_labels <- c("<50k", "50–100k", "100–250k", "250k–1M", "≥1M")

direction_year_size <- direction_year %>%
  left_join(pop22, by = "county_ihme") %>%
  mutate(
    size_bucket = cut(pop_2022, breaks = size_breaks, labels = size_labels, right = FALSE)
  )

ts_size <- direction_year_size %>%
  filter(!is.na(size_bucket)) %>%
  group_by(year, size_bucket) %>%
  summarise(avg_direction = mean(direction_score, na.rm = TRUE), .groups = "drop")

g_size <- ggplot(ts_size, aes(year, avg_direction, colour = size_bucket)) +
  geom_line(linewidth = 1) +
  labs(
    title = "Direction-score trend by 2022 county size bucket",
    x = NULL, y = "Mean direction score",
    colour = "2022 size bucket"
  ) +
  theme_bw()

ggsave(here("figures", "timeseries_direction_by_size_bucket.png"),
       g_size, width = 7, height = 4, dpi = 300)
print(g_size)

# ---- County household income bucket (using 2022) ----

# Get 2022 median household income (ACS table B19013)
income22_raw <- get_acs(
  geography = "county",
  variables = "B19013_001",
  year = 2022,
  survey = "acs5",
  cache_table = TRUE
)

income22 <- income22_raw %>%
  transmute(
    county_ihme = GEOID,
    medhhinc_2022 = estimate
  )

# Create household income buckets (edit cutpoints as desired)
income_breaks <- c(-Inf, 45000, 55000, 65000, 75000, Inf)
income_labels <- c("<$45k", "$45–55k", "$55–65k", "$65–75k", "$75k+")

direction_year_income <- direction_year %>%
  left_join(income22, by = "county_ihme") %>%
  mutate(
    income_bucket = cut(medhhinc_2022, breaks = income_breaks, labels = income_labels, right = FALSE)
  )

ts_income <- direction_year_income %>%
  filter(!is.na(income_bucket)) %>%
  group_by(year, income_bucket) %>%
  summarise(avg_direction = mean(direction_score, na.rm = TRUE), .groups = "drop")

g_income <- ggplot(ts_income, aes(year, avg_direction, colour = income_bucket)) +
  geom_line(linewidth = 1) +
  labs(
    title = "Direction-score trend by 2022 income bucket",
    x = NULL, y = "Mean direction score",
    colour = "Household income"
  ) +
  theme_bw()

ggsave(here("figures", "timeseries_direction_by_income_bucket.png"),
       g_income, width = 7, height = 4, dpi = 300)
print(g_income)

```
map diversity
```{r}
# ──────────────────────────────────────────────────────────────
# Map cluster-level diversity (S, Rao's Q) — with NA fill-in
# ──────────────────────────────────────────────────────────────
suppressPackageStartupMessages({
  library(dplyr); library(readr); library(stringr); library(purrr)
  library(sf); library(tigris); library(ggplot2); library(glue); library(here)
})

options(tigris_use_cache = TRUE, tigris_class = "sf")
crs_proj <- 2163

# 0) Inputs -----------------------------------------------------
div_tbl <- read_csv(here("output", "cluster_cod_diversity.csv"), show_col_types = FALSE)
stopifnot(all(c("cluster","period","deaths","S","I") %in% names(div_tbl)))

ccm <- read_csv(here("output", "county_cluster_membership_all_periods.csv"),
                show_col_types = FALSE) %>%
  mutate(fips = stringr::str_pad(as.character(fips), 5, pad = "0")) %>%
  select(fips, period, cluster)

# match your analysis/mapping vintages
shapefile_years <- c("1999_2004" = 2000,
                     "2005_2010" = 2010,
                     "2011_2017" = 2015,
                     "2018_2022" = 2020)

# 1) Helpers ----------------------------------------------------
.pick_fips <- function(df) {
  hits <- grep("^GEOID", names(df), value = TRUE)
  if (length(hits) >= 1) return(df[[hits[1]]])
  if (all(c("STATEFP","COUNTYFP") %in% names(df))) return(paste0(df$STATEFP, df$COUNTYFP))
  stop("No GEOID or STATEFP/COUNTYFP in counties() output: ", paste(names(df), collapse = ", "))
}

build_graph_for_year <- function(year) {
  sf_raw <- tigris::counties(year = year, cb = TRUE, class = "sf") |>
    sf::st_zm(drop = TRUE, what = "ZM") |>
    sf::st_transform(crs_proj)

  # compute FIPS outside mutate (no '.' pronoun issues)
  fips_vec <- stringr::str_pad(.pick_fips(sf_raw), 5, pad = "0")

  sf <- sf_raw |>
    dplyr::mutate(fips = fips_vec) |>
    dplyr::select(fips, geometry)

  adj <- sf::st_touches(sf)
  edges <- tibble::tibble(
    from = rep(sf$fips, lengths(adj)),
    to   = sf$fips[unlist(adj)]
  ) |>
    dplyr::filter(from < to)

  g <- igraph::graph_from_data_frame(edges, directed = FALSE, vertices = sf$fips)
  list(sf = sf, g = g)
}

# Fill missing cluster labels by assigning each unlabeled county
# to the modal cluster among its neighboring counties (or the largest cluster overall if isolated).
fix_na_by_nearest <- function(clu_named_vec, g) {
  stopifnot(!is.null(names(clu_named_vec)))
  unlabeled <- names(clu_named_vec)[is.na(clu_named_vec)]
  if (!length(unlabeled)) return(clu_named_vec)

  for (u in unlabeled) {
    # neighbors present in the vector
    neigh <- igraph::neighbors(g, u) |> names()
    neigh <- neigh[neigh %in% names(clu_named_vec)]
    nclu  <- clu_named_vec[neigh]
    nclu  <- nclu[!is.na(nclu)]

    if (length(nclu)) {
      tab <- sort(table(nclu), decreasing = TRUE)
      clu_named_vec[u] <- names(tab)[1]
    } else {
      # total fallback: largest cluster overall among labeled
      tab <- sort(table(clu_named_vec[!is.na(clu_named_vec)]), decreasing = TRUE)
      if (length(tab)) clu_named_vec[u] <- names(tab)[1]
    }
  }
  clu_named_vec
}

make_cluster_map <- function(sf_data, var, title = NULL) {
  states <- tigris::states(cb = TRUE, class = "sf") %>% sf::st_transform(crs_proj)
  vr   <- range(sf_data[[var]], na.rm = TRUE)
  lims <- c(floor(min(vr)*1000)/1000, ceiling(max(vr)*1000)/1000)
  plot_title <- if (is.null(title)) var else title

  ggplot() +
    geom_sf(data = sf_data, aes(fill = .data[[var]]), colour = NA) +
    geom_sf(data = states, fill = NA, colour = "white", linewidth = 0.3) +
    scale_fill_distiller(palette = "RdBu", direction = -1,
                         limits = lims, oob = scales::squish, na.value = "grey90") +
    coord_sf(xlim = c(-2500000, 2500000),
             ylim = c(-2200000, 730000),
             expand = FALSE) +
    labs(title = plot_title, fill = NULL) +
    theme_void() +
    theme(
      plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
      legend.text = element_text(size = 9)
    )
}

# 2) Build & plot (fills NAs before union) ---------------------
out_dir <- here("figures", "cluster_diversity_maps")
dir.create(out_dir, showWarnings = FALSE, recursive = TRUE)

imap(shapefile_years, function(yr, win) {
  message("Mapping ", win, " …")

  # counties + graph for THIS period's geography
  geo <- build_graph_for_year(yr)
  counties_sf <- geo$sf
  g_period    <- geo$g

  # membership for this period; ensure vector named by fips
  mem <- ccm %>% filter(period == win) %>% select(fips, cluster)

  # start from all counties in that year — join, then fill missing cluster labels
  all_cty <- counties_sf %>% left_join(mem, by = "fips")
  clu_vec <- setNames(all_cty$cluster, all_cty$fips)
  clu_vec <- fix_na_by_nearest(clu_vec, g_period)

  # attach filled clusters back
  all_cty$cluster <- unname(clu_vec[all_cty$fips])

  # build cluster polygons
  clusters_sf <- all_cty %>%
    group_by(cluster) %>%
    summarise(geometry = sf::st_union(geometry), .groups = "drop")

  # attach diversity for this period
  div_p <- div_tbl %>% filter(period == win) %>% select(cluster, S, I, deaths)

  sf_dat <- clusters_sf %>% left_join(div_p, by = "cluster")

  # plot & save
  pS <- make_cluster_map(sf_dat, "S", glue("Cause-of-death diversity S — {win}"))
  pI <- make_cluster_map(sf_dat, "I", glue("CoD inequality (Rao’s Q) — {win}"))

  ggsave(filename = file.path(out_dir, glue("cluster_S_{win}.png")), plot = pS, width = 8, height = 6, dpi = 320)
  ggsave(filename = file.path(out_dir, glue("cluster_I_{win}.png")), plot = pI, width = 8, height = 6, dpi = 320)

  print(pS); print(pI)
})
```
maps phillips detail metric
```{r}
# ──────────────────────────────────────────────────────────────
# Map Phillips detail — CLUSTERS ONLY (by period)
# ──────────────────────────────────────────────────────────────
suppressPackageStartupMessages({
  library(dplyr); library(readr); library(stringr); library(purrr)
  library(sf); library(tigris); library(ggplot2); library(glue); library(here)
})

options(tigris_use_cache = TRUE, tigris_class = "sf")
crs_proj <- 2163

# ◼ Choose which metric to plot: "detail_phillips_refsize" or "detail_phillips_raw"
VAR <- "detail_phillips_refsize"

# ‣ Inputs -----------------------------------------------------
clu_det <- read_csv(here("output", "cluster_phillips_detail.csv"), show_col_types = FALSE)
stopifnot(all(c("cluster","period") %in% names(clu_det)))

# if refsize column isn’t present (older runs), fall back to raw
if (!VAR %in% names(clu_det)) {
  message("Requested VAR '", VAR, "' not found. Falling back to 'detail_phillips_raw'.")
  VAR <- "detail_phillips_raw"
  stopifnot(VAR %in% names(clu_det))
}

ccm <- read_csv(here("output", "county_cluster_membership.csv"), show_col_types = FALSE) %>%
  mutate(fips = str_pad(as.character(fips), 5, pad = "0")) %>%
  select(fips, period, cluster)

# ‣ Period → shapefile year (UPDATED periods) ------------------
shapefile_years <- c("1999_2006" = 2000,
                     "2007_2014" = 2010,
                     "2015_2022" = 2020)

# ‣ Helpers ----------------------------------------------------
.pick_fips <- function(df) {
  hits <- grep("^GEOID", names(df), value = TRUE)
  if (length(hits) >= 1) return(df[[hits[1]]])
  if (all(c("STATEFP","COUNTYFP") %in% names(df))) return(paste0(df$STATEFP, df$COUNTYFP))
  stop("No GEOID or STATEFP/COUNTYFP columns found in counties() output. Names: ",
       paste(names(df), collapse = ", "))
}

build_cluster_sf <- function(period_name, shp_year, ccm_df) {
  counties_raw <- tigris::counties(year = shp_year, cb = TRUE, class = "sf") %>%
    sf::st_zm(drop = TRUE, what = "ZM") %>%
    sf::st_transform(crs_proj)

  counties_sf <- counties_raw %>%
    mutate(fips = str_pad(.pick_fips(counties_raw), 5, pad = "0")) %>%
    select(fips, geometry)

  m <- ccm_df %>% filter(period == period_name)
  if (nrow(m) == 0) stop("No county→cluster rows for period ", period_name)

  j <- counties_sf %>% left_join(m, by = "fips") %>% filter(!is.na(cluster))

  clusters_sf <- j %>%
    group_by(cluster) %>%
    summarise(geometry = sf::st_union(geometry), .groups = "drop") %>%
    suppressWarnings()
  clusters_sf
}

make_map <- function(sf_data, var, title = NULL) {
  states <- tigris::states(cb = TRUE, class = "sf") %>% sf::st_transform(crs_proj)
  lims <- c(54, 60)  # fixed scale range
  plot_title <- if (is.null(title)) var else title

  ggplot() +
    geom_sf(data = sf_data, aes(fill = .data[[var]]), colour = NA) +
    geom_sf(data = states, fill = NA, colour = "white", linewidth = 0.3) +
    scale_fill_distiller(palette = "RdBu", direction = -1,
                         limits = lims, oob = scales::squish, na.value = "grey90") +
    coord_sf(xlim = c(-2500000, 2500000),
             ylim = c(-2200000, 730000),
             expand = FALSE) +
    labs(title = plot_title, fill = NULL) +
    theme_void() +
    theme(
      plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
      legend.text = element_text(size = 9)
    )
}
# ‣ Output dir -------------------------------------------------
out_dir_clu <- here("figures","phillips_maps",
                    paste0("cluster_", if (VAR=="detail_phillips_refsize") "refsize" else "raw"))
dir.create(out_dir_clu, recursive = TRUE, showWarnings = FALSE)
dev_png <- "png"  # switch to ragg if you like

# ‣ Build + save (cluster‑only) --------------------------------
imap(shapefile_years, function(yr, win) {
  message("Mapping clusters — ", win, "…")

  cl_sf  <- build_cluster_sf(win, yr, ccm)
  dat_cl <- clu_det %>% filter(period == win) %>% select(cluster, !!VAR)
  sf_cl  <- cl_sf %>% left_join(dat_cl, by = "cluster")

  pC <- make_map(sf_cl, VAR, glue("Phillips detail — {if (VAR=='detail_phillips_refsize') 'm=2000' else 'raw'} — {win}"))
  ggsave(filename = file.path(out_dir_clu, glue("cluster_phillips_{if (VAR=='detail_phillips_refsize') 'refsize' else 'raw'}_{win}.png")),
         plot = pC, width = 8, height = 6, dpi = 320, device = dev_png)
})
```
Validation: direction score by public health spending
```{r}
# ──────────────────────────────────────────────────────────────
# 12 · Robust: build direction_year (if missing) + PH spend time series
# ──────────────────────────────────────────────────────────────
suppressPackageStartupMessages({
  library(dplyr); library(readr); library(stringr); library(tidyr)
  library(ggplot2); library(scales); library(purrr); library(here)
})

options(tigris_use_cache = TRUE)

# -- Helper: standardize to 5-digit county_ihme ----------------
std_fips <- function(x) stringr::str_pad(as.character(x), 5, pad = "0")
as_county_ihme <- function(df) {
  nm <- names(df)
  id_col <- dplyr::case_when(
    "county_ihme" %in% nm ~ "county_ihme",
    "GEOID"       %in% nm ~ "GEOID",
    "fips"        %in% nm ~ "fips",
    "FIPS"        %in% nm ~ "FIPS",
    "countyrs"    %in% nm ~ "countyrs",
    TRUE ~ NA_character_
  )
  if (is.na(id_col)) stop("No county ID column found. Expect one of county_ihme/GEOID/fips/FIPS/countyrs.")
  df %>% mutate(county_ihme = std_fips(.data[[id_col]]))
}

# -- 0) Build direction_year if it doesn't exist ----------------
if (!exists("direction_year")) {
  stopifnot(exists("df"))
  # Expect these cols in df (same as your education chunk)
  needed <- c("DQ_prop_garbage","prop_light","pct_overd_miss","pct_acc_miss","year")
  if (!all(needed %in% names(df))) stop("df is missing required columns: ", paste(setdiff(needed, names(df)), collapse=", "))
  df <- as_county_ihme(df)

  global_stats <- df %>%
    summarise(across(
      c(DQ_prop_garbage, prop_light, pct_overd_miss, pct_acc_miss),
      list(mean = ~mean(.x, na.rm = TRUE),
           sd   = ~sd(.x, na.rm = TRUE)),
      .names = "{.col}_{.fn}"
    ))
  zscore_global <- function(x, m, s) (x - m) / s

  direction_year <- df %>%
    filter(year >= 1999, year <= 2022) %>%
    mutate(
      z_DQ_prop_garbage  = zscore_global(DQ_prop_garbage, global_stats$DQ_prop_garbage_mean, global_stats$DQ_prop_garbage_sd),
      z_prop_light       = zscore_global(prop_light,      global_stats$prop_light_mean,      global_stats$prop_light_sd),
      z_pct_overd_miss   = zscore_global(pct_overd_miss,  global_stats$pct_overd_miss_mean,  global_stats$pct_overd_miss_sd),
      z_pct_acc_miss     = zscore_global(pct_acc_miss,    global_stats$pct_acc_miss_mean,    global_stats$pct_acc_miss_sd),
      direction_score    = (z_prop_light + z_pct_overd_miss + z_pct_acc_miss + z_DQ_prop_garbage) / 4
    ) %>%
    select(county_ihme, year, direction_score) %>%
    filter(!is.na(county_ihme), !is.na(direction_score))
}

# -- 1) Finance + population for per-capita ---------------------
# Expect fin_all (county_ihme, fin_year, ph_exp_total) and pid_all from previous chunk.
stopifnot(exists("fin_all"))
fin_all <- as_county_ihme(fin_all)

# Try ACS; if it fails, fallback to PID population (fixed-width field)
get_pop_acs_safe <- function(fin_years) {
  ok <- FALSE
  out <- NULL
  if (requireNamespace("tidycensus", quietly = TRUE)) {
    try({
      out <- purrr::map_dfr(fin_years, function(y) {
        tidycensus::get_acs(geography = "county", variables = "B01001_001",
                            year = y, survey = "acs5", cache_table = TRUE, show_call = FALSE) %>%
          transmute(county_ihme = GEOID, fin_year = y, pop = estimate)
      })
      ok <- TRUE
    }, silent = TRUE)
  }
  list(ok = ok, dat = out)
}

# --- clean available finance years -----------------------------------------
avail_fin_years <- sort(unique(na.omit(fin_all$fin_year)))
if (length(avail_fin_years) == 0L) stop("No valid fin_year values in fin_all.")

# --- re-run ACS join result chosen earlier (unchanged) ----------------------
# pop_join already set above via ACS or PID fallback

ph_pc <- fin_all %>%
  filter(!is.na(fin_year)) %>%               # drop NA fin_year rows before join
  left_join(pop_join, by = c("county_ihme","fin_year")) %>%
  mutate(ph_pc = ph_exp_total / pop) %>%
  filter(is.finite(ph_pc))

# --- helper: nearest financed year (robust to empty avail) ------------------
nearest_fin_year <- function(y, avail) {
  if (!length(avail) || is.na(y)) return(NA_integer_)
  avail[ which.min(abs(avail - y)) ]
}

# --- helper: safe quintiles per-group ---------------------------------------
safe_quintile <- function(x) {
  labs <- c("Q1 lowest","Q2","Q3","Q4","Q5 highest")
  v <- x[is.finite(x)]
  if (length(v) < 5L || length(unique(v)) < 5L) {
    # not enough data spread to form 5 bins
    return(factor(rep(NA_character_, length(x)), levels = labs))
  }
  qs <- quantile(v, probs = seq(0, 1, 0.2), na.rm = TRUE, names = FALSE, type = 7)
  # ensure strictly increasing breaks for cut(); widen ends slightly
  qs[1] <- min(v) - 1e-9
  qs[length(qs)] <- max(v) + 1e-9
  cut(x, breaks = qs, include.lowest = TRUE, right = FALSE, labels = labs)
}

# --- build dir_ph with guarded quintiles ------------------------------------
dir_ph <- direction_year %>%
  mutate(fin_year = vapply(year, nearest_fin_year, integer(1), avail = avail_fin_years)) %>%
  left_join(ph_pc, by = c("county_ihme","fin_year")) %>%
  group_by(fin_year) %>%
  mutate(
    ph_quintile = safe_quintile(ph_pc)
  ) %>%
  ungroup()

# -- 3) Time series by spend quintile ---------------------------
ts_ph <- dir_ph %>%
  filter(!is.na(ph_quintile)) %>%
  group_by(year, ph_quintile) %>%
  summarise(avg_direction = mean(direction_score, na.rm = TRUE), .groups = "drop")

g_ph <- ggplot(ts_ph, aes(year, avg_direction, colour = ph_quintile)) +
  geom_line(linewidth = 1) +
  labs(title = "Direction-score trend by public health spend quintile",
       subtitle = paste0("Per-capita (func 32), snapped to nearest finance year: ",
                         paste(avail_fin_years, collapse = ", ")),
       x = NULL, y = "Mean direction score", colour = "Spend quintile") +
  scale_y_continuous(labels = number_format(accuracy = 0.01)) +
  theme_bw()

ggsave(here("figures", "timeseries_direction_by_ph_spend_quintile.png"),
       g_ph, width = 7, height = 4, dpi = 300)
print(g_ph)

# -- 4) Cross-section R² (county averages) vs spend -------------
if (!exists("res")) {
  res <- direction_year %>%
    group_by(county_ihme) %>%
    summarise(direction_score = mean(direction_score, na.rm = TRUE), .groups = "drop")
}
res <- as_county_ihme(res)

latest_fin <- max(avail_fin_years)
var_ph <- res %>%
  select(county_ihme, direction_score) %>%
  left_join(ph_pc %>% filter(fin_year == latest_fin) %>% select(county_ihme, ph_pc),
            by = "county_ihme") %>%
  filter(is.finite(direction_score), is.finite(ph_pc)) %>%
  mutate(log_ph_pc = log10(pmax(ph_pc, 1e-6)))

fit_ph <- lm(direction_score ~ log_ph_pc, data = var_ph)
r2_ph <- summary(fit_ph)$r.squared
cat("Public health spend R²:", sprintf("%.3f", r2_ph),
    "→", round(r2_ph*100,1), "% of variance in average direction_score explained by log10(per-capita PH spend).\n")
```
Direction score by reporting type
```{r}
# ──────────────────────────────────────────────────────────────
# 5) Reporting type (ME/Coroner/Mixed/etc.) — time series & R²
#     expects a CSV with county FIPS + reporting type per county
# ──────────────────────────────────────────────────────────────

# ──────────────────────────────────────────────────────────────
# 5.1 · Read reporting-type lookup  (robust column detection)
# ──────────────────────────────────────────────────────────────
reporting_path_opts <- c(
  here("data_raw", "County-Death-Investigation-System-2018-1-9-2024.csv"),
  "/mnt/data/County-Death-Investigation-System-2018-1-9-2024.csv"
)
reporting_path <- reporting_path_opts[file.exists(reporting_path_opts)][1]
if (is.na(reporting_path)) stop("Could not find the reporting-type CSV at either default path.")

rep_raw <- readr::read_csv(reporting_path, show_col_types = FALSE)

# helper: return the FIRST column whose name matches any regex in 'patterns'
get_colname <- function(df, patterns) {
  nm <- names(df)
  hits <- which(Reduce(`|`, lapply(patterns, function(p) grepl(p, nm, ignore.case = TRUE))))
  if (length(hits) == 0) return(NA_character_)
  nm[hits[1]]
}

# Try common variants
fips_col <- get_colname(rep_raw, c("^fips$", "^fips_?code$", "geoid", "county_?fips", "countyrs", "fips.*5"))
type_col <- get_colname(rep_raw, c("reporting.*type", "investigation.*type", "death.*investigation.*system", "^type$", "system"))

# If still not found, print names to help and stop with a clear message
if (is.na(fips_col) || is.na(type_col)) {
  message("Columns in reporting CSV:\n- ", paste(names(rep_raw), collapse = "\n- "))
  stop("Could not detect FIPS and/or reporting-type column. ",
       "Set them manually, e.g.: fips_col <- 'FIPS'; type_col <- 'Reporting Type'")
}

message("Using columns → FIPS: '", fips_col, "' | reporting type: '", type_col, "'")

# Build lookup (harmonize & clean)
rep_lu <- rep_raw %>%
  mutate(
    county_ihme    = std_fips(.data[[fips_col]]),
    reporting_type = trimws(as.character(.data[[type_col]]))
  ) %>%
  filter(!is.na(county_ihme), nchar(county_ihme) == 5, !is.na(reporting_type), reporting_type != "") %>%
  mutate(                                     # optional harmonization
    reporting_type = dplyr::recode(tolower(reporting_type),
      "medical examiner" = "Medical Examiner",
      "me"               = "Medical Examiner",
      "coroner"          = "Coroner",
      "mixed"            = "Mixed",
      "hybrid"           = "Mixed",
      .default = stringr::str_to_title(reporting_type)
    )
  ) %>%
  select(county_ihme, reporting_type) %>%
  distinct()

# Quick sanity check
print(rep_lu %>% count(reporting_type, sort = TRUE))

# 5.2 · Time series: mean direction_score by reporting type -------------------
ts_rep <- direction_year %>%
  left_join(rep_lu, by = "county_ihme") %>%
  filter(!is.na(reporting_type)) %>%
  group_by(year, reporting_type) %>%
  summarise(avg_direction = mean(direction_score, na.rm = TRUE), .groups = "drop")

g_rep <- ggplot(ts_rep, aes(year, avg_direction, colour = reporting_type)) +
  geom_line(linewidth = 1) +
  labs(
    title = "Direction-score trend by death investigation reporting type",
    x = NULL, y = "Mean direction score", colour = "Reporting type"
  ) +
  scale_y_continuous(labels = scales::number_format(accuracy = 0.01)) +
  theme_bw()

ggsave(here("figures", "timeseries_direction_by_reporting_type.png"),
       g_rep, width = 7, height = 4, dpi = 300)
print(g_rep)

# 5.3 · Optional: cross-section R² of county averages vs reporting type -------
var_rep <- res %>%                 # 'res' was built above as county averages
  left_join(rep_lu, by = "county_ihme") %>%
  filter(!is.na(reporting_type))

if (nrow(var_rep) > 0) {
  fit_rep <- lm(direction_score ~ reporting_type, data = var_rep)
  r2_rep  <- summary(fit_rep)$r.squared
  cat("Reporting-type R²:", sprintf("%.3f", r2_rep),
      "→", round(r2_rep * 100, 1),
      "% of variance in average direction_score explained by reporting type.\n")
} else {
  message("No overlap between county averages and reporting-type lookup.")
}
```








