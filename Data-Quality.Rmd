---
title: "Data Quality Project"
output: html_notebook
---
Load packages and summarize data.
```{r}
library(arrow)
library(dplyr)
library(tidyr)
library(purrr)
library(stringr)
library(ggplot2)
library(readr)
library(janitor)

## ── 0.  Parameters ───────────────────────────────────────────────────────────
parquet_dir   <- "/Users/amymann/Documents/Data Quality Project/data/parquet"
garbage_path  <- "/Users/amymann/Documents/Data Quality Project/data_quality/cause-codes/gbd_garbage_codes_with_descr.csv"

needed_cols   <- c("year", "sex", "race5", "marstat",
                   "placdth", "ucod", "ranum")
extra_cols    <- "educ2003"                  # only present ≥2003
years_wanted  <- 1999:2023

## the 60 secondary-factor columns you care about
sec_cols <- c(paste0("record_",  1:20),
              paste0("econdp_",  1:20),
              paste0("econds_",  1:20))

keep_cols <- c(needed_cols, extra_cols, sec_cols)   # one master list

## ── 1.  Garbage-code lookup table ───────────────────────────────────────────
garbage_lu <- read_csv(garbage_path, show_col_types = FALSE) %>% 
  transmute(
    icd10        = str_to_upper(icd10),
    gbd_severity = as.integer(gbd_severity)
  )

clean_icd <- function(x)
  str_remove_all(str_to_upper(x), "[^A-Z0-9\\.]")

## ── 2.  File-level summariser ───────────────────────────────────────────────
summarise_file <- function(path) {

  ## --- read only the columns we need ----------------------------------------
  dat <- read_parquet(path, col_select = any_of(keep_cols)) %>% 
         as.data.frame()

  ## which of the 60 secondary columns actually exist in this file?
  sec_cols_here <- intersect(keep_cols, names(dat))

  ## --- garbage classification -----------------------------------------------
  dat <- dat %>% 
    mutate(
      year  = as.integer(year),
      icd10 = clean_icd(ucod)
    ) %>% 
    left_join(garbage_lu, by = "icd10")

  ## TRUE if *any* secondary factor is a T-code AND not “… .9”
if (length(sec_cols_here)) {

  # helper that returns TRUE/FALSE for the whole column *at once*
  is_good_T <- function(x) {
    x <- toupper(x)                 # vectorised upper-case
    startsWith(x, "T") &            # begins with T…
      substr(x, 4L, 4L) != "9"      # …but 4th char ≠ 9  (i.e. not T__.9)
  }

  # OR-reduce the 60 logical vectors without touching R’s slow loop
  has_specific_T <- Reduce(`|`, lapply(dat[sec_cols_here], is_good_T))

} else {
  has_specific_T <- rep(FALSE, nrow(dat))      # file had no secondary cols
}


  dat <- dat %>% 
    mutate(
      is_garbage = case_when(
        is.na(gbd_severity)                     ~ FALSE,           # not on garbage list
        !str_starts(icd10, "X")                 ~ TRUE,            # garbage, not X-code
        str_starts(icd10, "X") & !has_specific_T~ TRUE,            # X w/o good T
        TRUE                                    ~ FALSE            # X w/ good T
      )
    )

  ## --- collapse to long format ----------------------------------------------
  dat %>% 
    select(-any_of(c("ucod", "icd10", "gbd_severity", sec_cols_here))) %>% 
    pivot_longer(
      -c(year, is_garbage),
      names_to  = "variable",
      values_to = "level",
      values_transform = list(level = as.character)
    ) %>% 
    filter(
      !is.na(level),
      !(variable == "educ2003" & year < 2003)
    ) %>% 
    group_by(year, variable, level) %>% 
    summarise(
      total_n      = n(),
      garbage_n    = sum(is_garbage),
      prop_garbage = garbage_n / total_n,
      .groups      = "drop"
    )
}


## ── 3.  Run over all Parquet files ──────────────────────────────────────────
file_paths <- list.files(parquet_dir, "\\.parquet$", full.names = TRUE)
ts_long    <- map_dfr(file_paths, summarise_file)

## ── 4–5.  Add yearly totals (unchanged) -------------------------------------
totals_by_year <- ts_long %>%
  group_by(year) %>%
  summarise(
    total_n      = sum(total_n),
    garbage_n    = sum(garbage_n),
    prop_garbage = garbage_n / total_n,
    .groups      = "drop"
  ) %>%
  mutate(variable = "ALL", level = "ALL")

ts_long <- bind_rows(ts_long, totals_by_year)

```
Plots of proportion of garbage codes overtime and average severity overtime 
```{r}
fig_dir <- "/Users/amymann/Documents/Data Quality Project/data_quality/figures"
dir.create(fig_dir, recursive = TRUE, showWarnings = FALSE)

ts_garbage <- ts_long %>%
  filter(variable == "garbage_n") %>% 
  group_by(year) 

prop_garbage_plot<-ts_garbage %>%
  filter(level == "Garbage") %>% 
  ggplot(aes(year, prop)) +
    geom_line(color = "black") +
    scale_y_continuous(labels = scales::percent) +
    labs(title = "Proportion of deaths classified as garbage",
         x = NULL, y = "Proportion of deaths")

avg_severity_ts <- ts_long %>% 
  filter(variable == "gbd_severity", level != "0-Non") %>% 
  mutate(
    severity = as.numeric(level),
    n        = as.numeric(total_n)) %>% 
  group_by(year) %>% 
  summarise(
    total_deaths = sum(n),
    avg_severity = sum(severity * total_n) / total_deaths,
    .groups      = "drop" )

severity_plot <- ggplot(avg_severity_ts, aes(year, avg_severity)) +
  geom_line() +
  scale_y_continuous(limits = c(1, 4)) +
  labs(title = "Average garbage-code severity, 1999–2023",
       x = NULL, y = "Average severity (scale 1-4)")

## 1 ── make (or reuse) the yearly-total table -----------------------
totals_by_year <- ts_long %>%           # if you haven’t built it yet
  group_by(year) %>% 
  summarise(
    total_n      = sum(total_n),
    garbage_n    = sum(garbage_n),
    prop_garbage = garbage_n / total_n,
    .groups      = "drop"
  ) %>% 
  mutate(variable = "ALL", level = "ALL")

## 2 ── plot the proportion ------------------------------------------
prop_garbage_plot <- ggplot(totals_by_year,
                            aes(year, prop_garbage)) +
  geom_line(colour = "black") +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "Proportion of deaths classified as garbage",
       x = NULL, y = "Proportion of deaths")

ggsave(file.path(fig_dir, "prop_garbage_over_time.png"),
       plot   = prop_garbage_plot,
       width  = 7, height = 4, dpi = 300)

ggsave(file.path(fig_dir, "avg_garbage_severity_over_time.png"),
       plot   = severity_plot,
       width  = 7, height = 4, dpi = 300)

```
Demographics v.s. proportion garbage codes overtime
```{r}
# assumes `ts_demo` and `fig_dir` already exist
demo_vars <- c("sex", "race5", "marstat", "placdth", "educ2003", "popsizeoc", "stateoc")

walk(demo_vars, \(v) {
  p <- ggplot(filter(ts_long, variable == v),
              aes(year, prop_garbage, colour = level)) +
         geom_line() +
         scale_y_continuous(labels = percent) +
         labs(title  = paste("Proportion of garbage-coded deaths –", v),
              x = NULL, y = "Proportion of deaths", colour = v)

  ggsave(file.path(fig_dir, paste0("prop_garbage_by_", v, ".png")),
         plot = p, width = 7, height = 4, dpi = 300)
})

```
Controlling for cause of death, plots odds ratios for place of death, sex, and marital status.
```{r}
# ── 0.  Packages ─────────────────────────────────────────────────────────────
library(arrow)
library(dplyr)
library(stringr)
library(purrr)
library(readr)
library(broom)
library(ggplot2)
library(tidyr)

# ── 1.  Paths & constants ────────────────────────────────────────────────────
parquet_dir <- "/Users/amymann/Documents/Data Quality Project/data/parquet"
garbage_path <- file.path(
  "/Users/amymann/Documents/Data Quality Project",
  "data_quality/cause-codes",
  "gbd_garbage_codes_with_descr_no_overdose.csv"
)

years_wanted   <- 1999:2023
vars_interest  <- c("placdth", "sex", "marstat")

base_cols  <- c("year", "ucr39", "ucod", "ranum", vars_interest)
extra_cols <- "educ2003"                      # present only ≥2003

## all 60 secondary-factor columns you might need
sec_cols   <- c(paste0("record_",  1:20),
                paste0("econdp_",  1:20),
                paste0("econds_",  1:20))

keep_cols  <- c(base_cols, extra_cols, sec_cols)

garbage_lu <- read_csv(garbage_path, show_col_types = FALSE) %>%
  transmute(
    icd10        = str_to_upper(icd10),
    gbd_severity = as.integer(gbd_severity)
  )

clean_icd <- function(x) str_remove_all(str_to_upper(x), "[^A-Z0-9\\.]")

# ── helper: identify “good” T-codes vectorised over a whole column ───────────
is_good_T <- function(x) {
  x <- toupper(x)
  startsWith(x, "T") & substr(x, 4L, 4L) != "9"
}

# ── 2.  Fit *separate* models for each variable in one file/year ────────────
fit_year <- function(path) {

  ## read only available columns in this parquet
  dat <- read_parquet(path, col_select = any_of(keep_cols)) %>% 
    as.data.frame()

  ## which secondary columns exist in this particular file?
  sec_cols_here <- intersect(sec_cols, names(dat))

  ## -------------------------------------------------------------------------
  ## garbage-code flag with the new overdose rule
  ## -------------------------------------------------------------------------
  dat <- dat %>% 
    mutate(
      year  = as.integer(year),
      icd10 = clean_icd(ucod)
    ) %>% 
    left_join(garbage_lu, by = "icd10")

  ## TRUE for at least one specific T-code (vectorised OR)
  if (length(sec_cols_here)) {
    has_specific_T <- Reduce(`|`, lapply(dat[sec_cols_here], is_good_T))
  } else {
    has_specific_T <- rep(FALSE, nrow(dat))
  }

  dat <- dat %>% 
    mutate(
      is_garbage = case_when(
        is.na(gbd_severity)                         ~ FALSE,            # not on list
        !str_starts(icd10, "X")                     ~ TRUE,             # garbage, not X
        str_starts(icd10, "X") & !has_specific_T    ~ TRUE,             # X, no good T
        TRUE                                        ~ FALSE             # X with good T
      ),
      ucr39 = factor(ucr39)
    ) %>% 
    select(-ucod, -icd10, -gbd_severity, -any_of(sec_cols_here))

  yr <- unique(dat$year)
  stopifnot(length(yr) == 1)

  ## ── run separate models for each focal variable ──────────────────────────
  bind_rows(lapply(vars_interest, function(var) {

    if (!(var %in% names(dat))) return(NULL)

    dat_var <- dat %>% 
      mutate(across(all_of(var), factor)) %>% 
      filter(!is.na(.data[[var]]), !is.na(ucr39)) %>% 
      droplevels()

    if (nlevels(dat_var[[var]]) < 2 || nlevels(dat_var$ucr39) < 2)
      return(NULL)

    fmla <- as.formula(paste("is_garbage ~", var, "+ ucr39"))
    mod  <- glm(fmla, family = binomial("logit"), data = dat_var)

    broom::tidy(mod, conf.int = FALSE, exponentiate = FALSE) %>% 
      filter(term != "(Intercept)", !startsWith(term, "ucr39")) %>% 
      mutate(
        conf.low  = estimate - 1.96 * std.error,
        conf.high = estimate + 1.96 * std.error,
        estimate  = exp(estimate),
        conf.low  = exp(conf.low),
        conf.high = exp(conf.high),
        year      = yr,
        variable  = var
      )
  }))
}

# ── 3.  Run every file/year ──────────────────────────────────────────────────
file_paths <- list.files(parquet_dir, "\\.parquet$", full.names = TRUE)
coefs      <- map_dfr(file_paths, fit_year)

out_dir <- "/Users/amymann/Documents/Data Quality Project/data_quality/figures"
dir.create(out_dir, recursive = TRUE, showWarnings = FALSE)
write_csv(coefs, file.path(out_dir, "garbage_logit_coefs.csv"))

# ── 5.  Prep for plotting ────────────────────────────────────────────────────
plot_dat <- coefs %>% 
  mutate(level = str_remove(term, paste0("^", variable))) %>% 
  arrange(variable, level, year)

# ── 6.  Plot: odds-ratio trajectories ────────────────────────────────────────
or_plot <- ggplot(plot_dat,
                  aes(x = year, y = estimate,
                      colour = level, fill = level, group = level)) +
  geom_ribbon(aes(ymin = conf.low, ymax = conf.high),
              alpha = 0.20, linewidth = 0) +
  geom_line(linewidth = 0.7) +
  scale_y_log10(expand = expansion(mult = c(0.05, 0.05))) +
  facet_wrap(~ variable, scales = "free_y") +
  labs(title = "Odds ratios for garbage cause-of-death coding, 1999–2023",
       subtitle = "Separate logistic models adjusting for ICD-39 category (ucr39)",
       y = "Odds ratio (log scale)", x = NULL,
       colour = "Level", fill = "Level") +
  theme_minimal(base_size = 11) +
  theme(legend.position = "bottom")

ggsave(file.path(out_dir, "odds_ratios_over_time.png"),
       or_plot, width = 8, height = 4.5, dpi = 320)

# ── 7.  Quick look if you’re in RStudio ──────────────────────────────────────
print(or_plot)

```
Controlling for cause of death, plots odds ratios for race and educational attainment.
```{r}
library(arrow)
library(dplyr)
library(stringr)
library(purrr)
library(readr)
library(broom)
library(ggplot2)
library(tidyr)

## ─────────────────────────── 1 · Paths & constants ────────────────────────
parquet_dir <- "/Users/amymann/Documents/Data Quality Project/data/parquet"

garbage_path <- file.path(
  "/Users/amymann/Documents/Data Quality Project",
  "data_quality/cause-codes",
  "gbd_garbage_codes_with_descr_no_overdose.csv"
)

years_wanted  <- 1999:2023

## focal predictors — *just* race and education -----------------------------
vars_interest <- c("race5", "educ2003")   # <-- change made here

base_cols  <- c("year", "ucr39", "ucod", "ranum", vars_interest)
extra_cols <- "educ2003"                  # present only ≥2003

## all 60 potential secondary-factor columns
sec_cols <- c(paste0("record_",  1:20),
              paste0("econdp_",  1:20),
              paste0("econds_",  1:20))

keep_cols <- c(base_cols, extra_cols, sec_cols)

garbage_lu <- read_csv(garbage_path, show_col_types = FALSE) %>%
  transmute(
    icd10        = str_to_upper(icd10),
    gbd_severity = as.integer(gbd_severity)
  )

clean_icd <- function(x)
  str_remove_all(str_to_upper(x), "[^A-Z0-9\\.]")

## ── helper: identify “good” T-codes (vectorised) ───────────────────────────
is_good_T <- function(x) {
  x <- toupper(x)
  startsWith(x, "T") & substr(x, 4L, 4L) != "9"
}

## ───────────────────── 2 · Fit models for one file/year ────────────────────
fit_year <- function(path) {

  ## read only available columns
  dat <- read_parquet(path, col_select = any_of(keep_cols)) |>
         as.data.frame()

  ## which secondary columns exist here?
  sec_cols_here <- intersect(sec_cols, names(dat))

  ## ── garbage flag with overdose rule ──────────────────────────────────────
  dat <- dat %>%
    mutate(
      year  = as.integer(year),
      icd10 = clean_icd(ucod)
    ) %>%
    left_join(garbage_lu, by = "icd10")

  ## TRUE if any secondary column contains a specific T-code
  has_specific_T <-
    if (length(sec_cols_here))
      Reduce(`|`, lapply(dat[sec_cols_here], is_good_T))
    else
      rep(FALSE, nrow(dat))

  dat <- dat %>%
    mutate(
      is_garbage = case_when(
        is.na(gbd_severity)                      ~ FALSE,           # not on list
        !str_starts(icd10, "X")                  ~ TRUE,            # garbage, not X
        str_starts(icd10, "X") & !has_specific_T ~ TRUE,            # X with no good T
        TRUE                                     ~ FALSE            # X with good T
      ),
      ucr39 = factor(ucr39)
    ) %>%
    select(-ucod, -icd10, -gbd_severity, -any_of(sec_cols_here))

  yr <- unique(dat$year)
  stopifnot(length(yr) == 1)

  ## ── separate models for each focal variable ──────────────────────────────
  bind_rows(lapply(vars_interest, function(var) {

    if (!(var %in% names(dat))) return(NULL)

    dat_var <- dat %>%
      mutate(across(all_of(var), factor)) %>%
      filter(!is.na(.data[[var]]), !is.na(ucr39)) %>%
      droplevels()

    if (nlevels(dat_var[[var]]) < 2 || nlevels(dat_var$ucr39) < 2)
      return(NULL)

    fmla <- as.formula(paste("is_garbage ~", var, "+ ucr39"))
    mod  <- glm(fmla, family = binomial("logit"), data = dat_var)

    broom::tidy(mod, conf.int = FALSE, exponentiate = FALSE) %>%
      filter(term != "(Intercept)", !startsWith(term, "ucr39")) %>%
      mutate(
        conf.low  = estimate - 1.96 * std.error,
        conf.high = estimate + 1.96 * std.error,
        estimate  = exp(estimate),
        conf.low  = exp(conf.low),
        conf.high = exp(conf.high),
        year      = yr,
        variable  = var
      )
  }))
}

## ───────────────────── 3 · Run across all parquet files ────────────────────
file_paths <- list.files(parquet_dir, "\\.parquet$", full.names = TRUE)
coefs      <- map_dfr(file_paths, fit_year)

out_dir <- "/Users/amymann/Documents/Data Quality Project/data_quality/figures"
dir.create(out_dir, recursive = TRUE, showWarnings = FALSE)
write_csv(coefs, file.path(out_dir, "garbage_logit_coefs.csv"))

## ────────────────────────── 4 · Prepare for plotting ───────────────────────
plot_dat <- coefs %>%
  mutate(level = str_remove(term, paste0("^", variable))) %>%
  arrange(variable, level, year)

## ─────────────────────────── 5 · Plot trajectories ─────────────────────────
or_plot <- ggplot(plot_dat,
                  aes(x = year, y = estimate,
                      colour = level, fill = level, group = level)) +
  geom_ribbon(aes(ymin = conf.low, ymax = conf.high),
              alpha = 0.20, linewidth = 0) +
  geom_line(linewidth = 0.7) +
  scale_y_log10(expand = expansion(mult = c(0.05, 0.05))) +
  facet_wrap(~ variable, scales = "free_y") +
  labs(title = "Odds ratios for garbage cause-of-death coding, 1999–2023",
       subtitle = "Separate logistic models adjusting for ICD-39 category (ucr39)",
       y = "Odds ratio (log scale)", x = NULL,
       colour = "Level", fill = "Level") +
  theme_minimal(base_size = 11) +
  theme(legend.position = "bottom")

ggsave(file.path(out_dir, "odds_ratios_over_time.png"),
       or_plot, width = 8, height = 4.5, dpi = 320)

## ───────────────────────────── 6 · Quick look ──────────────────────────────
print(or_plot)

```
Calculate timeseries for no secondary factor, number of secondary factors and autopsy for place of death, education level, race, and marital status
```{r}
# ── 1b.  Pull only what we need ───────────────────────────────────────────────
vars_interest <- c("placdth", "sex", "marstat", "race5")
base_cols  <- c("year", "ucr39", "ucod", "ranum", vars_interest)
extra_cols <- "educ2003"  # if you still need it elsewhere
# ── 2c.  Summarize no‐secondary and avg‐secondary only ────────────────────────
fit_secondary_models <- function(path) {
  # 1. Read & preprocess
  dat <- read_parquet(path, col_select = any_of(c(base_cols, extra_cols))) %>%
    mutate(
      year          = as.integer(year),
      ranum         = as.integer(ranum),
      no_secondary  = as.integer(ranum <= 1),
      num_secondary = pmax(ranum - 1, 0),
      ucr39         = factor(ucr39)
    ) %>%
    filter(year %in% years_wanted) %>%
    select(year, all_of(vars_interest), no_secondary, num_secondary, ucr39)

  yr <- unique(dat$year)
  stopifnot(length(yr) == 1)
  print(paste("Processing year:", yr))

  # 2. Per-variable aggregation + fast GLMs with Wald CIs
  bind_rows(lapply(vars_interest, function(var) {
    if (!(var %in% names(dat))) return(NULL)

    d <- dat %>%
      filter(!is.na(.data[[var]])) %>%
      mutate(across(all_of(var), factor)) %>%
      droplevels()
    if (nlevels(d[[var]]) < 2) return(NULL)

    # 2a) logistic on aggregated counts
    d_log <- d %>%
      group_by(.data[[var]], ucr39) %>%
      summarise(
        success = sum(no_secondary),
        failure = n() - sum(no_secondary),
        .groups = "drop"
      )
    f1 <- as.formula(paste0("cbind(success, failure) ~ ", var, " + ucr39"))
    m1 <- glm(f1, data = d_log, family = binomial())

    t1 <- broom::tidy(m1, conf.int = FALSE, exponentiate = FALSE) %>%
      filter(term != "(Intercept)", !grepl("^ucr39", term)) %>%
      transmute(
        year      = yr,
        variable  = var,
        outcome   = "No secondary factor (OR)",
        level     = sub(paste0("^", var), "", term),
        estimate  = exp(estimate),
        standard_error = std.error,
        conf.low  = exp(estimate - 1.96 * std.error),
        conf.high = exp(estimate + 1.96 * std.error)
      )

    # 2b) Poisson on aggregated sums with offset
    d_pois <- d %>%
      group_by(.data[[var]], ucr39) %>%
      summarise(
        count    = sum(num_secondary),
        exposure = n(),
        .groups  = "drop"
      )
    f2 <- as.formula(paste0("count ~ ", var, " + ucr39 + offset(log(exposure))"))
    m2 <- glm(f2, data = d_pois, family = poisson())

    t2 <- broom::tidy(m2, conf.int = FALSE, exponentiate = FALSE) %>%
      filter(term != "(Intercept)", !grepl("^ucr39", term)) %>%
      transmute(
        year      = yr,
        variable  = var,
        outcome   = "Secondary count (IRR)",
        level     = sub(paste0("^", var), "", term),
        estimate  = exp(estimate),
        conf.low  = exp(estimate - 1.96 * std.error),
        conf.high = exp(estimate + 1.96 * std.error)
      )

    bind_rows(t1, t2)
  }))
}

# ── 3d.  Run over all years and collect coefficients ─────────────────────────
file_paths <- list.files(parquet_dir, "\\.parquet$", full.names = TRUE)
sec_coefs  <- map_dfr(file_paths, fit_secondary_models)

# And to peek in R:
print(sec_coefs)

```
Calculate timeseries for no secondary factor, number of secondary factors by education level
```{r}
# ── Education-only coefficient calculation (2003–2023) ──────────────────────

library(stringr)
library(purrr)
library(dplyr)

fit_secondary_models_educ2003 <- function(path) {
  # 1) read only the columns we need and coerce year to integer
  dat0 <- read_parquet(path,
                       col_select = any_of(c("year", "ranum", "ucr39", "educ2003"))) %>%
    mutate(
      year = as.integer(year)
    )

  # 2) grab the single year, bail if something’s off
  yr <- unique(dat0$year)
  stopifnot(length(yr) == 1)
  message("Processing year: ", yr)

  # 3) skip files with no educ2003 column
  if (!"educ2003" %in% names(dat0)) {
    message("  → skipping: no educ2003 column")
    return(NULL)
  }

  # 4) filter to our years and non‐missing educ2003
  dat <- dat0 %>%
    filter(year %in% years_wanted, !is.na(educ2003)) %>%
    mutate(
      ranum         = as.integer(ranum),
      no_secondary  = as.integer(ranum <= 1),
      num_secondary = pmax(ranum - 1, 0),
      ucr39         = factor(ucr39),
      educ2003      = factor(educ2003)
    ) %>%
    select(year, educ2003, no_secondary, num_secondary, ucr39)

  # 5) if only one level, nothing to do
  if (nlevels(dat$educ2003) < 2) {
    message("  → skipping: only one educ2003 level")
    return(NULL)
  }

  # 6a) logistic (OR) on aggregated counts
  d_log <- dat %>%
    group_by(educ2003, ucr39) %>%
    summarise(
      success = sum(no_secondary),
      failure = n() - sum(no_secondary),
      .groups = "drop"
    )
  m1 <- glm(cbind(success, failure) ~ educ2003 + ucr39,
            data   = d_log,
            family = binomial())
  t1 <- broom::tidy(m1, conf.int = FALSE) %>%
    filter(!grepl("^(Intercept|ucr39)", term)) %>%
    transmute(
      year      = yr,
      variable  = "educ2003",
      outcome   = "No secondary factor (OR)",
      level     = sub("^educ2003", "", term),
      estimate  = exp(estimate),
      conf.low  = exp(estimate - 1.96 * std.error),
      conf.high = exp(estimate + 1.96 * std.error)
    )

  # 6b) Poisson (IRR) on aggregated sums
  d_pois <- dat %>%
    group_by(educ2003, ucr39) %>%
    summarise(
      count    = sum(num_secondary),
      exposure = n(),
      .groups  = "drop"
    )
  m2 <- glm(count ~ educ2003 + ucr39 + offset(log(exposure)),
            data   = d_pois,
            family = poisson())
  t2 <- broom::tidy(m2, conf.int = FALSE) %>%
    filter(!grepl("^(Intercept|ucr39)", term)) %>%
    transmute(
      year      = yr,
      variable  = "educ2003",
      outcome   = "Secondary count (IRR)",
      level     = sub("^educ2003", "", term),
      estimate  = exp(estimate),
      conf.low  = exp(estimate - 1.96 * std.error),
      conf.high = exp(estimate + 1.96 * std.error)
    )

  # 7) return combined results (year is now always integer)
  bind_rows(t1, t2)
}


years_wanted <- 2006:2023
file_paths_edu <- list.files(parquet_dir, "\\.parquet$", full.names = TRUE) %>%
  keep(~ as.integer(str_extract(basename(.), "\\d{4}")) %in% years_wanted)

# Run the (efficient) model‐fitting function over those files
sec_coefs <- map_dfr(file_paths_edu, fit_secondary_models_educ2003) %>%
  filter(variable == "educ2003")

# Inspect the first few rows
head(sec_coefs)

```
Plot the time series
```{r}
# ── 4d.  Plot each variable in its own two‐panel figure (dropping any intercept rows) ────────
library(ggplot2)
library(patchwork)

out_dir <- "/Users/amymann/Documents/Data Quality Project/data_quality/figures/"

# Remove any rows corresponding to the intercept before plotting
plot_dat_sec <- sec_coefs %>%
  filter(level != "(Intercept)") %>%
  arrange(variable, outcome, level)

# Loop over each demographic variable
variables <- unique(plot_dat_sec$variable)
for (var in variables) {
  df_var <- plot_dat_sec %>% filter(variable == var)

  # 1) No secondary factor (OR)
  p_or <- ggplot(
    df_var %>% filter(outcome == "No secondary factor (OR)"),
    aes(x = year, y = estimate, colour = level, fill = level, group = level)
  ) +
    geom_line(size = 0.7) +
    scale_y_log10(expand = expansion(mult = c(0.05, 0.05))) +
    labs(
      title  = paste0(var, ": Odds of No Secondary Factor"),
      x      = NULL,
      y      = "Odds Ratio (log scale)",
      colour = "Level",
      fill   = "Level"
    ) +
    theme_minimal(base_size = 11) +
    theme(legend.position = "bottom")

  # 2) Secondary count (IRR)
  p_irr <- ggplot(
    df_var %>% filter(outcome == "Secondary count (IRR)"),
    aes(x = year, y = estimate, colour = level, fill = level, group = level)
  ) +
    geom_line(size = 0.7) +
    scale_y_log10(expand = expansion(mult = c(0.05, 0.05))) +
    labs(
      title  = paste0(var, ": Rate of Secondary Factors"),
      x      = NULL,
      y      = "Incidence Rate Ratio (log scale)",
      colour = "Level",
      fill   = "Level"
    ) +
    theme_minimal(base_size = 11) +
    theme(legend.position = "bottom")

  # Combine vertically and save
  combined <- p_or / p_irr + plot_layout(ncol = 1, heights = c(1, 1))

  ggsave(
    filename = file.path(out_dir, paste0("secondary_effects_", var, ".png")),
    plot     = combined,
    width    = 8, height = 10, dpi = 320
  )

  message("Saved figure for variable: ", var)
}
```

