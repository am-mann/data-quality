library(readr)
library(arrow)

input_path <- "/Users/amymann/Documents/Data Quality Project/data/txt/Mort2018US.PubUse.txt"
temp_dir <- "/Users/amymann/Documents/Data Quality Project/data/tmp_parquet_chunks"
final_parquet <- "/Users/amymann/Documents/Data Quality Project/data/mortality2018.parquet"
dir.create(temp_dir, showWarnings = FALSE)

col_names <- c(
  "reserved_1", "resident_status", "place_of_death_and_status", "day_of_week_of_death",
  "data_year", "state_of_occurrence", "county_of_occurrence", "county_population",
  "state_of_residence", "residence_state_country_recode", "county_of_residence", "residence_county_population",
  "month_of_death", "sex", "race", "age_detail_type", "age_detail", "age_recode_52",
  "age_substitution_flag", "marital_status", "state_of_birth", "education_1989", "education_2003",
  "education_reporting_flag", "injury_at_work", "manner_of_death", "method_of_disposition", "autopsy",
  "activity_code", "place_of_injury", "icd10", "recode_358", "recode_113", "recode_130", "recode_39",
  "reserved_2", "num_entity_axis_conditions",
  paste0("entity_axis_condition_", 1:20),
  "reserved_3", "num_record_axis_conditions", "reserved_4",
  paste0("record_axis_condition_", 1:20),
  "bridged_race_flag", "bridged_race_recode", "race_imputation_flag",
  "race_recode_3", "race_recode_5", "race_recode_6",
  "hispanic_origin_code", "hispanic_origin_recode", "hispanic_origin_and_race_recode",
  paste0("reserved_", 5:44),
  paste0("col_", 130:152)
)

col_widths <- c(
  19, 1, 1, 1, 4, 2, 3, 3, 2, 3, 3, 3,
  2, 1, 2, 1, 3, 2, 1, 1, 2, 2, 1, 1,
  1, 1, 1, 1, 1, 1, 4, 3, 3, 3, 2, 1,
  2, rep(7, 20),
  36, 2, 1,
  rep(5, 20),
  1, 1, 1, 1, 1, 1,
  3, 1, 1,
  rep(1, 40),
  rep(1, 23)
)

stopifnot(length(col_names) == length(col_widths))

chunk_size <- 100000
con <- file(input_path, open = "r")
chunk_index <- 0

repeat {
  lines <- readLines(con, n = chunk_size)
  if (length(lines) == 0) break

  if (all(nzchar(lines) == FALSE)) next  # skip all-empty line chunks

  # Write lines to a temp file (so read_fwf works)
  tmp_fwf <- tempfile(fileext = ".txt")
  writeLines(lines, tmp_fwf)
  chunk_df <- tryCatch({
    read_fwf(tmp_fwf, fwf_widths(col_widths, col_names), col_types = cols(.default = "c"))
  }, error = function(e) {
    cat("⚠️ Error reading chunk", chunk_index, ":", conditionMessage(e), "\n")
    NULL
  })
  unlink(tmp_fwf)

  if (is.null(chunk_df) || nrow(chunk_df) == 0) next

  out_path <- file.path(temp_dir, sprintf("chunk_%04d.parquet", chunk_index))
  tryCatch({
    write_parquet(chunk_df, out_path)
    cat("✅ Wrote chunk", chunk_index, "\n")
  }, error = function(e) {
    cat("⚠️ Error writing chunk", chunk_index, ":", conditionMessage(e), "\n")
  })

  chunk_index <- chunk_index + 1
}

close(con)

# -- Combine all chunks into single Parquet --
all_chunks <- list.files(temp_dir, pattern = "\\.parquet$", full.names = TRUE)
if (length(all_chunks) > 0) {
  dataset <- open_dataset(all_chunks)
  write_parquet(dataset, final_parquet)
  cat("All done! Final Parquet saved to:\n", final_parquet, "\n")
} else {
  cat("No data chunks were written.\n")
}

unlink(temp_dir, recursive = TRUE)

